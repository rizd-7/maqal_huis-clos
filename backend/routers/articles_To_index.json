{
  "data": [
    "{\n  \"Article_ID\": \"992ab4f99c144c57aec9df254679d1d5\",\n  \"DocumentTitle\": \"Large Language Model Augmented Narrative Driven Recommendations\",\n  \"Auteurs\": \"Sheshera Mysore, Andrew McCallum, Hamed Zamani, \",\n  \"Institutions\": \"University of Massachusetts Amherst, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"CCS CONCEPTS\",\n      \"paragraphs\": [\n        \"\\u2022 Information systems \\u2192 Recommender systems; Users and\\ninteractive retrieval; \\u2022 Computing methodologies \\u2192 Natural language\\ngeneration.\"\n      ]\n    },\n    {\n      \"title\": \"INTRODUCTION\",\n      \"paragraphs\": [\n        \"\\n        Recommender systems personalized to users are an important\\ncomponent of several industry-scale platforms \\n        \",\n        \"\\n        However, with the emergence of conversational interfaces for\\ninformation access tasks, support for complex NDR tasks is likely\\nto become necessary. In this context, recent work has noted an\\nincrease in complex and subjective natural language requests\\ncompared to more conventional search interfaces \\n        \",\n        \"Specifically, given a user\\u2019s interactions,  , with items and\\ntheir accompanying text documents (e.g., reviews, descriptions)\\n = { }=1, selected from a user-item interaction dataset I, we\\nprompt InstructGPT, a 175B parameter LLM, to author a synthetic\\nnarrative query  based on  (Figure 2). Since we expect the\\nquery  to be noisy and not fully representative of all the user\\nreviews,  is filtered to retain only a fraction of the reviews based\\non a language-model assigned likelihood of  given a user\\ndocument,  . Then, a pre-trained LM based retrieval model (110M\\nparameters) is fine-tuned for retrieval on the synthetic queries and\\nifltered reviews.\",\n        \"\\n        Our approach, which we refer to as Mint2, follows from the\\nobservation that while narrative queries and suggestions are often\\nmade in online discussion forums, and could serve as training data,\\nthe number of these posts and the diversity of domains for which\\nthey are available is significantly smaller than the size and diversity\\nof passively gathered user-item interaction datasets. E.g. while\\nBogers and Koolen \\n        \",\n        \"\\n        We empirically evaluate Mint in a publicly available test\\ncollection for point of interest recommendation: pointrec \\n        \"\n      ]\n    },\n    {\n      \"title\": \"RELATED WORK\",\n      \"paragraphs\": [\n        \"\\n        Data Augmentation for Information Access. A line of recent\\nwork has explored using language models to generate synthetic\\nqueries for data augmentation to train models for information\\nretrieval tasks \\n        \",\n        \"\\n          Besides creating queries for ad-hoc retrieval tasks, concurrent\\nwork of Leszczynski et al. \\n          \",\n        \"\\n          Finally, while our work explores data augmentation from\\nuseritem interactions for a retrieval-oriented NDR task, prior work has\\nalso explored data augmentation of the user-item graph for training\\ncollaborative filtering models. This work has often explored\\naugmentation to improve recommendation performance for minority\\n\\n          \",\n        \"\\n          Complex Queries in Information Access. With the advent\\nof performant models for text understanding, focus on complex\\nand interactive information access tasks has seen a resurgence\\n\\n          \",\n        \"\\n          Besides this, a range of work has explored more complex,\\nlongform, and interactive query formulations for information access;\\nthese resemble queries in NDR. Arguello et al. \\n          \"\n      ]\n    },\n    {\n      \"title\": \"3https://github.com/iesl/narrative-driven-rec-mint/\",\n      \"paragraphs\": [\n        \"\\n          Besides creating queries for ad-hoc retrieval tasks, concurrent\\nwork of Leszczynski et al. \\n          \",\n        \"\\n          Finally, while our work explores data augmentation from\\nuseritem interactions for a retrieval-oriented NDR task, prior work has\\nalso explored data augmentation of the user-item graph for training\\ncollaborative filtering models. This work has often explored\\naugmentation to improve recommendation performance for minority\\n\\n          \",\n        \"\\n          Complex Queries in Information Access. With the advent\\nof performant models for text understanding, focus on complex\\nand interactive information access tasks has seen a resurgence\\n\\n          \",\n        \"\\n          Besides this, a range of work has explored more complex,\\nlongform, and interactive query formulations for information access;\\nthese resemble queries in NDR. Arguello et al. \\n          \"\n      ]\n    },\n    {\n      \"title\": \"Proposed Method\",\n      \"paragraphs\": [\n        \"\\n        Our proposed method, Mint, for NDR, re-purposes a dataset of\\n\\nabundantly available user-item interactions, I = {(, { }=1)} into\\ntraining data for retrieval models by using LLMs as query\\ngeneration models to author narrative queries  : D = {(, { }=1)}.\\nThen, retrieval models are trained on the synthetic dataset D\\n(Figure 3).\\n3.2.1 Narrative Queries from LLMs. To author a narrative query \\nfor a user in I, we make use of the 175B parameter InstructGPT4\\nmodel as our query generation model QGen. We include the text\\nof interacted items { }=1 in the prompt for QGen, and instruct it\\nto author a narrative query (Figure 2). To improve the coherence\\nof generated queries and obtain correctly formatted outputs, we\\nmanually author narrative queries for 3 topically diverse users\\nbased on their interacted items and include it in the prompt for\\nQGen. The same three few shot examples are used for the whole\\ndataset I, and the three users were chosen from I. Generating\\nnarrative queries based on user interactions may also be considered\\na form of multi-document summarization for generating a natural\\nlanguage user profile \\n        \",\n        \"q and d . Embeddings are obtained by averaging token embeddings\\nfrom the final layer of MPNet, and the same model is used for both\\nqueries and items. Cross-encoder models input both the query and\\nitem and output a score to be used for ranking  = Cr (  ;  ),\\nwhere Cr is parameterized as w dropout W MPNet(\\u00b7) . We\\ntrain our bi-encoder model with a margin ranking loss: L =\\n\\u00cd \\u00cd=1 max2(q, d ) \\u2212 2(q, d\\u2032 ) + , 0 with randomly\\nsampled negatives \\u2032 and  = 1. Our cross-encoders are trained with\\na cross-entropy loss: L = \\u00cd \\u00cd=1 log( \\u00cd\\u2032\\u2032 ). For training, 4\\nnegative example items \\u2032 are randomly sampled from ranks\\n100300 from our trained bi-encoder. At test time, we retrieve the top\\n200 items with our trained bi-encoder and re-rank them with the\\ncross-encoder - we evaluate both these components in experiments\\nand refer to them as BiEnc-Mint and CrEnc-Mint.\\n4\"\n      ]\n    },\n    {\n      \"title\": \"4https://platform.openai.com/docs/models/gpt-3, text-davinci-003\",\n      \"paragraphs\": [\n        \"q and d . Embeddings are obtained by averaging token embeddings\\nfrom the final layer of MPNet, and the same model is used for both\\nqueries and items. Cross-encoder models input both the query and\\nitem and output a score to be used for ranking  = Cr (  ;  ),\\nwhere Cr is parameterized as w dropout W MPNet(\\u00b7) . We\\ntrain our bi-encoder model with a margin ranking loss: L =\\n\\u00cd \\u00cd=1 max2(q, d ) \\u2212 2(q, d\\u2032 ) + , 0 with randomly\\nsampled negatives \\u2032 and  = 1. Our cross-encoders are trained with\\na cross-entropy loss: L = \\u00cd \\u00cd=1 log( \\u00cd\\u2032\\u2032 ). For training, 4\\nnegative example items \\u2032 are randomly sampled from ranks\\n100300 from our trained bi-encoder. At test time, we retrieve the top\\n200 items with our trained bi-encoder and re-rank them with the\\ncross-encoder - we evaluate both these components in experiments\\nand refer to them as BiEnc-Mint and CrEnc-Mint.\\n4\"\n      ]\n    },\n    {\n      \"title\": \"EXPERIMENTS AND RESULTS\",\n      \"paragraphs\": [\n        \"Next, we evaluate Mint on a publicly available test collection for\\nNDR and present a series of ablations.\\n4.1\",\n        \"\\n        Experimental Setup\\n4.1.1 Datasets. We perform evaluations on an NDR dataset for\\npoint-of-interest (POI) recommendation Pointrec \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Results\",\n      \"paragraphs\": [\n        \"Table 1 presents the performance of the proposed method compared\\nagainst baselines. Here, bold numbers indicate the best-performing\\nmodel, and superscripts indicate statistical significance computed\\nwith two-sided t-tests at  < 0.05.\",\n        \"\\n        Here, we first note the performance of baseline approaches. We\\nsee BM25 outperformed by Contriver, a transformer bi-encoder\\nmodel trained for zero-shot retrieval; this mirrors prior work \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Ablations\",\n      \"paragraphs\": [\n        \"In Table 2, we ablate various design choices in Mint. Diferent\\nchoices result in diferent training sets for the BiEnc and CrEnc\\nmodels. Also, note that in reporting ablation performance for CrEnc,\\nwe still use the performant BiEnc-Mint model for obtaining\\nnegative examples for training and first-stage ranking. Without\\nhighquality negative examples, we found CrEnc to result in much poorer\\nperformance.\",\n        \"No item fil tering. Since synthetic queries are unlikely to\\nrep\\nresent all the items of a user, Mint excludes user items { }=1\\nwhich have a low likelihood of being generated from the document\\n(\\u00a73.2.2). Without this step, we expect the training set for training\\nretrieval models to be larger and noisier. In Table 2, we see that\\nexcluding this step leads to a lower performance for BiEnc and\\nCrEnc, indicating that the quality of data obtained is important for\\nperformance.\",\n        \"\\n        6B LLM for QGen. Mint relies on using an expensive 175B\\nparameter InstructGPT model for QGen. Here, we investigate the\\neficacy for generating  for { }=1 with a 6B parameter\\nInstructGPT model (text-curie-001). We use an identical setup to the\\n175B LLM for this. In Table 2, we see that training on the synthetic\\nnarrative queries of the smaller LLM results in worse models \\u2013\\noften underperforming the baselines in Table 1. This indicates the\\ninability of a smaller model to generate complex narrative queries\\nwhile conditioning on a set of user items. This necessity of a larger\\nLLM for generating queries in complex retrieval tasks has been\\nobserved in prior work \\n        \",\n        \"\\n          MAP\\n6B LLM for Item Queries. We find a smaller 6B LLM to result\\nin poor quality data when used to generate narrative queries\\nconditioned on { }=1. Here we simplify the text generation task \\u2013\\nusing a 6B LLM to generate queries for individual items  . This\\nexperiment also mirrors the setup for generating synthetic queries\\nfor search tasks \\n          \"\n      ]\n    },\n    {\n      \"title\": \"Pointrec\",\n      \"paragraphs\": [\n        \"\\n          MAP\\n6B LLM for Item Queries. We find a smaller 6B LLM to result\\nin poor quality data when used to generate narrative queries\\nconditioned on { }=1. Here we simplify the text generation task \\u2013\\nusing a 6B LLM to generate queries for individual items  . This\\nexperiment also mirrors the setup for generating synthetic queries\\nfor search tasks \\n          \"\n      ]\n    },\n    {\n      \"title\": \"CONCLUSIONS\",\n      \"paragraphs\": [\n        \"In this paper, we present Mint, a data augmentation method for the\\nnarrative-driven recommendation (NDR) task. Mint re-purposes\\nhistorical user-item interaction datasets for NDR by using a 175B\\nparameter large language model to author long-form narrative queries\\nwhile conditioning on the text of items liked by users. We evaluate\\nbi-encoder and cross-encoder models trained on data from Mint on\\nthe publicly available Pointrec test collection for narrative-driven\\npoint of interest recommendation. We demonstrate that the\\nresulting models outperform several strong baselines and ablated models\\nand match or outperform a 175B LLM directly used for NDR in a\\n1-shot setup.\",\n        \"However, Mint also presents some limitations. Given our use of\\nhistorical interaction datasets for generating synthetic training data\\nand the prevalence of popular interests in these datasets longer,\\ntailed interests are unlikely to be present in the generated\\nsynthetic datasets. In turn, causing retrieval models to likely see poorer\\nperformance on these requests. Our use of LLMs to generate\\nsynthetic queries also causes the queries to be repetitive in structure,\\nlikely causing novel longer-tail queries to be poorly served. These\\nlimitations may be addressed in future work.\",\n        \"MAP\",\n        \"Recall@100\",\n        \"Besides this, other avenues also present rich future work. While\\nMint leverages a 175B LLM for generating synthetic queries, smaller\\nparameter LLMs may be explored for this purpose - perhaps by\\ntraining dedicated QGen models. Mint may also be expanded to\\nexplore more active strategies for sampling items and users for\\nwhom narrative queries are authored - this may allow more\\neficient use of large parameter LLMs while ensuring higher quality\\ntraining datasets. Next, the generation of synthetic queries from\\nsets of documents may be explored for a broader range of retrieval\\ntasks beyond NDR given its promise to generate larger training\\nsets \\u2013 a currently underexplored direction. Finally, given the lack of\\nlarger-scale test collections for NDR and the efectiveness of LLMs\\nfor authoring narrative queries from user-item interaction, fruitful\\nfuture work may also explore the creation of larger-scale datasets\\nin a mixed-initiative setup to robustly evaluate models for NDR.\"\n      ]\n    },\n    {\n      \"title\": \"ACKNOWLEDGMENTS\",\n      \"paragraphs\": [\n        \"We thank anonymous reviewers for their invaluable feedback. This\\nwork was partly supported by the Center for Intelligent\\nInformation Retrieval, NSF grants IIS-1922090 and 2143434, the Ofice of\\nNaval Research contract number N000142212688, an Amazon Alexa\\nPrize grant, and the Chan Zuckerberg Initiative under the project\\nScientific Knowledge Base Construction. Any opinions, findings\\nand conclusions or recommendations expressed here are those of\\nthe authors and do not necessarily reflect those of the sponsors.\"\n      ]\n    }\n  ],\n  \"references\": [\n    \"Ref ID: ref1, Given Names: Jafar Krisztian, Surname: Afzali Balog, Year: 2021, Article Title: POINTREC: A Test Collection for Narrative-Driven Point of Interest Recommendation, Source: In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21), Volume: None, Issue: None, First Page: 2478, Last Page: 2484\",\n    \"Ref ID: ref2, Given Names: Jaime Fernando, Surname: Arguello Diaz, Year: 2021, Article Title: Tip of the Tongue Known-Item Retrieval: A Case Study in Movie Identification, Source: In Proceedings of the 6th international ACM SIGIR Conference on Human Information Interaction and Retrieval, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref3, Given Names: Toine Mette, Surname: Bogers Skov, Year: 2018, Article Title: What was this Movie About this Chick?\\u201d A Comparative Study of Relevance Aspects in Book and Movie Discovery, Source: In Transforming Digital Worlds: 13th International Conference, Volume: 25, Issue: None, First Page: 323, Last Page: 334\",\n    \"Ref ID: ref4, Given Names: Toine Mette, Surname: Bogers Skov, Year: 2019, Article Title: Looking for an amazing game I can relax and sink hours into, Source: None, Volume: None, Issue: None, First Page: 503, Last Page: 515\",\n    \"Ref ID: ref5, Given Names: Toine Marijn, Surname: Bogers Koolen, Year: 2017, Article Title: Defining and Supporting Narrative-Driven Recommendation, Source: In Proceedings of the Eleventh ACM Conference on Recommender Systems (Como, Volume: None, Issue: None, First Page: 238, Last Page: 242\",\n    \"Ref ID: ref6, Given Names: Toine Marijn, Surname: Bogers Koolen, Year: 2018, Article Title: \\u201cI'm looking for something like, Source: In Knowledge-aware and Conversational Recommender Systems Workshop. CEUR Workshop Proceedings., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref7, Given Names: Luiz Rodrigo, Surname: Bonifacio Nogueira, Year: 2022, Article Title: InPars: Unsupervised Dataset Generation for Information Retrieval, Source: In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Volume: None, Issue: None, First Page: 2387, Last Page: 2392\",\n    \"Ref ID: ref8, Given Names: Leonid Eric, Surname: Boytsov Nyberg, Year: 2023, Article Title: InPars-Light: Cost-Efective Unsupervised Training of Eficient Rankers, Source: None, Volume: 2301, Issue: None, First Page: 02998, Last Page: None\",\n    \"Ref ID: ref9, Given Names: Tom Mateusz McCandlish Alec Dario M. R. M.F., Surname: Brown Jared D Kaplan Litwin Sam Radford Amodei Ranzato Hadsell Balcan, Year: 2020, Article Title: Language Models are Few-Shot Learners, Source: In Advances in Neural Information Processing Systems, Volume: 33, Issue: None, First Page: 1877, Last Page: 1901\",\n    \"Ref ID: ref10, Given Names: Chris, Surname: Buckley and Ellen M. Voorhees, Year: 2004, Article Title: Retrieval Evaluation with Incomplete Information, Source: In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Shefield, United Kingdom) (SIGIR '04), Volume: None, Issue: None, First Page: 25, Last Page: 32\",\n    \"Ref ID: ref11, Given Names: Chae, Surname: Dong-Kyu Sang-Wook Kim, Year: 2020, Article Title: ARCF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems, Source: In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20), Volume: None, Issue: None, First Page: 1251, Last Page: 1260\",\n    \"Ref ID: ref12, Given Names: Chen Meng, Surname: Lei Le Wu Wang, Year: 2023, Article Title: Improving Recommendation Fairness via Data Augmentation, Source: In Proceedings of the ACM Web Conference, Volume: None, Issue: None, First Page: 1012, Last Page: 1020\",\n    \"Ref ID: ref13, Given Names: Chen Lehong, Surname: Li Zhao, Year: 2022, Article Title: A Pilot Study for Understanding Users' Attitudes Towards a Conversational Agent for News Recommendation, Source: In Proceedings of the 4th Conference on Conversational User Interfaces (Glasgow, Volume: 36, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref14, Given Names: Hyung William Eric Xuezhi Siddhartha, Surname: Won Chung Fedus Li Wang Brahma, Year: 2022, Article Title: Scaling instruction-finetuned language models, Source: arXiv preprint arXiv:2210.11416, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref15, Given Names: Dai, Surname: Zhuyun Ming-Wei Chang, Year: 2023, Article Title: Promptagator: Few-shot Dense Retrieval From 8 Examples, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref16, Given Names: Shyam, Surname: Abhinandan S. Das Mayur Datar Rajaram, Year: 2007, Article Title: Google News Personalization: Scalable Online Collaborative Filtering, Source: In Proceedings of the 16th International Conference on World Wide Web (Banf, Volume: None, Issue: None, First Page: 271, Last Page: 280\",\n    \"Ref ID: ref17, Given Names: Davidson He Dasarathi, Surname: James Ullas Gargi Yu Sampath, Year: 2010, Article Title: The YouTube Video Recommendation System, Source: In Proceedings of the Fourth ACM Conference on Recommender Systems (Barcelona, Volume: None, Issue: None, First Page: 293, Last Page: 296\",\n    \"Ref ID: ref18, Given Names: Eberhard Denis, Surname: Lukas Helic, Year: 2019, Article Title: Evaluating Narrative-Driven Movie Recommendations on Reddit, Source: In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, Volume: None, Issue: None, First Page: 1, Last Page: 11\",\n    \"Ref ID: ref19, Given Names: Gao and Jamie, Surname: Luyu Callan, Year: 2022, Article Title: Precise Zero-Shot Dense Retrieval without Relevance Labels, Source: arXiv preprint arXiv:2212.10496, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref20, Given Names: Hariri Robin, Surname: Negar Burke, Year: 2013, Article Title: Query-Driven Context Aware Recommendation, Source: In Proceedings of the 7th ACM Conference on Recommender Systems (Hong Kong, China) (RecSys '13), Volume: None, Issue: None, First Page: 9, Last Page: 16\",\n    \"Ref ID: ref21, Given Names: Seyyed, Surname: Hadi Hashemi, Year: 2016, Article Title: Overview of the TREC 2016 Contextual Suggestion Track., Source: Charles LA Clarke, and Ellen M Voorhees, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref22, Given Names: Izacard Sebastian Edouard, Surname: Gautier Riedel Grave, Year: 2022, Article Title: Unsupervised Dense Information Retrieval with Contrastive Learning, Source: Transactions on Machine Learning Research, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref23, Given Names: Jeronymo Rodrigo, Surname: Vitor Nogueira, Year: 2023, Article Title: InPars-v2: Large Language Models as Eficient Dataset Generators for Information Retrieval, Source: arXiv:2301, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref24, Given Names: Koolen David, Surname: Marijn Walsh Interaction, Year: 2016, Article Title: Overview of the CLEF 2016 Social Book Search Lab, Source: None, Volume: None, Issue: None, First Page: 351, Last Page: 370\",\n    \"Ref ID: ref25, Given Names: Leszczynski, Surname: Megan, Year: 2023, Article Title: Generating Synthetic Data for Conversational Music Recommendation Using Random Walks and Language Models, Source: None, Volume: 2301, Issue: None, First Page: 11489, Last Page: None\",\n    \"Ref ID: ref26, Given Names: Liu Chunyan, Surname: Xin Miao, Year: 2013, Article Title: Personalized Point-ofInterest Recommendation by Mining Users' Preference Transition, Source: In Proceedings of the 22nd ACM International Conference on Information & Knowledge, Volume: None, Issue: None, First Page: 733, Last Page: 738\",\n    \"Ref ID: ref27, Given Names: Liu Pham Quan, Surname: Yiding Tuan-Anh Nguyen Gao Cong Yuan, Year: 2017, Article Title: An Experimental Evaluation of Point-of-Interest Recommendation in Location-Based Social Networks, Source: Proc. VLDB Endow, Volume: 10, Issue: 10, First Page: 1010, Last Page: 1021\",\n    \"Ref ID: ref28, Given Names: L\\u00f3pez Michael Lucas, Surname: Federico Strube Dixon, Year: 2021, Article Title: Augmenting the user-item graph with textual similarity models, Source: arXiv preprint arXiv:2109.09358, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref29, Given Names: Xing Siva, Surname: Han Lu Reddy, Year: 2023, Article Title: The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents, Source: In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, Volume: None, Issue: None, First Page: 2799, Last Page: 2829\",\n    \"Ref ID: ref30, Given Names: Kai Hanze and Hojin, Surname: Luo Li Yang, Year: 2020, Article Title: Latent Linear Critiquing for Conversational Recommender Systems, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref31, Given Names: Ma, Surname: Ji Ryan McDonald, Year: 2021, Article Title: Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation, Source: In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:, Volume: None, Issue: None, First Page: 1075, Last Page: 1088\",\n    \"Ref ID: ref32, Given Names: Mysore and Hamed, Surname: Sheshera Tim O'Gorman Andrew McCallum Zamani, Year: 2021, Article Title: CSFCube - A Test Collection of Computer Science Research Articles for Faceted Query by Example, Source: In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round, Volume: 2, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref33, Given Names: Ouyang Luke Peter Ryan S. A. D. K., Surname: Long Miller Welinder Jan Leike Lowe Mohamed Agarwal Belgrave Cho A, Year: 2022, Article Title: Training language models to follow instructions with human feedback, Source: In Advances in Neural Information Processing Systems, Volume: 35, Issue: None, First Page: 27730, Last Page: 27744\",\n    \"Ref ID: ref34, Given Names: Papenmeier Norbert ACT, Surname: Andrea Fuhr Retrieval (Canberra, Year: 2021, Article Title: Starting Conversations with Search Engines - Interfaces That Elicit Natural Language Queries, Source: In Proceedings of the 2021 Conference on Human Information Interaction, Volume: None, Issue: None, First Page: 261, Last Page: 265\",\n    \"Ref ID: ref35, Given Names: Penha Alice Hugues, Surname: Gustavo Wang Bouchard, Year: 2023, Article Title: Improving Content Retrievability in Search with Controllable Query Generation, Source: In Proceedings of the ACM Web Conference, Volume: None, Issue: None, First Page: 3182, Last Page: 3192\",\n    \"Ref ID: ref36, Given Names: Radlinski Ben, Surname: Filip Wedin, Year: 2022, Article Title: On Natural Language User Profiles for Transparent and Scrutable Recommendation, Source: In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Volume: None, Issue: None, First Page: 2863, Last Page: 2874\",\n    \"Ref ID: ref37, Given Names: Nils Iryna, Surname: Reimers Gurevych, Year: 2019, Article Title: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, Source: In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref38, Given Names: Stephen Hugo Beyond. Found. Trends, Surname: Robertson Zaragoza Inf, Year: 2009, Article Title: The Probabilistic Relevance Framework: BM25 and, Source: Retr. 3, Volume: None, Issue: 4, First Page: 333, Last Page: 389\",\n    \"Ref ID: ref39, Given Names: Jon Christopher, Surname: Saad-Falcon Potts, Year: 2023, Article Title: UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers, Source: None, Volume: 2303, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref40, Given Names: Sachan Mike Luke, Surname: Devendra Lewis Yih Zettlemoyer, Year: 2022, Article Title: Improving Passage Retrieval with Zero-Shot Question Generation, Source: In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Volume: None, Issue: None, First Page: 3781, Last Page: 3797\",\n    \"Ref ID: ref41, Given Names: Song, Surname: Kaitao Tie-Yan Liu, Year: 2020, Article Title: MPNet: Masked and Permuted Pre-training for Language Understanding, Source: In Advances in Neural Information Processing Systems, Volume: 33, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref42, Given Names: Teevan Eric, Surname: Jaime Horvitz, Year: 2005, Article Title: Personalizing Search via Automated Analysis of Interests and Activities, Source: In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Salvador, Brazil) (SIGIR '05), Volume: None, Issue: None, First Page: 449, Last Page: 456\",\n    \"Ref ID: ref43, Given Names: Mengting, Surname: Wan and Julian McAuley, Year: 2018, Article Title: Item Recommendation on Monotonic Behavior Chains, Source: In Proceedings of the 12th ACM Conference on Recommender Systems, Volume: None, Issue: None, First Page: 86, Last Page: 94\",\n    \"Ref ID: ref44, Given Names: Wang Hongxia Jingrui, Surname: Haonan Chang Zhou Yang He, Year: 2021, Article Title: Controllable Gradient Item Retrieval, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref45, Given Names: Wang Lizhen, Surname: Qinyong Cui AK, Year: 2019, Article Title: Enhancing Collaborative Filtering with Generative Augmentation, Source: In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Anchorage, Volume: None, Issue: None, First Page: 548, Last Page: 556\",\n    \"Ref ID: ref46, Given Names: Xu Andrew Charles, Surname: Jiajing Zhai Rosenberg, Year: 2022, Article Title: Rethinking Personalized Ranking at Pinterest: An End-to-End Approach, Source: In Proceedings of the 16th ACM Conference on Recommender Systems, Volume: None, Issue: None, First Page: 502, Last Page: 505\",\n    \"Ref ID: ref47, Given Names: Ying Deqing Hongwei, Surname: Yuxin Wang Zheng, Year: 2023, Article Title: CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users in Recommendation, Source: In Proceedings of the ACM Web Conference, Volume: None, Issue: None, First Page: 1396, Last Page: 1404\",\n    \"Ref ID: ref48, Given Names: Zamani Filip, Surname: Hamed Radlinski, Year: 2022, Article Title: Conversational information seeking, Source: arXiv preprint arXiv:2201.08808, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref49, Given Names: Zou Evangelos, Surname: Jie Kanoulas, Year: 2020, Article Title: Towards Question-Based Recommender Systems, Source: In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20), Volume: None, Issue: None, First Page: 881, Last Page: 890\"\n  ]\n}",
    "{\n  \"Article_ID\": \"881c466d0163446fa511df0667d11b37\",\n  \"DocumentTitle\": \"Framing the News: From Human Perception to Large Language Model Inferences\",\n  \"Auteurs\": \"David Alonso del Barrio, Daniel Gatica-Perez, \",\n  \"Institutions\": \"Idiap Research Institute and EPFL, Idiap Research Institute, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"CCS CONCEPTS\",\n      \"paragraphs\": [\n        \"\\u2022 Computing methodologies \\u2192 Information extraction; \\u2022\\nHuman-centered computing \\u2192 Text input.\\nCovid-19 no-vax, news framing, GPT-3, prompt-engineering,\\ntransformers, large language models\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\",\n        \"ICMR \\u201923, June 12\\u201315, 2023, Thessaloniki, Greece\\n\\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-0178-8/23/06. . . $15.00\\nhttps://doi.org/10.1145/3591106.3592278\"\n      ]\n    },\n    {\n      \"title\": \"INTRODUCTION\",\n      \"paragraphs\": [\n        \"\\n        In recent years, there has been a proliferation in the use of concepts\\nsuch as data journalism, computational journalism, and\\ncomputerassisted reporting \\n        \",\n        \"Frame analysis is a concept from journalism, which consists of\\nstudying the way in which news stories are presented on an issue,\\nand what aspects are emphasized: Is a merely informative vision\\ngiven in an article? Or is it intended to leave a moral lesson? Is\\na news article being presented from an economic point of view?\\nOr from a more human, emotional angle? The examples above\\ncorrespond to diferent frames with which an article can be written.\",\n        \"\\n        The concept of news framing has been studied in computing as\\na step beyond topic modeling and sentiment analysis, and for this\\npurpose, in recent years, pre-trained language models have been\\nused for fine-tuning the classification process of these frames \\n        \",\n        \"Our work aims to address this research gap by posing the\\nfollowing research questions:\",\n        \"RQ1: What are the main frames in the news headlines about\\nthe anti-vaccine movement, as reported in newspapers across 5\\nEuropean countries?\",\n        \"RQ2: Can prompt engineering be used for classification of\\nheadlines according to frames?\",\n        \"By addressing the above research questions, our work makes the\\nfollowing contributions:\",\n        \"Contribution 1. We implemented a process to do human\\nannotation of the main frame of 1786 headlines of articles about the\\nCovid-19 no-vax movement, as reported in 19 newspapers from 5\\nEuropean countries (France, Italy, Spain, Switzerland and United\\nKingdom.) At the headline level, we found that the predominant\\nframe was human interest, where this frame corresponds to a\\npersonification of an event, either through a statement by a person,\\nor the explanation of a specific event that happened to a person.\\nFurthermore, we found a large number of headlines annotated as\\ncontaining no frame, as they simply present information without\\nentering into evaluations. We also found that for all the countries\\ninvolved, the distribution of frame types was very similar, i.e.,\\nhuman interest and no frame are the two predominant frames. Finally,\\nthe generated annotations allowed to subsequently study the\\nperformance of a large language model.\",\n        \"Contribution 2. We studied the performance of GPT-3.5 on\\nthe task of frame classification of headlines. In addition to using\\nthe fine-tuning approach from previous literature, we propose an\\nalternative approach for frame classification that requires no labeled\\ndata for training, namely prompt-engineering using GPT-3.5. The\\nresults show that fine-tuning with GPT-3.5 produces 72% accuracy\\n(slightly higher than other smaller models), and that the\\npromptengineering approach results in lower performance (49% accuracy.)\\nOur analysis also shows that the subjectivity of the human labeling\\ntask has an efect on the obtained accufracy.\",\n        \"The paper is organized as follows. In Section 2, we discuss related\\nwork. In Section 3, we describe the news dataset. In Section 4, we\\ndescribe the methodology for both human labeling and machine\\nclassification of news frames. We present and discuss results for\\nRQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide\\nconclusions in Section 7.\\n2\"\n      ]\n    },\n    {\n      \"title\": \"RELATED WORK\",\n      \"paragraphs\": [\n        \"\\n        Framing has been a concept widely studied in journalism, with a\\ndefinition that is rooted in the study of this domain \\n        \",\n        \"\\n        For frame recognition, there are two main approaches: the\\ninductive approach \\n        \",\n        \"\\n        We now compare the two approaches on a common topic, such\\nas Covid-19. Ebrahim et al. \\n        \",\n        \"We decided to follow the deductive approach because a\\npredeifned list of frames allows to compare among topics, countries,\\nprevious literature, and also because they represent a fixed list of\\nlabels for machine classification models. Furthermore, the\\ninductive approach tends to be more specific to a topic, and from the\\ncomputing viewpoint, past work has tried to justify topic modeling\\nas a technique to extract frames from articles.\",\n        \"\\n        Yl\\u00e4-Antitila et al. \\n        \",\n        \"\\n        From Entman\\u2019s definition of frame \\n        \",\n        \"\\n        Isoaho et al.\\n        \",\n        \"\\n        We also consider that the larger the number of possible frame\\ntypes, the more likely it is to end up doing topic modeling instead of\\nframe analysis. Using a deductive approach, Dallas et al. \\n        \",\n        \"\\n        A final decision in our work was the type of text to analyze,\\nwhether headlines or whole article. For this decision, the chosen\\nclassification method was also going to be important. For example,\\nKhanehzar et al. \\n        \",\n        \"\\n        Continuing with the question of the methods used for\\nclassiifcation, much work has been developed in prompt engineering,\\nespecially since the release of GPT-3. Liu et al.\\n        \",\n        \"\\n        As mentioned before, the emergence of giant models like GPT-3,\\nBLOOM, and ChatGPT are a very active research topic. To the best\\nof our knowledge, on one hand our work extends the computational\\nanalysis of news related to the covid-19 no-vax movement, which\\nillustrates the influence of the press on the ways societies think\\nabout relevant issues \\n        \"\n      ]\n    },\n    {\n      \"title\": \"3 DATA: EUROPEAN COVID-19 NEWS\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"DATASET\",\n      \"paragraphs\": [\n        \"\\n        We used part of the European Covid-19 News dataset collected in\\nour recent work \\n        \",\n        \"In the first phase, annotators had to read the codebook and get\\nfamiliar with the task. In the second phase, they were asked to\\nidentify the main frame in the same subset of 50 headlines. At the\\nend of the second phase, the intercoder reliability (ICR) was 0.58\\nbetween the 2 annotators. We analyzed those cases where there\\nwere discrepancies, and observed that in some cases, there was not a\\nunique main frame, because both annotators had valid arguments to\\nselect one of the frames. In other cases, the discrepancies were due\\nto slight misunderstanding of the definitions. In the third phase, the\\nannotators coded again 50 headlines, and the ICR increased to was\\n0.66. We realized that the possibility of having two frames remained.\\nThey discussed the cases in which they had disagreed, and if the\\nother person\\u2019s arguments were considered valid, it could be said that\\nthere were two frames. After this three-phase training procedure,\\nannotators were ready to annotate the dataset independently. We\\ndivided the dataset into two equal parts, and each person annotated\\n893 headlines.\\n4.2\"\n      ]\n    },\n    {\n      \"title\": \"Fine-tuning GPT-3.5 and BERT-based models\",\n      \"paragraphs\": [\n        \"With the annotated dataset, we investigated two NLP approaches:\\nthe first one involves fine-tuning a pre-trained model; the second\\none is prompt engineering. Pre-trained language models have been\\ntrained with large text strings based on two unsupervised tasks,\\nnext sentence prediction and masked language model. Figure 1\\nsummarizes these techniques.\",\n        \"\\n        In the first approach, a model with a fixed architecture is\\npretrained as a language model (LM), predicting the likelihood of the\\nobserved textual data. This can be done due to the availability of\\nlarge, raw text data needed to train LMs. This learning process can\\nproduce general purpose features of the modeled language. The\\nlearning process produces robust, general-purpose features of the\\nlanguage being modeled. The above pre-trained LM is then adapted\\nto diferent downstream tasks, by introducing additional parameters\\nand adjusting them using task-specific objective functions. In this\\napproach, the focus was primarily on goal engineering, designing\\nthe training targets used in both the pre-training and the fine-tuning\\nstages \\n        \",\n        \"We present an example to illustrate the idea. Imagine that the\\ntask is sentiment analysis, and we have a dataset with sentences\\nand their associated sentiment, and a pre-trained model, which is a\\nsaved neural network trained with a much larger dataset. For that\\npre-trained model to address the target task, we unfreeze a few of\\nthe top layers of the saved model base and jointly train both the\\nnewly-added classifier layers and the last layers of the base model.\\nThis allows to \\\"fine-tune\\\" the higher-order feature representations\\nin the base model to make them more relevant for the sentiment\\nanalysis task. In this way, instead of having to obtain a very large\\ndataset with target labels to train a model, we can reuse the\\npretrained model and use a much smaller train dataset. We use a part\\nof our dataset as examples for the model to learn the task, while\\nthe other part of the dataset is used to evaluate model performance.\",\n        \"Previous works related to frame classification in the computing\\nliterature have used fine-tuning, BERT-based models. In our work,\\nwe have done the same as a baseline, but we aimed to go one step\\nfurther and also produce results using fine-tuning of GPT-3.5.\\n4.3\"\n      ]\n    },\n    {\n      \"title\": \"Prompt-engineering with GPT-3.5\",\n      \"paragraphs\": [\n        \"\\n        Model fine-tuning has been widely used, but with the emergence\\nof generative models such as GPT-3, another way to approach\\nclassification tasks has appeared. The idea is to use the pre-trained\\nmodel directly and convert the task to be performed into a format\\nas close as possible to the tasks for which it has been pre-trained.\\nThat is, if the model has been pre-trained from next word prediction\\nas in the case of GPT-3, classification can be done by defining a\\nprompt, where the input to the model is an incomplete sentence,\\nand the model must complete it with a word or several words, just\\nas it has been trained. This avoids having to use part of the already\\nlabeled dataset to teach the task to be performed to the model, and\\na previous labeling is not needed \\n        \",\n        \"\\n        In this approach, instead of adapting pre-trained LMs to\\ndownstream tasks via objective engineering, downstream tasks are\\nreformulated to look more like those solved during the original LM\\ntraining with the help of a textual prompt. For example, when\\nrecognizing the emotion of a social media post, \\u201cI missed the bus today.\\u201d,\\nwe may continue with a prompt \\u201cI felt so _\\u201d, and ask the LM to\\nifll the blank with an emotion-bearing word. Or if we choose the\\nprompt \\u201cEnglish: I missed the bus today. French: _\\u201d), an LM may\\nbe able to fill in the blank with a French translation. In this way,\\nby selecting the appropriate prompts, we can influence the model\\nbehavior so that the pre-trained LM itself can be used to predict the\\ndesired output, even without any additional task-specific training\\n\\n        \",\n        \"\\n        We use this emerging NLP approach to classify frames at headline\\nlevel. We are not aware of previous uses of this strategy to classify\\nframes as we propose here. The idea is the following. Prompt\\nengineering consists of giving a prompt to the model, and understands\\nthat prompt as an incomplete sentence. To do prompt\\nengineering with our dataset, we needed to define an appropriate prompt\\nthat would produce the headline frames as output. We defined\\nseveral experiments with the Playground of GPT-3, in order to find\\nthe best prompt for our task. In our initial experiments, we\\nfollowed existing approaches in prompt engineering to do sentiment\\nanalysis, where the individual answer was an adjective, and this\\nadjective was matched with a sentiment. In a similar fashion, we\\ndecided to build a thesaurus of adjectives that define each of the\\nframes. For instance, the human interest frame could be\\n\\u2019interesting\\u2019, \\u2019emotional\\u2019, \\u2019personal\\u2019, \\u2019human\\u2019. The conflict frame could be:\\n\\u2019conflictive\\u2019, \\u2019bellicose\\u2019, \\u2019troublesome\\u2019, \\u2019rowdy\\u2019, \\u2019quarrelsome\\u2019,\\n\\u2019troublemaker\\u2019, \\u2019agitator\\u2019, etc. After the list of adjectives was defined,\\nwe needed to define the prompt in order to get, as an answer, one\\nof the adjectives in our thesaurus to match them with the frame.\\nWe used the GPT-3 playground using the headline as input and\\nasking for the frame as output, but the strategy did not work. In\\nour final experiment, instead of giving the headline as input, we\\ngave the definitions of each type of frame plus the headline, and we\\nasked the model to choose between the diferent types of frames\\nas output. In this way, the output of the model was directly one of\\nthe frames, and we avoided the step of matching adjectives with\\nframes. An example is shown in Figure 2.\\nFor the GPT-3 configuration 1, there are 3 main concepts:\\n\\u2022 TEMPERATURE \\n        \",\n        \"After testing with the GPT-3 playground and varying diferent\\nhyper-parameters to assess performance, we set the temperature to\\n0, since the higher the temperature the more random the response.\\nFurthermore, the Top-p parameter was set to 1, as it would likely\\nget a set of the most likely words for the model to choose from. The\\nmaximum number of tokens was set to 2; in this way, the model\\nis asked to choose between one of the responses. As a model, we\\nused the one with the best performance at the time of experimental\\ndesign, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\\n5\"\n      ]\n    },\n    {\n      \"title\": \"RESULTS: HUMAN LABELING OF FRAMES\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"IN NO-VAX NEWS HEADLINES (RQ1)\",\n      \"paragraphs\": [\n        \"In this section, we present and discuss the results of the analysis\\nrelated to our first RQ.\",\n        \"Figure 3 shows the distribution of frames per country at headline\\nlevel, with human interest and no-frame being the predominant\\n1https://beta.openai.com/docs/introduction\\nones. Attribution of responsibility is the third one except in\\nSwitzerland, where the corresponding frame is conflict. Finally, morality\\nand economic are the least represented in the dataset for every\\ncountry.\",\n        \"The monthly distribution of frames aggregated for all countries\\nis shown in Fig. 4. We can see two big peaks, the first one in January\\n2021 and the second one in August 2021. In all countries, the\\nvaccination process started at the end of December 2020, so it makes\\nsense that the no-vax movement started to be more predominant in\\nthe news in January 2021. Human interest is the most predominant\\nframe. Manual inspection shows that this is because the headlines\\nare about personal cases of people who are pro- or anti- vaccine.\\nAttribution of responsibility is also present. Manual inspection\\nindicates that local politicians and health authorities had to make\\ndecisions about who could be vaccinated at the beginning of the\\nprocess. The second peak at the end of summer 2021 coincided\\nwith the health pass (also called Covid passport in some countries),\\nand we can observe a peak in the curve corresponding to the\\nconlfict frame, reflecting the demonstrations against the measure of\\nmandatory health passes taken by country governments.\",\n        \"\\n        In Figure 5, we compare the sentiment per frame and per country,\\nto understand if there were any major diferences. The sentiment\\nanalysis labels were obtained using BERT-sent from the Hugging\\nFace package \\n        \",\n        \"Switzerland, and the United Kingdom,\\n\\u2022 No frame: 20-30% of negative content.\",\n        \"Regarding the results of the annotation process, the fact that the\\ndistribution of the 6 frame types is relatively similar between\\ncountries suggests that the anti-vaccine movement issue was treated\\nin a similar way in these countries. The fact that human interest\\nis the most dominant frame indicates that this issue was treated\\nfrom a more human and emotional approach, with headlines about\\npersonal experiences, celebrities giving their opinion about\\nvaccination, and politicians defending vaccine policies. Moreover, the\\nreason for many headlines being classified as no-frame is partly\\ndue to how data was selected. We chose articles that contained\\nwords related to no-vax, either in the headline or in the article. This\\nresulted in many headlines not containing anything specific related\\nto no-vax, while the no-vax content was actually included in the\\nmain text of the corresponding articles.\",\n        \"It is worth mentioning that prior to obtaining the results, we had\\nexpected that attribution of responsibility would be among the most\\nprominent frames, since governments took many measures such as\\nmandatory health pass requirements to access certain sites; we had\\nalso expected that the conflict frame would be prominent, since\\nthere were many demonstrations in Europe. In reality, however,\\nthese frames categories were not reflected as frequently at the\\nheadline level.\",\n        \"Regarding the analysis at the temporal level, it is clear that certain\\nevents were captured by the press, such as the start of vaccination\\nor the mandatory vaccination passport.\",\n        \"\\n        Finally, the sentiment analysis of the diferent frames shows that\\nthe predominant tone in all of them is neutral or negative, with very\\nsimilar trends between countries. This association between\\nsentiment analysis and frames has been discussed in previous literature\\n\\n        \"\n      ]\n    },\n    {\n      \"title\": \"RESULTS: GPT-3.5 FOR FRAME\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"CLASSIFICATION OF HEADLINES (RQ2)\",\n      \"paragraphs\": [\n        \"Here, we present and discuss the results related to our second RQ.\\n6.1\"\n      ]\n    },\n    {\n      \"title\": \"Fine-tuning GPT-3.5\",\n      \"paragraphs\": [\n        \"Table 4 shows the results of the 6-class classification task using\\n5-cross validation. Three models were used: GPT-3.5 and two\\nBERTbased models. We observe that, on average, GPT-3.5 performs better\\nthan the BERT-based models. This is somehow expected as\\nGPT3.5 is a much larger model. Overall, in the case of fine-tuning, the\\nbest performance for the six-class frame classification task is 72%\\naccuracy, which is promising, with an improvement over previous\\nmodels based on BERT. Yet, it should be noted that the performance\\ndiferences are modest (2% improvement between GPT-3.5 and\\nRoBERTa).\",\n        \"On the other hand, BERT is open-source, while GPT-3 has an\\neconomic cost as the use of the model is not free, which monetarily\\nlimits the number of experiments that can be performed with it,\\nas well as the diferent configurations one can explore to improve\\nperformance. This is important because much of the improvement\\nin performance requires empirical explorations of model parameters\\nMore specifically, the cost of an experiment for each of the folds has\\na cost of 4 dollars (at the time of writing this paper.) This represents\\na limitation in practice.\",\n        \"\\n        Furthermore, GPT-3 has a significant carbon footprint. Similarly,\\nfor prompt engineering (discussed in the next subsection), choosing\\nthe right prompt (i.e., the words that best define the task so that the\\nmodel is able to perform adequately) is also based on trial and error.\\nThis also has an impact on carbon footprint. In connection with\\nthis topic, Strubell et al.\\n        \"\n      ]\n    },\n    {\n      \"title\": \"Prompt-engineering with GPT-3.5\",\n      \"paragraphs\": [\n        \"For each headline, we got the frame that the model considered the\\nmost likely, and we compared these GPT-3.5 inferences with the\\nframes labeled by the annotators. The agreement between model\\nand annotator was of 49%. Analyzing the results, and specifically\\nlooking at the cases where the annotator and GPT-3.5 disagreed,\\nwe discovered that according to the frame definitions, the model\\nin some cases proposed a frame that indeed made sense. This\\nobservation, together with our previous experience in the annotation\\nprocess, where headlines could have more than one valid frame,\\nled us to design a second post-hoc experiment. We took all the\\nheadlines where each of the two annotators had disagreed with\\nGPT-3.5, and we asked the annotators to state whether they would\\nagree (or not) with each GPT-inferred label for a given headline.\\nIt is important to emphasize that the annotators did not know the\\norigin of that label, i.e., they did not know if it was the label they\\nhad originally assigned, or if it was a random one. In this way, we\\ncould quantify how GPT-3.5 worked according to valid arguments\\nprovided by the annotators. In this post-hoc experiment, the model\\nagreed in 76% of cases with the annotators.\",\n        \"\\n        Looking at the results of the classification models, the 49%\\naccuracy of the prompt-engineering approach can be considered low,\\nyet we consider that it is a valid avenue for further investigation,\\nas in the second post-hoc analysis, we found that the model agrees\\nwith human annotators in 76% of the cases. Clearly, framing\\ninvolves aspects of subjectivity \\n        \",\n        \"\\n        News reading is never fully objective, and the annotators\\nengaged in the frame classification task, influenced by their personal\\nstate of mind, experience, and culture, may perceive information\\ndiferently. Monarch afirms that \\\"for simple tasks, like binary labels\\non objective tasks, the statistics are fairly straightforward to decide\\nwhich is the \\u2018correct\\u2019 label when diferent annotators disagree. But\\nfor subjective tasks, or even objective tasks with continuous data,\\nthere are no simple heuristics for deciding what the correct label\\nshould be\\\" \\n        \",\n        \"\\n        Subjectivity is involved in both the generation and perception\\nof information: the assumption that there is only one frame is\\ncomplicated by the point of view of the reader. In the case of news, the\\ninformation sender (the journalist) has an intention, but the receiver\\n(the reader) plays a role and is influenced by it. In psychology, this\\nis known as the lens model of interpersonal communication, where\\nthe sender has certain objectives, but the receiver can interpret\\nor re-interpret what the sender wants to say, with more or less\\naccuracy \\n        \",\n        \"Following this discussion on subjectivity, the question arose as to\\nwhat would happen if, instead of headlines, we used the complete\\narticle as a source of analysis. We wondered if longer text could\\nmake the frame labeling task clearer than when using headlines.\\nYet another possible hypothesis is that having to read longer texts\\ncould lead to the same subject being presented from diferent angles.\\nPlease recall that in the existing literature discussed in Section 2,\\nboth headlines and full articles have been used from frame analysis\\n(see Table 1.) This remains as an issue for future work.\\n7\"\n      ]\n    },\n    {\n      \"title\": \"CONCLUSIONS\",\n      \"paragraphs\": [\n        \"In this paper, we first presented an analysis of human-generated\\nnews frames on the covid-19 no-vax movement in Europe, and\\nthen studied diferent approaches using large language models for\\nautomatic inference of frames. We conclude by answering the two\\nresearch questions we posed:\",\n        \"RQ1: What are the main frames in the news headlines about the\\ncovid-19 anti-vaccine movement in 5 European countries? After\\nannotating the headlines, we found that of the 1786 headlines,\\nthe predominant frame is human interest (45.3% of cases), which\\npresents a news item with an emotional angle, putting a face to a\\nproblem or situation. We also found that a substantial proportion\\nof headlines were annotated as not presenting any frame (40.2% of\\ncases). Finally, the other frame types are found more infrequently.\",\n        \"RQ2: Can prompt engineering be used for classification of\\nheadlines according to frames? We first used fine-tuning of a number of\\nlanguage models, and found that GPT-3.5 produced classicfiation\\naccuracy of 72% on a six-frame classification task. This represented a\\nmodest 2% improvement over BERT-based models, at a significantly\\nlarger environmental cost. We then presented a new way of\\nclassifying frames using prompts. At the headline level, inferences made\\nwith GPT-3.5 reached 49% of agreement with human-generated\\nframe labels. In many cases, the GPT-3.5 model inferred frame\\ntypes that were considered as valid choices by human annotators,\\nand in an post-doc experiment, the human-machine agreement\\nreached 76%. These results have opened several new directions for\\nfuture work.\"\n      ]\n    },\n    {\n      \"title\": \"ACKNOWLEDGMENTS\",\n      \"paragraphs\": [\n        \"This work was supported by the AI4Media project, funded by the\\nEuropean Commission (Grant 951911) under the H2020 Programme\\nICT-48-2020. We also thank the newspapers for sharing their online\\narticles. Finally, we thank our colleagues Haeeun Kim and Emma\\nBouton-Bessac for their support with annotations, and Victor Bros\\nand Oleksii Polegkyi for discussions.\"\n      ]\n    }\n  ],\n  \"references\": [\n    \"Ref ID: ref1, Given Names: Justito, Surname: Adiprasetio, Year: 2020, Article Title: Pandemic crisis in online media: Quantitative framing analysis on Detik, Source: com's coverage of Covid-19. Jurnal Ilmu Sosial Dan Ilmu Politik, Volume: 24, Issue: 2, First Page: 153, Last Page: 170\",\n    \"Ref ID: ref2, Given Names: Neel C Jess Alexis, Surname: Alex Riedel Carlier, Year: 2021, Article Title: RAFT: A real-world few-shot text classification benchmark, Source: arXiv preprint arXiv:2109.14076, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref3, Given Names: Vaccination, Surname: News? A Five-Country Analysis., Year: 2022, Article Title: How Did Europe's Press Cover Covid-, Source: [3] David Alonso del Barrio and Daniel Gatica-Perez, Volume: None, Issue: None, First Page: 35, Last Page: 43\",\n    \"Ref ID: ref4, Given Names: M Bender Timnit Shmargaret, Surname: Emily Gebru Angelina McMillan-Major Shmitchell, Year: 2021, Article Title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? (, Source: None, Volume: None, Issue: None, First Page: 610, Last Page: 623\",\n    \"Ref ID: ref5, Given Names: Santosh, Surname: Kumar, Year: 2020, Article Title: Artificial intelligence in journalism: A boon or bane? In Optimization in machine learning and applications, Source: None, Volume: None, Issue: None, First Page: 155, Last Page: 167\",\n    \"Ref ID: ref6, Given Names: Erik Rana, Surname: Bleich Abdelhamid, Year: 2015, Article Title: Media portrayals of minorities: Muslims in British newspaper headlines,, Source: Journal of Ethnic and Migration Studies, Volume: 41, Issue: 6, First Page: 2012, Last Page: 962\",\n    \"Ref ID: ref7, Given Names: Michael, Surname: Bommarito, Year: 2022, Article Title: GPT Takes the Bar Exam, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref8, Given Names: Meredith, Surname: Broussard Ching-Hua Chuan, Year: 2019, Article Title: Artificial intelligence and journalism, Source: Journalism & Mass Communication Quarterly, Volume: 96, Issue: 3, First Page: 673, Last Page: 695\",\n    \"Ref ID: ref9, Given Names: Tom Amanda, Surname: Brown Jared D Kaplan Askell, Year: 2020, Article Title: Language models are few-shot learners, Source: Advances in neural information processing systems, Volume: 33, Issue: None, First Page: 1877, Last Page: 1901\",\n    \"Ref ID: ref10, Given Names: Burscher, Surname: Bj\\u00f6rn Claes H De Vreese, Year: 2014, Article Title: Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis, Source: Communication Methods and Measures 8, Volume: None, Issue: 3, First Page: 190, Last Page: 206\",\n    \"Ref ID: ref11, Given Names: Burscher, Surname: Bjorn Claes H de Vreese, Year: 2016, Article Title: Frames beyond words: Applying cluster and sentiment analysis to news coverage of the nuclear power issue, Source: Social Science Computer Review, Volume: 34, Issue: 5, First Page: 530, Last Page: 545\",\n    \"Ref ID: ref12, Given Names: Card Noah, Surname: Dallas Smith, Year: 2015, Article Title: The Media Frames Corpus: Annotations of Frames Across Issues, Source: None, Volume: 2, Issue: 01, First Page: 438, Last Page: 444\",\n    \"Ref ID: ref13, Given Names: Carlos, Surname: Daniel El\\u00edas, Year: 2020, Article Title: Vaccine hesitancy in the age of coronavirus and fake news: analysis of journalistic sources in the Spanish quality press, Source: International Journal of Environmental Research and Public Health, Volume: 17, Issue: 21, First Page: 8136, Last Page: None\",\n    \"Ref ID: ref14, Given Names: Carmen, Surname: Daniel Pe\\u00f1afiel-Saiz, Year: 2019, Article Title: Media and mistrust of vaccines: a content analysis of press headlines, Source: None, Volume: 74, Issue: None, First Page: 786, Last Page: 802\",\n    \"Ref ID: ref15, Given Names: Mark, Surname: Coddington, Year: 2015, Article Title: Clarifying journalism's quantitative turn: A typology for evaluating data journalism, computational journalism, and computer-assisted reporting, Source: Digital journalism 3, Volume: None, Issue: 3, First Page: 331, Last Page: 348\",\n    \"Ref ID: ref16, Given Names: D, Surname: Stephen Cooper, Year: 2010, Article Title: The oppositional framing of bloggers, Source: In Doing News Framing Analysis. Routledge, Volume: 151, Issue: None, First Page: 172, Last Page: None\",\n    \"Ref ID: ref17, Given Names: Robert, Surname: Dale, Year: 2021, Article Title: GPT-3: What's it good for?, Source: Natural Language Engineering, Volume: 27, Issue: 1, First Page: 113, Last Page: 118\",\n    \"Ref ID: ref18, Given Names: Astrid Dave, Surname: Dirikx Gelders, Year: 2010, Article Title: To frame is to explain: A deductive frame-analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties, Source: Public Understanding of Science 19, Volume: None, Issue: 6, First Page: 732, Last Page: 742\",\n    \"Ref ID: ref19, Given Names: Astrid Dave, Surname: Dirikx Gelders, Year: 2010, Article Title: To frame is to explain: A deductive frameanalysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties, Source: Public understanding of science 19, Volume: None, Issue: 6, First Page: 732, Last Page: 742\",\n    \"Ref ID: ref20, Given Names: Dou Graham, Surname: Zi-Yi Neubig, Year: 2020, Article Title: Gsum: A general framework for guided neural abstractive summarization, Source: None, Volume: 08014, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref21, Given Names: Sumayya, Surname: Ebrahim, Year: 2022, Article Title: The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa, Source: Health SA Gesondheid (Online) 27, Volume: None, Issue: None, First Page: 1, Last Page: 8\",\n    \"Ref ID: ref22, Given Names: Hend, Surname: Abdelgaber Ahmed El-Behary Egypt., Year: 2021, Article Title: A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref23, Given Names: M, Surname: Robert Entman, Year: 1993, Article Title: Framing: Towards clarification of a fractured paradigm, Source: McQuail's reader in mass communication theory 390, Volume: None, Issue: None, First Page: 397, Last Page: None\",\n    \"Ref ID: ref24, Given Names: Gao Danqi, Surname: Tianyu Chen, Year: 2020, Article Title: Making pre-trained language models better few-shot learners, Source: None, Volume: 15723, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref25, Given Names: Piyush Koji, Surname: Ghasiya Okamura, Year: 2021, Article Title: Investigating COVID-19 news across four nations: a topic modeling and sentiment analysis approach, Source: Ieee Access, Volume: 9, Issue: None, First Page: 36645, Last Page: 36656\",\n    \"Ref ID: ref26, Given Names: Robert, Surname: Giford, Year: 1994, Article Title: A Lens-Mapping Framework for Understanding the Encoding and Decoding of Interpersonal Dispositions in Nonverbal Behavior, Source: Journal of Personality and Social Psychology, Volume: 66, Issue: None, First Page: 398, Last Page: 412\",\n    \"Ref ID: ref27, Given Names: Grail Eric, Surname: Quentin Gaussier, Year: 2021, Article Title: Globalizing BERT-based transformer architectures for long document summarization, Source: In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:, Volume: None, Issue: None, First Page: 1792, Last Page: 1810\",\n    \"Ref ID: ref28, Given Names: Gupta Rahul, Surname: Anushka Katarya, Year: 2022, Article Title: Automated news summarization using transformers, Source: In Sustainable Advanced Computing, Volume: None, Issue: None, First Page: 249, Last Page: 259\",\n    \"Ref ID: ref29, Given Names: Alfred, Surname: Hermida, Year: 2017, Article Title: Finding the data unicorn: A hierarchy of hybridity in data and computational journalism, Source: Digital Journalism, Volume: 5, Issue: 2, First Page: 159, Last Page: 176\",\n    \"Ref ID: ref30, Given Names: Isoaho Eetu, Surname: Karoliina M\\u00e4kel\\u00e4, Year: 2021, Article Title: Topic modeling and text analysis for qualitative policy research, Source: Policy Studies Journal, Volume: 49, Issue: 1, First Page: 300, Last Page: 324\",\n    \"Ref ID: ref31, Given Names: Jacobi and Kasper, Surname: Carina Welbers, Year: 2016, Article Title: Quantitative analysis of large amounts of journalistic texts using topic modelling, Source: Digital journalism 4, Volume: None, Issue: 1, First Page: 89, Last Page: 106\",\n    \"Ref ID: ref32, Given Names: Jiang Jun Graham, Surname: Zhengbao Araki Neubig, Year: 2020, Article Title: How can we know what language models know? Transactions of the Association for Computational Linguistics 8 (, Source: None, Volume: None, Issue: None, First Page: 423, Last Page: 438\",\n    \"Ref ID: ref33, Given Names: Khanehzar Andrew Gosia, Surname: Shima Turpin Miko\\u0142ajczak, Year: 2019, Article Title: Modeling Political Framing Across Policy Issues and Contexts, Source: In ALTA., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref34, Given Names: Jeesun Wayne, Surname: Kim Wanta, Year: 2018, Article Title: News framing of the US immigration debate during election years: Focus on generic frames, Source: The Communication Review, Volume: 21, Issue: 2, First Page: 89, Last Page: 115\",\n    \"Ref ID: ref35, Given Names: Liang Tony Ananya, Surname: Percy Lee Kumar, Year: 2022, Article Title: Holistic evaluation of language models, Source: arXiv preprint arXiv:2211.09110, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref36, Given Names: Liu Graham, Surname: Pengfei Neubig, Year: 2021, Article Title: Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref37, Given Names: Liu, Surname: Siyi, Year: 2019, Article Title: Detecting frames in news headlines and its application to analyzing news framing trends surrounding US gun violence, Source: In Proceedings of the 23rd conference on computational natural language learning (CoNLL)., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref38, Given Names: J\\u00f6rg Matthias, Surname: Matthes Kohring, Year: 2008, Article Title: The Content Analysis of Media Frames: Toward Improving Reliability and Validity, Source: Journal of Communication, Volume: 58, Issue: None, First Page: 2466, Last Page: None\",\n    \"Ref ID: ref39, Given Names: Meyer Bernd David E, Surname: Selina Ludwig Losada, Year: 2022, Article Title: Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI, Source: In Proceedings of the 4th Conference on Conversational User Interfaces. 1-6., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref40, Given Names: E Middleton Yiannis, Surname: Stuart Kompatsiaris, Year: 2018, Article Title: Social computing for verifying social media content in breaking news, Source: IEEE Internet Computing, Volume: 22, Issue: 2, First Page: 83, Last Page: 89\",\n    \"Ref ID: ref41, Given Names: Marko Igor, Surname: Milosavljevi\\u0107 Vobi\\u010d, Year: 2021, Article Title: Our task is to demystify fears': Analysing newsroom management of automation in journalism, Source: Journalism, Volume: 22, Issue: 9, First Page: 2203, Last Page: 2221\",\n    \"Ref ID: ref42, Given Names: R., Surname: Monarch, Year: 2021, Article Title: Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref43, Given Names: Tom Pepper D, Surname: Nicholls Culpepper, Year: 2021, Article Title: Computational identification of media frames: Strengths, weaknesses, and opportunities, Source: Political Communication, Volume: 38, Issue: None, First Page: 1, Last Page: 2\",\n    \"Ref ID: ref44, Given Names: Zhongdang, Surname: Pan and Gerald M Kosicki, Year: 1993, Article Title: Framing analysis: An approach to news discourse, Source: Political communication 10, Volume: None, Issue: 1, First Page: 55, Last Page: 75\",\n    \"Ref ID: ref45, Given Names: Raul Bryan, Surname: Puri Catanzaro, Year: 2019, Article Title: Zero-shot text classification with generative language models, Source: None, Volume: 10165, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref46, Given Names: Guanghui Jason, Surname: Qin Eisner, Year: 2021, Article Title: Learning how to ask: Querying lms with mixtures of soft prompts, Source: arXiv preprint arXiv:2104.06599, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref47, Given Names: Rabindra, Surname: Lamsal, Year: 2021, Article Title: Sentiment Analysis of English Tweets with BERTsent, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref48, Given Names: Radford Dario Ilya, Surname: Alec Amodei Sutskever, Year: 2019, Article Title: Language models are unsupervised multitask learners, Source: OpenAI blog 1, Volume: None, Issue: 8, First Page: 9, Last Page: None\",\n    \"Ref ID: ref49, Given Names: Rai Ahad, Surname: Nishant Ali, Year: 2022, Article Title: Fake News Classification using transformer based enhanced LSTM and BERT, Source: International Journal of Cognitive Computing in Engineering, Volume: 3, Issue: None, First Page: 98, Last Page: 105\",\n    \"Ref ID: ref50, Given Names: V, Surname: Frida Rodelo, Year: 2021, Article Title: Framing of the Covid-19 pandemic and its organizational predictors, Source: Cuadernos. info 50, Volume: None, Issue: None, First Page: 91, Last Page: 112\",\n    \"Ref ID: ref51, Given Names: Teven Matthias, Surname: Le Scao Gall\\u00e9, Year: 2022, Article Title: Bloom: A 176b-parameter open-access multilingual language model, Source: arXiv preprint arXiv:2211.05100, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref52, Given Names: Holli Patti, Surname: Semetko Valkenburg, Year: 2000, Article Title: Framing European Politics: A Content Analysis of Press and Television News, Source: Journal of Communication, Volume: 50, Issue: None, First Page: 93, Last Page: 109\",\n    \"Ref ID: ref53, Given Names: Shin, Surname: Richard, Year: 2021, Article Title: Constrained language models yield few-shot semantic parsers, Source: arXiv preprint arXiv:2104.08768, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref54, Given Names: Efstathios Andreas, Surname: Sidiropoulos Veglis, Year: 2017, Article Title: Computer Supported Collaborative Work trends on Media Organizations: Mixing Qualitative and Quantitative Approaches, Source: Studies in Media and Communication, Volume: 5, Issue: 04, First Page: 2279, Last Page: None\",\n    \"Ref ID: ref55, Given Names: Strubell, Surname: Emma Andrew McCallum, Year: 2019, Article Title: Energy and policy considerations for deep learning in NLP, Source: None, Volume: 02243, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref56, Given Names: Tamkin Deep, Surname: Alex Ganguli, Year: 2021, Article Title: Understanding the capabilities, limitations, and societal impact of large language models, Source: arXiv preprint arXiv:2102.02503, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref57, Given Names: H, Surname: Trieu Trinh and Quoc V Le, Year: 2018, Article Title: A simple method for commonsense reasoning, Source: None, Volume: 02847, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref58, Given Names: Tsimpoukelli Felix, Surname: Maria Oriol Vinyals Hill, Year: 2021, Article Title: Serkan Cabi, Source: Advances in Neural Information Processing Systems, Volume: 34, Issue: None, First Page: 200, Last Page: 212\",\n    \"Ref ID: ref59, Given Names: A Vannoy Prashant, Surname: Sandra Palvia, Year: 2010, Article Title: The social influence model of technology adoption, Source: Commun. ACM 53, Volume: None, Issue: 6, First Page: 149, Last Page: 153\",\n    \"Ref ID: ref60, Given Names: Tuukka Veikko Anna, Surname: Yl\\u00e4-Anttila Eranti Kukkonen, Year: 2022, Article Title: Topic modeling for frame analysis: A study of media debates on climate change in India and USA, Source: Global Media and Communication, Volume: 18, Issue: 1, First Page: 91, Last Page: 112\"\n  ]\n}",
    "{\n  \"Article_ID\": \"5488c5958a2847e1aad91f9481552489\",\n  \"DocumentTitle\": \"Generating Diverse Code Explanations using the GPT-3 Large Language Model\",\n  \"Auteurs\": \"Stephen MacNeil, Seth Bernstein, Andrew Tran, Erin Ross, Dan Mogil, Ziheng Huang, \",\n  \"Institutions\": \"Temple University, University of California-San Diego, \",\n  \"Abstract\": \"no\",\n  \"Sections\": [\n    {\n      \"title\": \"USE CASES\",\n      \"paragraphs\": [\n        \"\\n        To understand the types of explanations GPT-3 \\n        \",\n        \"\\n        Analyzing and explaining time complexity\\nInstructors rate time complexity as the most dificult programming\\ntopic \\n        \",\n        \"\\n        Identifying common mistakes made by\\nbeginner programmers\\nCommonality exists in how students solve programming\\nproblems \\n        \",\n        \"\\n        Summarizing code at multiple levels of\\nabstraction\\nBefore understanding how a code snippet executes, it is often useful\\nto understand the purpose of the code \\n        \"\n      ]\n    },\n    {\n      \"title\": \"DISCUSSION\",\n      \"paragraphs\": [\n        \"Our three use cases demonstrate the potential for GPT-3 to explain\\ncode for intro CS students. Our poster presentation will feature all\\neight explanation types as a design space of explanations to convey\\nthe diversity of explanations that can be generated by LLMs. We will\\nhighlight best practices for generating efective explanations and\\npitfalls that lead to less efective explanations. We are evaluating\\nthe usefulness of these explanations in a series of summer classes.\"\n      ]\n    }\n  ],\n  \"references\": [\n    \"Ref ID: ref1, Given Names: Amjad, Surname: Altadmri Neil CC Brown, Year: 2015, Article Title: 37 million compilations: Investigating novice programming mistakes in large-scale student data, Source: In Proceedings of the 46th ACM Technical Symposium on Computer Science Education, Volume: 522, Issue: None, First Page: 527, Last Page: None\",\n    \"Ref ID: ref2, Given Names: Tom Amanda, Surname: Brown Jared D Kaplan Askell, Year: 2020, Article Title: Language models are few-shot learners, Source: Advances in Neural Information Processing Systems, Volume: 33, Issue: None, First Page: 1877, Last Page: 1901\",\n    \"Ref ID: ref3, Given Names: Adam Cynthia, Surname: Carberry Waters, Year: 2013, Article Title: Unmuddying\\u201d course content using muddiest point reflections, Source: In 2013 IEEE Frontiers in Education Conference (FIE), Volume: None, Issue: None, First Page: 937, Last Page: 942\",\n    \"Ref ID: ref4, Given Names: Mark Greg, Surname: Chen Brockman, Year: 2021, Article Title: Evaluating large language models trained on code, Source: arXiv preprint arXiv:2107.03374, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref5, Given Names: Kathryn, Surname: Cunningham Eleanor O'Rourke, Year: 2022, Article Title: Bringing \\\"High-Level\\\" Down to Earth: Gaining Clarity in Conversational Programmer Learning Goals, Source: In Proceedings of the 53rd ACM Technical Symposium on Computer Science, Volume: 1, Issue: None, First Page: 551, Last Page: 557\",\n    \"Ref ID: ref6, Given Names: Elvina Oscar, Surname: Elvina Karnalim, Year: 2017, Article Title: Complexitor: An educational tool for learning algorithm time complexity in practical manner, Source: ComTech: Computer, Mathematics and Engineering Applications, Volume: 8, Issue: 1, First Page: 21, Last Page: 27\",\n    \"Ref ID: ref7, Given Names: James Andrew James, Surname: Finnie-Ansley Luxton-Reilly Prather, Year: 2022, Article Title: The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming, Source: In Australasian Computing Education Conference (Virtual Event, Australia) (ACE '22), Volume: None, Issue: None, First Page: 10, Last Page: 19\",\n    \"Ref ID: ref8, Given Names: J, Surname: Philip Guo, Year: 2013, Article Title: Online python tutor: embeddable web-based program visualization for cs education, Source: In Proceeding of the 44th ACM technical symposium on Computer science education. 579-584., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref9, Given Names: Andrew Bj\\u00f6rn, Surname: Head Hartmann, Year: 2015, Article Title: Marti A Hearst,, Source: In 2015 IEEE Symposium on Visual Languages and HumanCentric Computing, Volume: None, Issue: None, First Page: 3, Last Page: 12\",\n    \"Ref ID: ref10, Given Names: Marwan Ge Tifany, Surname: Samiha Gao Barnes, Year: 2020, Article Title: Adaptive Immediate Feedback Can Improve Novice Programming Engagement and Intention to Persist in Computer Science, Source: In Proceedings of the 2020 ACM Conference on International Computing Education Research (Virtual Event, New Zealand) (ICER '20), Volume: None, Issue: None, First Page: 194, Last Page: 203\",\n    \"Ref ID: ref11, Given Names: McCall Michael, Surname: Davin K\\u00f6lling, Year: 2014, Article Title: Meaningful categorisation of novice programmer errors, Source: In 2014 IEEE Frontiers in Education Conference (FIE) Proceedings. IEEE, 1-8., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref12, Given Names: L Nelson, Surname: Greg Benjamin Xie Amy, Year: 2017, Article Title: Comprehension first: evaluating a novel pedagogy and tutoring system for program tracing in CS1, Source: In Proceedings of the 2017 ACM conference on international computing education research, Volume: None, Issue: None, First Page: 11, Last Page: None\",\n    \"Ref ID: ref13, Given Names: Miranda Colleen, Surname: Parker Lewis, Year: 2014, Article Title: What makes big-O analysis dificult: understanding how students understand runtime analysis, Source: Journal of Computing Sciences in Colleges 29, Volume: None, Issue: 4, First Page: 164, Last Page: 174\",\n    \"Ref ID: ref14, Given Names: Daniel Laird Maria, Surname: Perez Kramer Charters, Year: 2020, Article Title: WIP: An exploration into the muddiest points and self-eficacy of students in introductory computer science courses, Source: In 2020 IEEE Frontiers in Education Conference (FIE), Volume: None, Issue: None, First Page: 5, Last Page: None\",\n    \"Ref ID: ref15, Given Names: Piech Leonidas, Surname: Chris Guibas, Year: 2015, Article Title: Autonomously generating hints by inferring problem solving policies, Source: In Proceedings of the second, Volume: None, Issue: None, First Page: 195, Last Page: 204\",\n    \"Ref ID: ref16, Given Names: W Price Dragan, Surname: Thomas Lipovac, Year: 2017, Article Title: iSnap: towards intelligent tutoring in novice programming environments, Source: In Proceedings of the 2017 ACM SIGCSE Technical Symposium on computer science education, Volume: 483, Issue: None, First Page: 488, Last Page: None\",\n    \"Ref ID: ref17, Given Names: Carsten Jens, Surname: Schulte Bennedsen, Year: 2006, Article Title: What do teachers teach in introductory programming?, Source: In Proceedings of the second international workshop on Computing education research, Volume: 17, Issue: None, First Page: 28, Last Page: None\"\n  ]\n}",
    "{\n  \"Article_ID\": \"64e656853ab94ec0b37a416110ae348b\",\n  \"DocumentTitle\": \"The Programmer's Assistant: Conversational Interaction with a Large Language Model for Software Development\",\n  \"Auteurs\": \"Steven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller, Justin D. Weisz, code-fluent large language models, foundation models, conversa-, \",\n  \"Institutions\": \"IBM Argentina, IBM Research AI, IBM Research AI, tional interaction, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"CCS CONCEPTS\",\n      \"paragraphs\": [\n        \"\\u2022 Human-centered computing \\u2192 HCI theory, concepts and\\nmodels; \\u2022 Software and its engineering \\u2192 Designing software;\\n\\u2022 Computing methodologies \\u2192 Generative and developmental\\napproaches.\\n1\"\n      ]\n    },\n    {\n      \"title\": \"INTRODUCTION\",\n      \"paragraphs\": [\n        \"\\n        Software development is a highly skilled task that requires\\nknowledge, focus, and creativity \\n        \",\n        \"\\n        The pioneering work of Rich and Waters on The Programmer\\u2019s\\nApprentice \\n        \",\n        \"\\n        Recently, models leveraging the transformer architecture \\n        \",\n        \"Although autocompletion interfaces are useful and valuable\\nwhen the system can discern the developer\\u2019s intent, there are many\\ninstances where that is insuficient. For example, the developer\\nmay have a good idea of what they want to do, but may be unclear\\non what functions, libraries, or even algorithms to employ. They\\nmay even have general programming questions that need to be\\nanswered before they are able to write any code.\",\n        \"In this paper, we seek to understand whether modern\\ndevelopments in code-fluent foundation models \\u2013 large language models\\nthat have been fine-tuned on source code data \\u2013 are suficient to\\nsupport a conversational agent that can act as an assistant in the\\nsoftware development process. We developed the Programmer\\u2019s\\nAssistant to explore the capabilities that conversational interaction\\ncould enable and the extent to which users would find\\nconversational assistance with programming tasks desirable and useful.\",\n        \"\\n        We hypothesize that a conversational system may provide a\\nlfexible and natural means for interacting with a code-fluent LLM.\\nConversational interaction could enable users to pursue their\\nquestions in a multiple exchange dialog (as observed by Barke et al.\\n\\n        \",\n        \"Our paper makes the following contributions to the IUI\\ncommunity:\\n\\u2022 We provide empirical evidence that a conversational\\nprogramming assistant based on a state-of-the-art, code-fluent\\nfoundation model provides valuable assistance to software\\nengineers in a myriad of ways: by answering general\\nprogramming questions, by generating context-relevant code,\\nby enabling the model to exhibit emergent behaviors, and by\\nenabling users to ask follow-up questions that depend upon\\ntheir conversational and code contexts.\\n\\u2022 We show how diferent interaction models \\u2013 conversation,\\ndirect manipulation, and search \\u2013 provide complementary\\ntypes of support to software engineers with tradeofs\\nbetween the user\\u2019s focus and attention, the relevance of\\nsupport to their code context, the provenance of that support,\\nand their ability to ask follow-up questions.\\n\\u2022 We motivate the need to further understand how to design\\nhuman-centered AI systems that enhance the joint\\nperformance of the human-AI collaboration.\\n2\"\n      ]\n    },\n    {\n      \"title\": \"RELATED WORK\",\n      \"paragraphs\": [\n        \"\\n        We discuss three areas of related work that have either motivated\\nour study of conversational programming assistance or provided\\nthe technical foundations for it. We begin by briefly summarizing\\nRich and Waters\\u2019 visionary work on the Programmer\\u2019s\\nApprentice \\n        \"\n      ]\n    },\n    {\n      \"title\": \"The Programmer\\u2019s Apprentice\",\n      \"paragraphs\": [\n        \"\\n        Our work is inspired by the vision laid out by Rich and Waters \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Code-fluent Foundation Models and\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Human-Centered Evaluations of\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Programming Assistance\",\n      \"paragraphs\": [\n        \"\\n        Generative models based on the transformer architecture \\n        \",\n        \"\\n        New tools based on code-fluent LLMs are actively being\\ndeveloped. GitHub Copilot1 is described as \\u201cYour AI pair programmer.\\u201d It\\nis optimized for the code autocompletion use case: given a starting\\nsnippet such as a method\\u2019s documentation, signature, or partial\\nimplementation, Copilot completes the implementation. Copilot is\\nbased on the OpenAI Codex model \\n        \",\n        \"\\n          this model have shown that, although the quality of its outputs is\\nquite good, those outputs may still be problematic \\n          \",\n        \"\\n          A grounded theory analysis of how programmers interact with\\nCopilot \\n          \"\n      ]\n    },\n    {\n      \"title\": \"1https://github.com/features/copilot\",\n      \"paragraphs\": [\n        \"\\n          this model have shown that, although the quality of its outputs is\\nquite good, those outputs may still be problematic \\n          \",\n        \"\\n          A grounded theory analysis of how programmers interact with\\nCopilot \\n          \"\n      ]\n    },\n    {\n      \"title\": \"Conversational Interaction and Analysis\",\n      \"paragraphs\": [\n        \"\\n        2.3.1 Conversational Interaction. Using natural language to\\ninteract with technology has had a long research history \\n        \",\n        \"Task-oriented chatbots are typically built with frameworks such\\nas the Microsoft Bot Framework2, Google DialogFlow3, and IBM\\nWatson Assistant4. They operate using pre-defined dialogue trees\\nand use natural language processing to detect conversational intents\\nand extract contextual entities. This structure enables the creation of\\nspecial purpose, but fairly limited and rigid, conversational agents.\",\n        \"\\n        There have been several recent attempts to investigate\\nconversational programming assistance. Kuttal et al. \\n        \",\n        \"\\n          environments. PACT is structured as a discrete question-answering\\nsystem based on a neural machine translation approach, but it\\ndoesn\\u2019t maintain a conversational context.\\n2.3.2 Conversation Analysis. Conversation is a form of interaction\\nbetween people that enables robust communication. Conversation\\nAnalysis \\n          \"\n      ]\n    },\n    {\n      \"title\": \"2https://dev.botframework.com/ 3https://cloud.google.com/dialogflow 4https://www.ibm.com/products/watson-assistant/artificial-intelligence\",\n      \"paragraphs\": [\n        \"\\n          environments. PACT is structured as a discrete question-answering\\nsystem based on a neural machine translation approach, but it\\ndoesn\\u2019t maintain a conversational context.\\n2.3.2 Conversation Analysis. Conversation is a form of interaction\\nbetween people that enables robust communication. Conversation\\nAnalysis \\n          \"\n      ]\n    },\n    {\n      \"title\": \"THE PROGRAMMER\\u2019S ASSISTANT\",\n      \"paragraphs\": [\n        \"\\n        In order to explore conversational programming assistance, we\\ncreated a functional prototype system called The Programmer\\u2019s\\nAssistant. Our prototype, shown in Figure 1, combines a code editor\\nwith a chat interface. The code editor was implemented using the\\nMicrosoft Monaco Editor5 embedded in a React wrapper6. The\\nchat user interface was implemented using the React-Chatbot-Kit7\\nframework. To drive the conversational interaction, we employed\\nOpenAI\\u2019s Codex model \\n        \",\n        \"We developed our prototype as a lightweight coding\\nenvironment in order to examine the user experience of interacting with\\na conversational assistant. Our work was exploratory in nature,\\nand thus we did not have specific design goals for the prototype\\nbeyond integrating a code editor with a code-fluent LLM. We also\\ndid not attempt to target the prototype for a specific class of users\\n(e.g. novices or experts) or use cases (e.g. writing code vs. learning a\\nnew programming language), as we wanted any value provided by\\nconversational assistance to emerge from our user study. We also\\ndid not implement the ability to run or debug code in our prototype\\nas we wanted to explore the nature of the conversational\\ninteraction rather than having users focus extensively on the production\\nof working code.\",\n        \"\\n        When designing how users would interact with the\\nProgrammer\\u2019s Assistant, we decided that it should be available on demand\\nand not monitor the user\\u2019s work in progress or give unsolicited\\nsuggestions or advice, in keeping with the conversational agent\\ninteraction model proposed by Ross et al. \\n        \",\n        \"When a user interacts with the assistant, we keep track of their\\nselection state in the code editor. If a user sends a message to the\\nassistant without any code selected in the editor, then that message\\n(along with the prior conversational context) is passed directly to\\nthe model. If a user sends a message to the assistant with new code\",\n        \"selected in the editor (i.e. code that wasn\\u2019t previously selected when\\nthey sent their last message), then that code is appended to the\\nmessage before being communicated to the model.\",\n        \"The model may produce multiple types of responses to a user\\u2019s\\nmessage. We treat each type of response diferently in the UI.\\n\\u2022 Responses that do not contain code are always rendered in\\nthe chat UI (Figure 1E).\\n\\u2022 Responses containing short code snippets (\\u2264 10 lines) are\\nrendered inline in the chat UI (Figure 1G).\\n\\u2022 Responses containing longer code snippets (> 10 lines) show\\nthe code in a pop-up window (Figure 2A), with a proxy entry\\nin the chat transcript (Figure 2B) that allows users to\\nredisplay the code window after it has been closed. Non-code\\ntext in the response remains in the chat transcript.\",\n        \"The assistant never directly modifies the contents of the user\\u2019s\\nsource code; rather, any code the user desires to transfer from the\\nchat takes place via copy/paste.\",\n        \"Figure 1 shows a screenshot of a real, sample conversation, in\\nwhich the user asks a question that results in an inline response,\\nthen requests an explanation of some code in the editor, and then\\nrequests further elaboration. Figure 2 shows an example conversation\\nthat resulted in the generation of a longer code sample, shown in a\\npopup window. This example shows how the assistant produced an\\nincomplete solution, followed by criticism from the user regarding\\nthe missing code, and resulting in an apology and the generation\\nof a complete solution.\\n3.1\"\n      ]\n    },\n    {\n      \"title\": \"5https://microsoft.github.io/monaco-editor/ 6https://www.npmjs.com/package/@monaco-editor/react 7https://fredrikoseberg.github.io/react-chatbot-kit-docs/\",\n      \"paragraphs\": [\n        \"selected in the editor (i.e. code that wasn\\u2019t previously selected when\\nthey sent their last message), then that code is appended to the\\nmessage before being communicated to the model.\",\n        \"The model may produce multiple types of responses to a user\\u2019s\\nmessage. We treat each type of response diferently in the UI.\\n\\u2022 Responses that do not contain code are always rendered in\\nthe chat UI (Figure 1E).\\n\\u2022 Responses containing short code snippets (\\u2264 10 lines) are\\nrendered inline in the chat UI (Figure 1G).\\n\\u2022 Responses containing longer code snippets (> 10 lines) show\\nthe code in a pop-up window (Figure 2A), with a proxy entry\\nin the chat transcript (Figure 2B) that allows users to\\nredisplay the code window after it has been closed. Non-code\\ntext in the response remains in the chat transcript.\",\n        \"The assistant never directly modifies the contents of the user\\u2019s\\nsource code; rather, any code the user desires to transfer from the\\nchat takes place via copy/paste.\",\n        \"Figure 1 shows a screenshot of a real, sample conversation, in\\nwhich the user asks a question that results in an inline response,\\nthen requests an explanation of some code in the editor, and then\\nrequests further elaboration. Figure 2 shows an example conversation\\nthat resulted in the generation of a longer code sample, shown in a\\npopup window. This example shows how the assistant produced an\\nincomplete solution, followed by criticism from the user regarding\\nthe missing code, and resulting in an apology and the generation\\nof a complete solution.\\n3.1\"\n      ]\n    },\n    {\n      \"title\": \"Supporting Conversational Interaction\",\n      \"paragraphs\": [\n        \"\\n        We enabled Codex to conduct a conversational interaction by\\nprompting it with a conversational transcript and a request to produce\\nthe next conversational turn. The prompt establishes a pattern of\\nconversation between a user and a programming assistant named\\nSocrates. It provides several examples of Socrates responding to\\ngeneral coding questions, generating code in response to a request, and\\naccepting code as input. It establishes a convention for delimiting\\ncode in the conversation, making it easy to parse for display in the\\nUI. It also establishes an interaction style for the assistant, directing\\nit to be polite, eager, helpful, and humble, and to present its\\nresponses in a non-authoritative manner8. Because of the possibility\\nthat the model might produce erroneous answers or incorrect code\\n(as discussed in Weisz et al. \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Architecture & UI Design\",\n      \"paragraphs\": [\n        \"The Programmer\\u2019s Assistant communicates with the Codex API via\\na proxy server that forwards requests from the React client. The\\nproxy also rate-limits access to conform to the API\\u2019s policy, and\\nit logs UI events from the client (e.g. requests, responses, and UI\\n8The assistant\\u2019s use of non-authoritative responses was encoded into the LLM prompt;\\noutput token probabilities from the LLM were not utilized to influence the assistant\\u2019s\\nresponse.\\ninteractions) in a back-end database. To address inconsistencies\\nin the style or formatting of code generated by Codex, the proxy\\nserver reformats all code segments using the Black code formatter\\n9 before transmitting them to the client UI.\",\n        \"The client maintains the transcript of the ongoing conversation.\\nEach time the user sends a message in the chat, the client constructs\\na new prompt for the model by concatenating the initial prompt,\\nthe chat transcript, and the user\\u2019s new utterance, and makes a\\nrequest for the model to complete the transcript. This completion\\nrequest also specifies a stop sequence of tokens to prevent the\\nmodel from generating both sides of the conversation (e.g. what\\nthe model thinks the user\\u2019s next utterance might be after the\\nassistant\\u2019s response). Given the API\\u2019s limitation on context length\\n(4,096 tokens for both the prompt and model response), we silently\\n\\u201cforget\\u201d older exchanges in the chat transcript when constructing\\nthe prompt to ensure that our completion request remains within\\nbounds. Nonetheless, the entire conversational history remains\\nvisible to the user in the UI.\",\n        \"The client UI provides a loose coupling between the source code\\neditor and the chat interface. Users can hide the chat pane when\\nthey wish to focus solely on their code, and re-engage with it\\nwhen they desire assistance. Code selected in the editor is included\\nin the conversation in order to couple the code context with the\\nconversation. Easily-accessible buttons are provided in the UI to\\ncopy code responses from the assistant to the clipboard.\\n3.3\"\n      ]\n    },\n    {\n      \"title\": \"Handling Model Limitations\",\n      \"paragraphs\": [\n        \"While developing the Programmer\\u2019s Assistant, and in early pilot\\ntesting, we experienced some quirks and shortcomings of the model\\nand our approach to using it for conversational interaction. One\\nlimitation stemmed from the fact that the model sometimes\\nproduced incorrect responses (e.g. code with syntax errors), incomplete\\nresponses (e.g. code that was missing functionality), irrelevant\\nresponses (e.g. responses not related to the user\\u2019s question), or\\ninsubstantial responses (e.g. \\u201cI don\\u2019t know\\u201d). Because of the probabilistic\\nnature of model inference, re-prompting the model would\\nsometimes produce a more correct or appropriate response. Thus, we\\nadded the ability for users to \\u201ctry again,\\u201d either by asking in the chat\\nor by clicking a button in the UI (Figure 1C). This feature removes\\nthe assistant\\u2019s last response from the context presented to the model\\nand then re-invokes the model with an increased temperature10.\",\n        \"Although it is possible for transformer models such as Codex to\\nproduce multiple possible responses to a single prompt, we only\\nrequest a single response in order to speed up response time as\\nwell as to preserve the token budget for conversational context.\\nThus, the \\u201ctry again\\u201d feature provides an alternate way to produce\\na wider variety of responses.\",\n        \"During pilot testing, we noticed that the assistant sometimes\\nhappened to generate the same response to multiple, unrelated\\nrequests. In these cases, the assistant tended to get \\u201cstuck\\u201d in a pattern\\nof repeating the same response and was unable to resume normal\\nconversation. To avoid this problem, we automatically execute a\",\n        \"10Temperature is a parameter in a generative model that specifies the amount of\\nvariation in the generation process. Higher temperatures result in greater variability\\nin the model\\u2019s output.\\nA\\nB\",\n        \"C\",\n        \"D\",\n        \"E\\nF\\nG\\n\\u201ctry again\\u201d operation in the background when we see identical\\nconsecutive responses from the assistant.\",\n        \"Finally, we noticed that the accumulation of conversational\\ncontext sometimes resulted in the assistant becoming fixated on some\\nportion of the earlier conversation. For example, it might respond\\nto a question with portions of the prompt or of earlier conversation,\\nand become less responsive to newer requests. To address this issue,\\nwe introduced a \\u201cstart over\\u201d feature, accessible via the chat or by\\nclicking a button in the UI (Figure 1D), that resets the context to the\\noriginal prompt, forgetting the rest of the conversational history.\\nWe preserve the chat transcript in the UI, but delineate the break in\\nthe assistant\\u2019s memory with an annotation in the chat transcript.\\nThese annotations are added both for \\u201ctry again\\u201d and \\u201cstart over.\\u201d\"\n      ]\n    },\n    {\n      \"title\": \"9https://black.readthedocs.io/en/stable/\",\n      \"paragraphs\": [\n        \"10Temperature is a parameter in a generative model that specifies the amount of\\nvariation in the generation process. Higher temperatures result in greater variability\\nin the model\\u2019s output.\\nA\\nB\",\n        \"C\",\n        \"D\",\n        \"E\\nF\\nG\\n\\u201ctry again\\u201d operation in the background when we see identical\\nconsecutive responses from the assistant.\",\n        \"Finally, we noticed that the accumulation of conversational\\ncontext sometimes resulted in the assistant becoming fixated on some\\nportion of the earlier conversation. For example, it might respond\\nto a question with portions of the prompt or of earlier conversation,\\nand become less responsive to newer requests. To address this issue,\\nwe introduced a \\u201cstart over\\u201d feature, accessible via the chat or by\\nclicking a button in the UI (Figure 1D), that resets the context to the\\noriginal prompt, forgetting the rest of the conversational history.\\nWe preserve the chat transcript in the UI, but delineate the break in\\nthe assistant\\u2019s memory with an annotation in the chat transcript.\\nThese annotations are added both for \\u201ctry again\\u201d and \\u201cstart over.\\u201d\"\n      ]\n    },\n    {\n      \"title\": \"Sample Conversation\",\n      \"paragraphs\": [\n        \"We provide a real sample conversation with the Programmer\\u2019s\\nAssistant in Listing 1. This conversation begins with the assistant\\ngreeting the user (line 1). Next, the user asks a general Python\\nprogramming question (line 4), to which the assistant responds with\\na non-authoritative remark (\\u201cI think...\\u201d) and a code snippet (line\\n9). The user next asks a follow-up question that depends on their\\nprevious question and the assistant\\u2019s response (line 11), to which\\nthe assistant provides another code snippet (line 15), satisfying the\\nuser\\u2019s request.\",\n        \"The user then switches topics and asks the assistant to write\\na Fibonacci function (line 17), and the assistant again responds\\nwith a non-authoritative remark (\\u201cI will give it a try,\\u201d line 20) and\\na block of code. The user then asks how the function works (line\\n30) and the assistant provides an adequate description (line 32).\\nNext, the user asks the assistant to re-implement the function in a\\ndiferent way (line 37), again leveraging the ability to ask follow-up\\nquestions. The assistant produces an alternative implementation\\nthat conforms to the user\\u2019s request (line 41). The user follows up\\nwith a question that depends on multiple past utterances and\\nresponses in the chat transcript (line 47), and the assistant produces\\na relevant response (line 49). The conversation closes with the user\\nthanking the assistant (line 53) and the assistant acknowledging\\ntheir gratitude (line 55).\",\n        \"Code presented by the assistant is listed in bold face.\\n1 Socrates : Hello ! My name is Socrates your programming\\n2 assistant . What can I help you with today ?\\n3\\n4 User : How do I get a list of the keys in a\\n5 Python dictionary ?\\n6\\n7 Socrates : I think you would do something like this :\\n8\\n9 list(my_dict.keys())\\n10\\n11 User : What if I just wanted to iterate over them ?\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n4\",\n        \"Socrates : I think so . How about this ?\\ndef fib(n):\\na,b = 0,1\\nfor i in range(n):\",\n        \"a,b = b,a+b\\nreturn\\nUser :\",\n        \"Which version is better ?\\nSocrates : I think the recursive version is more\\nelegant , but the iterative version is more\\nefficient .\",\n        \"User :\",\n        \"Thanks !\\nSocrates : You ' re welcome !\"\n      ]\n    },\n    {\n      \"title\": \"Listing 1: A conversation with the Programmer\\u2019s Assistant.\",\n      \"paragraphs\": [\n        \"Code presented by the assistant is listed in bold face.\\n1 Socrates : Hello ! My name is Socrates your programming\\n2 assistant . What can I help you with today ?\\n3\\n4 User : How do I get a list of the keys in a\\n5 Python dictionary ?\\n6\\n7 Socrates : I think you would do something like this :\\n8\\n9 list(my_dict.keys())\\n10\\n11 User : What if I just wanted to iterate over them ?\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n4\",\n        \"Socrates : I think so . How about this ?\\ndef fib(n):\\na,b = 0,1\\nfor i in range(n):\",\n        \"a,b = b,a+b\\nreturn\\nUser :\",\n        \"Which version is better ?\\nSocrates : I think the recursive version is more\\nelegant , but the iterative version is more\\nefficient .\",\n        \"User :\",\n        \"Thanks !\\nSocrates : You ' re welcome !\"\n      ]\n    },\n    {\n      \"title\": \"EMPIRICAL STUDY OF CONVERSATIONAL\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"PROGRAMMING ASSISTANCE\",\n      \"paragraphs\": [\n        \"\\n        We conducted an empirical user study of the Programmer\\u2019s\\nAssistant to assess whether conversational assistance provides value in\\na software engineering context11. Our inquiry focused on the user\\nexperience and value of conversational interactions grounded in\\ncode. We therefore designed a qualitative study to investigate\\nattitudes toward a conversational programming assistant: do people\\nenjoy interacting conversationally, what kinds of questions do they\\nask, and how does the experience compare to other forms of\\nprogramming support such as searching the web? We note that prior\\nstudies (e.g. \\n        \",\n        \"To address our questions, we deployed the Programmer\\u2019s\\nAssistant within our organization \\u2013 a global technology company \\u2013 and\\ninvited people to try it out and give us feedback on their experience.\\nWe invited people with varying levels of programming skill in order\\nto obtain a wide range of feedback on the kinds of use cases for\\nwhich the tool could provide assistance.\\n4.1\"\n      ]\n    },\n    {\n      \"title\": \"Tasks\",\n      \"paragraphs\": [\n        \"\\n        We set up the Programmer\\u2019s Assistant as a playground environment\\nthat participants could try out with a few sample programming\\nproblems. We created a tutorial to orient participants to the\\nassistant, its capabilities, and how to interact with it. We also created\\nfour programming challenges focused on writing code,\\ndocumenting code, and writing tests for code. We designed these challenges\\nto expose participants to a broad range of the assistant\\u2019s capabilities.\\nFor each of these challenges, we explicitly did not evaluate metrics\\nsuch as the participant\\u2019s productivity, the quality of their solutions,\\nor the time taken to produce them, as the focus of our study was to\\nunderstand the utility of conversational interaction. We selected\\nPython as the language used for the tutorial and challenges because\\nof its general popularity \\n        \",\n        \"After participants completed their solution to a challenge, they\\nsubmitted it by clicking a button in the UI. The code editor used\\nin the Programmer\\u2019s Assistant was not a fully-functional IDE and\\ndid not provide syntax checking or the ability to run, test, or debug\\ncode. Due to these limitations, participants were asked to submit\\ntheir solutions when they felt they had completed the challenge to\\ntheir own satisfaction.\\n4.2\"\n      ]\n    },\n    {\n      \"title\": \"Participants\",\n      \"paragraphs\": [\n        \"To recruit participants for our study, we posted internal\\nadvertisements in various communications channels focused on software\\nengineering. Our advertisements stated that we were evaluating a\\nconversational programming assistant, but were kept deliberately\\nvague in order to minimize the impact on peoples\\u2019 expectations of\\nthe experience.\",\n        \"\\n        Our advertisement yielded a pool of 140 potential participants.\\nIn order to recruit a diverse sample, we used a screening survey\\nthat asked about their job role, their familiarity with and recency\\nof use of Python, and their availability to participate in our study.\\nWe accepted participants into the study on a rolling basis, selecting\\nparticipants to capture a range of programming experiences and\\nensure balanced gender representation. We conducted periodic\\nreviews to determine whether we were learning something new from\\neach participant or if we had reached the point of saturation \\n        \",\n        \"Our participants had the following self-identified characteristics:\\n\\u2022 Job role: 19 Software Engineers, 12 Researcher/Scientists, 3\\nSoftware Architects, 2 Data Scientists, 1 Machine Learning\\nEngineer, 1 Systems Test Engineer, 1 Business Analyst, 1\\nManager, 1 Marketer, and 1 Consultant.\\n\\u2022 Gender: 21 Female, 19 Male, 1 Gender Variant /\\nNon-conforming, and 1 Preferred not to say.\\n\\u2022 Python Experience: 17 participants had 3+ years of Python\\nexperience, 11 had 1-3 years, 11 had less than 1 year, and 3\\nwere not familiar with Python.\\n\\u2022 Recency of Python Use: 29 participants had written Python\\ncode within the past month, 4 within the past year, 5 within\\nthe past 5 years, and 4 had not written Python code within\\nthe past 5 years.\",\n        \"We provide full demographic information for individual\\nparticipants in Appendix E.\\n4.3\"\n      ]\n    },\n    {\n      \"title\": \"Procedure\",\n      \"paragraphs\": [\n        \"Participants completed the study on their own time, independently\\nand without moderation. Each participant was provided with a web\\nlink to a pre-study survey that described the nature of the study and\\nthe tasks that they would be expected to perform. They were then\\ndirected to the Programmer\\u2019s Assistant to complete the tutorial and\\nthe four programming challenges. When participants indicated they\\nwere finished with the challenges 12, they were directed to a final\\npost-study survey. Complete sessions generally required about an\\nhour of efort, though some participants spread their efort across\\na longer period of time and across multiple sessions. Participants\\nwere compensated for their time at a rate equivalent to US $15/hr.\\n4.4\"\n      ]\n    },\n    {\n      \"title\": \"Measures\",\n      \"paragraphs\": [\n        \"We collected a variety of data in our study from three sources:\\n(1) Surveys. We employed three surveys in the study: a\\nprestudy survey to collect demographic information, a pre-task\\nsurvey to gauge expectations of the conversational user\\nexperience, and a post-task survey to assess actual user\\nexperience. We describe these survey questions in the relevant\\ncontext of our results, and we provide a complete listing of\\nall survey instruments in Appendix A.\\n(2) Event logs. The Programmer\\u2019s Assistant was instrumented\\nto collect data on participants\\u2019 usage. The event logs\\nprovided timestamped records of interaction events, including\\nconversational exchanges, hiding/showing the assistant, use\\nof the \\u201ctry again\\u201d and \\u201cstart over\\u201d features, and use of\\ncopy/paste.\\n(3) Conversation logs. From the event logs, we extracted\\nconversational transcripts between each participant and the\\nProgrammer\\u2019s Assistant.\\n5\\n5.1\"\n      ]\n    },\n    {\n      \"title\": \"RESULTS\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Data & Analysis\",\n      \"paragraphs\": [\n        \"We collected a wealth of data in our study: 126 survey responses\\nfrom three surveys per participant, containing 296 written\\ncomments in open-ended survey questions, and 4,877 instances of 23\\ndiferent types of UI events, including 1,699 conversational\\nexchanges13 in the event logs. We also compute, for each participant,\\ncounts or durations for 21 diferent metrics from the event logs.\",\n        \"In our analysis, we deliberately exclude the portion of our data\\ncollected during the tutorial exercise. We exclude this data because\\nthat activity was guided by the tutorial instructions, not by our\\nparticipants\\u2019 own initiative. Thus, our final sample consists of 3,172\\n12We did not enforce that participants actually complete all of the challenges.\\nNevertheless, all participants but one did submit solutions to all of the challenges.\\n13We refer to a participant\\u2019s utterance, followed by the assistant\\u2019s response, as a\\nconversational exchange.\\nevents, including 968 conversational exchanges in the event logs;\\nno survey data was excluded.\",\n        \"\\n        Our primary analysis of this data is qualitative, as our\\nparticipants provided us with a rich source of interesting feedback and\\nthought-provoking insights in their comments. Where applicable,\\nwe supplement this data with quantitative data from the survey\\nand the event logs, as well as chat transcript data from the\\nconversation logs. In this way, we triangulate \\n        \",\n        \"\\n        In order to set the context for our analysis, we first describe\\nhow we used reflexive thematic analysis to analyze participants\\u2019\\nresponses to the open-ended survey questions. We then describe\\nour analysis of the conversation logs and our development of a\\ncoding guide based on Conversation Analysis \\n        \",\n        \"Appears Fixated, Claims Ignorance, Didn\\u2019t Understand, Grants Request (Complete), Grants Request\\n(Incomplete), Ofers Help, Provided Wrong Answer, Requests Details, Requests Paraphrase, Response\\nIncludes Code, Spews Garbage\\ncodes to identify meta-information such as utterances that included\\ncode, utterances that referenced selected code, utterances that\\nimplicitly or explicitly referenced earlier portions of the conversation,\\nor non-verbal UI activities such as copies, pastes, and invocations\\nof \\u201ctry again\\u201d and \\u201cstart over.\\u201d Finally, we classified a subset of the\\nhuman-applied codes based on whether they represented a\\nparticipant\\u2019s task or social orientation toward the assistant. We list\\nour codes in Table 1, but note that not all of them ended up being\\nrelevant to our analysis.\",\n        \"When coding conversational data, we applied individual codes\\nat the level of each conversational utterance. We allowed multiple\\ncodes to be applied to each utterance to account for utterances that\\nperformed multiple functions (e.g. greeting and self-identification).\",\n        \"In order to ensure consistency in how our codebook was applied,\\ntwo authors coded a 10% sample of the 968 conversational\\nexchanges, achieving a satisfactory level of inter-rater reliability\\n(Krippendorf\\u2019s  = 0.77, where agreement was conservatively defined\\nas having all of the same codes applied to both utterances in a\\nconversational exchange).\\n5.2\"\n      ]\n    },\n    {\n      \"title\": \"Interlocutor\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Orientation\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Codes\",\n      \"paragraphs\": [\n        \"Appears Fixated, Claims Ignorance, Didn\\u2019t Understand, Grants Request (Complete), Grants Request\\n(Incomplete), Ofers Help, Provided Wrong Answer, Requests Details, Requests Paraphrase, Response\\nIncludes Code, Spews Garbage\\ncodes to identify meta-information such as utterances that included\\ncode, utterances that referenced selected code, utterances that\\nimplicitly or explicitly referenced earlier portions of the conversation,\\nor non-verbal UI activities such as copies, pastes, and invocations\\nof \\u201ctry again\\u201d and \\u201cstart over.\\u201d Finally, we classified a subset of the\\nhuman-applied codes based on whether they represented a\\nparticipant\\u2019s task or social orientation toward the assistant. We list\\nour codes in Table 1, but note that not all of them ended up being\\nrelevant to our analysis.\",\n        \"When coding conversational data, we applied individual codes\\nat the level of each conversational utterance. We allowed multiple\\ncodes to be applied to each utterance to account for utterances that\\nperformed multiple functions (e.g. greeting and self-identification).\",\n        \"In order to ensure consistency in how our codebook was applied,\\ntwo authors coded a 10% sample of the 968 conversational\\nexchanges, achieving a satisfactory level of inter-rater reliability\\n(Krippendorf\\u2019s  = 0.77, where agreement was conservatively defined\\nas having all of the same codes applied to both utterances in a\\nconversational exchange).\\n5.2\"\n      ]\n    },\n    {\n      \"title\": \"Expectations and Experience\",\n      \"paragraphs\": [\n        \"Pilot testing of the Programmer\\u2019s Assistant suggested that software\\nengineers would be skeptical of a conversational programming\\nassistant and its ability to provide useful assistance. Our study\\nrevealed that, for most participants, their actual experience after\\nusing the tool was better than they had anticipated. Participants\\nwere surprised at the quality of the assistant\\u2019s responses and they\\nappreciated how its integration with the code editor reduced the\\namount of context switching they needed to do in the UI. Some\\nparticipants struggled with the code selection feature, although\\nothers appreciated the ability to ask questions related to selected\\ncode.\\n5.2.1 Usage. All of our participants engaged with the\\nProgrammer\\u2019s Assistant while working on the challenges, despite there\\nbeing no requirement to do so. Forty-one participants submitted\\nsolutions to all four challenges, and one participant, P14, only\\nsubmitted solutions for one of the four challenges. Participants spent\\nan average of 68 minutes engaged with the assistant, as measured\\nby the amount of time the Programmer\\u2019s Assistant window was in\\nfocus.\",\n        \"Participants made an average of 23.0 utterances (SD = 15.1\\nutterances) to the assistant. On average, 6.2 of their utterances (SD =\\n4.3 utterances) contained a code selection. The average latency per\\nrequest14 was 6.7 seconds (SD = 3.1 seconds).\",\n        \"\\n        We saw a 66.3% rate of acceptance of generated code, where we\\nconsidered code to be accepted if the participant performed a copy\\nimmediately after the code was generated. This acceptance rate is\\nmuch higher than the 27% acceptance rate reported for Copilot \\n        \",\n        \"We developed a short inventory of six scale items to measure user\\nexperience of code work15. The scale was administered twice: once\\nbefore participants were exposed to the Programmer\\u2019s Assistant\\n(but after they had been briefed that they would interact with an AI\\nchatbot), and once after completing the programming challenges.\\nThe items were presented with the appropriate tense: Do you expect\\n(Did you find that) the Programmer\\u2019s Assistant: (a) will be (was)\\neasy to use; (b) will understand (understood) your requests; (c) will\\nprovide (provided) high quality responses; (d) will help (helped)\\nyou to write better code; (e) will help (helped) you to write code\\nmore quickly; (f) will be (was) enjoyable to use. Each item was rated\\non a 4-point scale of extent: Not at all (1), A little (2), Somewhat (3),\\nA great deal (4).\",\n        \"A factor analysis revealed the items on this scale measured a\\nsingle construct, which we identify as user experience (Cronbach\\u2019s\\n = 0.87). Thus, we computed two scores of user experience (UX)\\nfor each participant: a pre-task UX score computed as the average\\nof their six pre-task expectation scale responses, and a post-task\\nUX score computed as the average of their six post-task experience\\nscale responses.\",\n        \"We found that participants had lower initial expectations for\\ntheir experience with a conversational programming assistant\\n(pretask UX M (SD) = 3.0 (0.62) of 4) than their experience actually\\nwas (post-task UX M (SD) = 3.6 (0.32) of 4). A paired sample t-test\\nshows that this diference was significant,  (41) = 5.94,  < .001,\\nCohen\\u2019s  = 0.92 (large). Measured another way, 32 participants\\n(76.2%) had post-task UX ratings that were higher than their\\npretask expectations, demonstrating a significant shift in attitudes\\ntoward conversational programming assistance.\",\n        \"However, the UX ratings alone fail to capture participants\\u2019\\nnuanced expectations of the assistant and the reasons for their shifted\\nattitudes after using it. Participants expressed a variety of\\nexpectations of the assistant before using it, including that it would be easy\\nto use (P30) and produce correct responses (P30), understand the\\nproblem and what is being asked of it (P8, P9, P11), not interfere\\nwith their flow state (P5), produce imperfect or questionable\\noutputs (P6, P21), improve with feedback (P31), provide generic and\\nunhelpful answers (P17) or only answer basic questions (P40), and\\nproduce responses quickly (P40).\",\n        \"P17 expected \\u201cto be frustrated very quickly and that what I\\u2019d think\\nwould be relatively common questions would be responded to with\\ngeneric, unhelpful answers.\\u201d P6 explained, \\u201cI didn\\u2019t have very good\\nexperiences with chatbots. I think I\\u2019ll need to spend more time in\\nreviewing and fixing the suggestions than in writing the code myself\\nfrom scratch.\\u201d P11 had a more balanced view, that \\u201cIt\\u2019ll do some tasks\\nreally well, but others will not be as reliable.\\u201d\",\n        \"After interacting with the Programmer\\u2019s Assistant, many\\nparticipants commented on how the experience was better than they\\nanticipated, because it \\u201cseemed to be able to handle complex issues\\u201d\\n(P10) and \\u201cwas a great help\\u201d (P8). P20 felt it was \\u201cincredible!\\u201d P6 and\\nP17, who were both initially skeptical, reported having a positive\\nexperience. For P6, \\u201cIt absolutely exceeded all my expectations, in\\n15Our scale items were modeled from scales published in Weisz et al. 103, Table 9 \\u2013\\nAI Support that measured constructs including ease of use (item 3), response quality\\n(item 1), the production of higher-quality code (item 5), and the ability to write code\\nmore rapidly (item 4). We added additional items to cover the constructs of request\\nunderstanding and enjoyment, and we cast all items on a 4-point scale of extent.\\nall aspects that I could have imagined and more!\\u201d P17 provided a\\nmore quantitative assessment: \\u201cInitial expectations: 3 Actual: 9.5.\\u201d\\nP38 was emphatic in their evaluation: \\u201cI was blown away how well\\nit allowing me to structure how I want the code to look and work and\\njust giving me the thing I asked for.\\u201d\",\n        \"Many participants described a sense of surprise in their\\nexperiences. P9 was surprised by how well it understood their requests:\\n\\u201cI was surprised at how well the Programmer Assistant\\nwas able to understand my requests and generate good\\ncode/documentation/tests. It understood major concepts\\nand was able to explain it to me in a clear way, and it\\nwas also able to understand and write functional code.\",\n        \"It even was able to help me review my answer. I was also\\nsurprised at how well it could understand the context\\nof what I was asking in follow-up questions when I did\\nnot specify exactly what I was talking about, but rather\\nreferencing our prior conversation (such as, \\u2018what does\\nthat mean\\u2019).\\u201d (P9)\",\n        \"Similarly, P6 was surprised that they liked the conversational\\ninteraction when they expected that they wouldn\\u2019t:\\n\\u201cI thought I wouldn\\u2019t like the chatbot interaction and\\nthat I would prefer something like the tool I\\u2019ve seen in\\nthose demos of Copilot. But surprisingly, after using\\nthe chatbot (and seeing the results: easy to use, it\\nunderstands well, I felt it like a partner) I like this kind of\\nhelp.\\u201d (P6)\\n5.2.3 Quality of Assistant\\u2019s Responses. In order to gauge the quality\\nof responses produced by the Programmer\\u2019s Assistant, we examined\\nthe 910 task-oriented requests made by participants in the study.\\nFor the vast majority (80.2%), the assistant produced a correct\\nresponse (Grants Request (Complete)); in other cases, the assistant\\u2019s\\nresponse was incorrect (9.6%; Provided Wrong Answer), correct but\\nincomplete (4.4%; Grants Request (Incomplete)), or the assistant\\ndidn\\u2019t understand (3.4%; Didn\\u2019t Understand), claimed ignorance of\\nthe subject (1.5%; Claims Ignorance), or produced another type of\\nresponse (0.9%; Appears Fixated, Spews Garbage).\",\n        \"Participants also reported experiencing this variability in the\\nquality of the assistant\\u2019s responses. Some participants described\\nhow the assistant provided \\u201cdetailed answers\\u201d (P17) and \\u201chigh\\nquality outputs\\u201d (P18) that were \\u201csurprisingly good\\u201d (P2). P6 felt it was\\n\\u201cincredible to see the quality of the responses,\\u201d and P3 even explored\\nthe assistant\\u2019s capabilities outside the scope of the challenges and\\nfound that it could handle those as well:\\n\\u201cIt was surprising the quality of the code and the ability\\nto answer all my questions correctly. Although I think\\nthe challenges may be biased towards what the Assistant\\nis able to do, it was a great experience because I asked\\nmany other things and it was able to answer correctly.\\u201d\\n(P3)\",\n        \"Of course, the Programmer\\u2019s Assistant wasn\\u2019t perfect, and some\\nparticipants did run into issues. For P35, \\u201cThe documentation\\ngeneration did not perform very well.\\u201d P16 questioned the accuracy of the\\nknowledge encoded in the model: \\u201cDoes the model need to be\\nupdated? It said latest python version is 3.7 but google says it\\u2019s 3.10.\\u201d In\\nsome instances, participants needed to ask their question multiple\\ntimes to get a good response: \\u201cyou need to ask many times if you\\nwant to get an answer and also a detailed explanation\\u201d (P3). P27 felt,\\n\\u201cit was annoying when I asked it to try again and it would give me\\nthe same response.\\u201d P22 struggled because, \\u201cIt didn\\u2019t seem to handle\\nmultiple sentences well.\\u201d\",\n        \"P28 perhaps ofered the most scathing criticism, that, \\u201cIt makes\\nmistakes often enough to be not very practical.\\u201d However, despite\\nthe production of poorer-quality responses, other participants felt\\nthat the assistant was still helpful. P36 reported that, \\u201cOnly minor\\ntweaks were normally needed to correct any issues.\\u201d Similarly, P38\\ndescribed how the assistant wasn\\u2019t able to completely solve their\\nproblem, but provided a useful start:\\n\\u201cThere was only one hickup I noticed where when I\\nasked it to memoize fibonacci it couldn\\u2019t, but it dropped\\nthe building blocks on my lap for me to finish so that\\nwas fine, that was like minutes of efort on my part.\\u201d\\n(P38)\\n5.2.4 UI Design & Afordances. Participants made many comments\\non our specific UI design and the afordances provided (or not\\nprovided) in our chat-augmented editor. Overall, the integration\\nbetween the chat pane and the code editor was \\u201cvery good\\u201d (P23),\\nwith a \\u201cnice interface between the code pane and the assistant pane\\u201d\\n(P17) that \\u201cmakes it really convenient\\u201d (P35).\",\n        \"\\n        Prior research by Brandt et al. \\n        \",\n        \"Some aspects of our user interface were confusing to participants,\\nsuch as the mechanism for selecting code to be included in the\\nconversational context. P7 remarked, \\u201cIt\\u2019s was a little confusing\\ndoing the selection part for it to tell me what a function does, but...\\nit gave me code that was insanely easy to copy and paste.\\u201d Other\\nparticipants appreciated the code selection mechanism, such as P11:\\n\\u201cI enjoyed the code selection feature, and found that very easy to use.\\u201d\\nIn the event logs, we identified 20 instances in which a participant\\nunintentionally included selected code in the conversation when\\nit wasn\\u2019t needed (Includes Extraneous Selection), 12 instances in\\nwhich a code selection was omitted when it was needed to provide\\ncontext for the question (Missing Selection), and 16 instances in\\nwhich a participant copy/pasted code directly into the chat rather\\nthan selecting it in the editor (Pasted Code in Chat). Although\\nthese cases represent a small fraction of the 227 instances in which\\na code selection was required and included in the conversation\\n(Includes Selection), their presence does indicate that more attention\\nis needed to the interaction design of code selection.\",\n        \"Another issue regarded the awareness of the \\u201ctry again\\u201d and\\n\\u201cstart over\\u201d features. The \\u201ctry again\\u201d feature was only used by 14\\nparticipants, who used it a total of 63 times over the course of\\nthe study. Some participants used it specifically when they got an\\nanswer which they saw as clearly wrong, while others used it to\\nget a variety of possible answers before proceeding. The \\u201cstart over\\u201d\\nfeature was used even less, by 5 participants who used it a total of\\n6 times. Despite our efort to surface these conversational features\\nin the UI via shortcut buttons, they may not have been suficiently\\nnoticeable or salient: \\u201cThe \\u2018try again\\u2019 button is not so reachable, often\\ntimes I forgot it exists\\u201d (P23). By contrast, at least one participant\\nwas successful with these features:\\n\\u201cat some point it had issue with challenge 3 and I had to\\nstart over. Just asking \\u2018try again\\u2019 was not enough and I\\nwas getting always the same (wrong and not related)\\nanswer. starting again solved the issue!\\u201d (P20)\\n5.3\"\n      ]\n    },\n    {\n      \"title\": \"Utility of Conversational Assistance\",\n      \"paragraphs\": [\n        \"\\n        Our next set of themes concerns the utility provided by\\nconversational programming assistance. Participants felt the assistant was\\nhighly valuable and desired to use it in their own work. They felt it\\nwould be most helpful for smaller or narrowly-scoped tasks, but\\nable to provide a wide variety of types of assistance. The fact that\\nthe interaction model was conversational and grounded in code\\nwere valuable aspects, as was the ability for the assistant to bolster\\nusers\\u2019 learning about programming topics through that\\ninteraction. Participants did question whether they could trust and rely\\nupon the assistant\\u2019s responses, echoing a similar theme discussed\\nin Weisz et al. \\n        \",\n        \"Participants described a wide variety of tasks for which they\\nfelt the assistant would be useful. These tasks included \\u201cordinary\\u201d\\n(P23), \\u201csimpler\\u201d (P2), and \\u201csmall, repetitive\\u201d (P4) tasks such as \\u201cquick\\nlookups\\u201d (P25) for \\u201cshort chunks of code\\u201d (P11) or for \\u201cnarrowed\\nquestions\\u201d (P26). Participants also felt the assistant was useful for\\n\\u201csmall containable novel algorithms\\u201d (P38) and \\u201clittle coding problems\\u201d\\n(P4).\",\n        \"Several kinds of task assistance were reported as being valuable,\\nsuch as explaining code (P31), implementing business logic in a\\nUI (P38), understanding what code does (P19, P37), and recalling\\nlanguage syntax, method names, and arguments (P12, P15, P20, P40,\\nP42). P27 felt that the assistant was \\u201cMore helpful when recognizing\\na specific well known algorithm but not things you make yourself.\\u201d\",\n        \"Participants also made recommendations for how to increase\\nthe value of the Programmer\\u2019s Assistant. P38 suggested, \\u201cWhat\\nwould blow me away though is if it\\u2019s able to help with what I do most\\noften which is to integrate, refactor and iterate on an existing system.\\u201d\\nP16, P26, and P38 all desired more information on the data sources\\nused to produce the assistant\\u2019s responses. P9 requested to \\u201cHave\\nthe Programmer\\u2019s Assistant examine your code and make proactive\\nsuggestions for improving it in the chat.\\u201d P36 requested the same,\\nbut cautioned that, \\u201cCare would need to be taken to avoid becoming\\nan annoyance or disrupting the flow of a coding session.\\u201d\",\n        \"In the post-task survey, we probed participants on how certain\\nchanges to the Programmer\\u2019s Assistant would either decrease,\\nincrease, or result in no change to its value. Over 75% of participants\\nfelt that the assistant would be more valuable if it operated in a\\nproactive manner, either by making improvement suggestions in\\nthe chat or as comments directly in the code. Similarly, 78.6% of\\nparticipants felt that having more buttons in the UI for common\\nfeatures such as explaining or documenting code would make the\\ntool more valuable.\\n5.3.2 Conversational Interactions Grounded in Code. One of the\\nchallenges in interpreting participants\\u2019 comments about the utility\\nof the Programmer\\u2019s Assistant was in disentangling the extent to\\nwhich value was derived from the quality of the underlying model\\nversus the integration of conversation in a code context. Indeed,\\nparticipants felt that the chat interaction was valuable: 69.0% of\\nparticipants felt that eliminating the conversational interaction\\nand making the assistant behave more like web search would\\ndecrease its value. Further, our analysis of the conversation transcripts\\nrevealed that 42% of the 910 task-oriented utterances from\\nparticipants required historical conversational context (Chat Context\\nRequired) in order to be correctly interpreted. Thus, we observe\\nthat behaviorally, participants did rely on conversational context\\nin their interactions.\",\n        \"In the post-task survey, 83% of participants rated the importance\\nof the ability to ask follow-up questions as being \\u201csomewhat\\u201d or \\u201ca\\ngreat deal.\\u201d Several participants specifically commented on the value\\nof this conversational context. P39 remarked, \\u201cI absolutely loved\\nhow you can straight up ask follow-up questions to the Programmers\\u2019\\nAssistant without having to reiterate the original topic/question.\\u201d P15\\nexpressed a similar sentiment, saying, \\u201cI think the conversational\\ncontext was someone helpful, just in communicating that it\\u2019s a running\\nconversation where my context is remembered.\\u201d P9 provided a similar\\nanalysis:\\n\\u201cThis tool was so helpful at answering questions I had\\nabout the code in the context of the code I am working\\non... I was also impressed with how well it was able to\\nremember the context of our conversation, especially\\nwhen I asked vague follow-up questions.\\u201d (P9)\",\n        \"In addition, some participants identified how a conversational\\ninteraction grounded in code was useful, \\u201cbecause I think to\\n\\u2018understand\\u2019 the dev context could be VERY important\\u201d (P31). In fact,\\n24.9% of task-oriented utterances included a relevant code selection\\n(Includes Selection), showing that participants valued this ability.\",\n        \"Contrasting with these participants, P18 felt that interacting\\nwith the assistant conversationally was tedious, and they employed\\na more direct approach:\\n\\u201cI really like the PA. But, I didn\\u2019t converse with it like\\na chat bot. I often told it what to do (\\u2018Document this\\ncode.\\u2019) as opposed to asking it what to do (\\u2018How do I\\ndocument this code?\\u2019). Talking to it the way that was\\nsuggested in the tutorial seemed overly verbose/tedious.\\u201d\\n(P18)\",\n        \"Despite these individual diferences in interaction preferences,\\nP39 envisioned that both interaction styles could be supported in\\nthe tool:\\n\\u201cI think both options should exist: people should be able\\nto input their queries like a search bar AND also give\\ntheir question as if in conversation.\\u201d (P39)\\n5.3.3 Learning Efects. One specific benefit of the Programmer\\u2019s\\nAssistant identified by participants is its ability to help people\\nimprove their programming skills and reinforce knowledge gaps.\\nFor example, it can help users \\u201cremember how to do things in certain\\nlanguages... such as, when I am using a language I haven\\u2019t used in a\\nwhile\\u201d (P9). The assistant can also serve as an memory aid, such\\nas when \\u201cI use a lot of libraries that I don\\u2019t always remember all\\nof the functions\\u201d (P15). Similarly, P31 said, \\u201cNo matter how good\\nyou\\u2019re as a developer, you can\\u2019t (humanly) remember all the API of\\nhundreds of libs or new languages... I\\u2019d learn new dev lang and new\\nlib/frameworks faster.\\u201d\",\n        \"P39 felt the assistant \\u201cis perfect for programmers of all levels,\\u201d and\\nP1 felt it could help them rapidly improve their Python skills:\\n\\u201cI have wanted to learn python... The main concern\\nhow much time spent learning is needed before I could\\nactually get some value out of learning python. I have\\na feeling this would cut that time down from weeks to\\na day or so.\\u201d (P1)\",\n        \"P39 also identified the fact that, because the interactions with\\nthe assistant are conversational, it forces people to learn how to\\ncommunicate to others about their code:\\n\\u201cThe conversation aspect promotes proper\\ncommunication, which would really stand to benefit budding\\nprogrammers if they want to learn how to explain\\nconcepts more fluently in the future to their colleagues.\\u201d\\n(P39)\",\n        \"Conversely, P36 suggested that over-reliance on programming\\nassistance might have a detrimental efect to one\\u2019s learning:\\n\\u201cIt\\u2019s definitely a huge time saver, but over-reliance on it\\nmay cause new developers to skip learning the reference\\nmaterial themselves and discovering new things and\\nsparking new ideas.\\u201d (P36)\\n5.3.4 Trust. Many participants raised questions about whether\\nthey could trust the responses provided by the Programmer\\u2019s\\nAssistant. P21 asked this question most directly: \\u201cwill the code be correct,\\nsafe, eficient?\\u201d Other participants raised similar questions, such as,\\n\\u201cI\\u2019m wondering how it validates it\\u2019s answers, if it can be trusted to\\nalways give a working answer\\u201d (P10), and \\u201cSometimes lack of source\\nand context may raise doubts in the mind of the programmer\\u201d (P16).\",\n        \"These issues of trust were exacerbated by the fact that the\\nProgrammer\\u2019s Assistant did not allow participants to actually run their\\ncode. Because of this limitation, participants had to rely on their\\nown knowledge to judge the correctness of the assistant\\u2019s responses.\\nP19 asserted, \\u201cThere is no way to evaluate if the Programmer\\u2019s\\nassistant is giving you the right advise or not other than your own\\nknowledge,\\u201d and P9 concurred: \\u201cI had to trust that it was correct (and\\nuse my own prior knowledge).\\u201d\\nSocrates : I think you would do something like this :\\ndef test_euclid():\\nassert euclid(2, 4) == 2\\nassert euclid(6, 9) == 3\\nassert euclid(17, 12) == 1\\nassert euclid(4, 4) == 4\\nwhat is a euclid function\\nSocrates : The euclid function is a function that\\ncomputes the greatest common divisor of two\\nnumbers .\\nis this a good unit test for a euclid\\nfunction\\nSocrates : I believe that it is a good unit test for\\na euclid function .\",\n        \"P9 :\",\n        \"why\\nSocrates : It seems to me that it tests the edge\\ncases of a and b being equal , a being\\ngreater than b , and b being greater than a.\",\n        \"P18 described the potential consequences of allowing the\\nassistant to write code for them:\\n\\u201cThe only thing that made me nervous was that it could\\nhave introduced a bug that wasn\\u2019t immediately\\napparent. And given I didn\\u2019t write the code, I could have easily\\nglossed over a mistake when reviewing it. Especially if\\nit is also the one writing the test cases.\\u201d (P18)\",\n        \"Despite our eforts to make the Programmer\\u2019s Assistant respond\\nin non-authoritative ways, we did observe participants sometimes\\nuncritically accept generated results that were clearly wrong or\\nincomplete. Thus, we did find behavioral evidence for over-reliance.\",\n        \"what is a unit test ?\\nSocrates : A unit test is a test of a single function\\nor method . It is typically written by the\\nprogrammer who wrote the code .\\ndef euclid(a, b):\\nwhile b != 0:\",\n        \"(a, b) = (b, a % b)\\nreturn a\\nhow would I write a unit test for this code ?\\nthanks Socrates this was helpful\\nSocrates : Always happy to help .\",\n        \"One way to address trust issues is for the assistant to provide\\nfurther explanations and justifications that can calibrate a user\\u2019s\\nconfidence in the assistant\\u2019s responses. Such explanations could\\nbe requested conversationally, though most participants did not\\nattempt to do so. One participant (P9) did ask for such explanations,\\nand we show a summary of their transcript in Listing 2. In this\\ninstance, P9 asked for a definition of a unit test (line 1), an\\nexplanation of the code being tested (line 25), and justifications of the\\nquality of the unit test (lines 31& 37). Thus, we observe that the\\nassistant is capable of producing explanations and justifications\\nwhen asked.\\n5.4\"\n      ]\n    },\n    {\n      \"title\": \"Listing 2: Building trust through explanations and justifications\",\n      \"paragraphs\": [\n        \"what is a unit test ?\\nSocrates : A unit test is a test of a single function\\nor method . It is typically written by the\\nprogrammer who wrote the code .\\ndef euclid(a, b):\\nwhile b != 0:\",\n        \"(a, b) = (b, a % b)\\nreturn a\\nhow would I write a unit test for this code ?\\nthanks Socrates this was helpful\\nSocrates : Always happy to help .\",\n        \"One way to address trust issues is for the assistant to provide\\nfurther explanations and justifications that can calibrate a user\\u2019s\\nconfidence in the assistant\\u2019s responses. Such explanations could\\nbe requested conversationally, though most participants did not\\nattempt to do so. One participant (P9) did ask for such explanations,\\nand we show a summary of their transcript in Listing 2. In this\\ninstance, P9 asked for a definition of a unit test (line 1), an\\nexplanation of the code being tested (line 25), and justifications of the\\nquality of the unit test (lines 31& 37). Thus, we observe that the\\nassistant is capable of producing explanations and justifications\\nwhen asked.\\n5.4\"\n      ]\n    },\n    {\n      \"title\": \"Patterns of Interaction and Mental Models\",\n      \"paragraphs\": [\n        \"Participants interacted with the assistant in a variety of ways with\\ntwo main patterns of usage standing out: (1) invoking the\\nassistant to solve the entire programming challenge, and (2) breaking\\nthe challenge down into a set of smaller tasks and invoking the\\nassistant\\u2019s help for each. There were no clear diferences in how\\nparticipants with difering Python experience approached the tasks.\",\n        \"Participants\\u2019 mental models of the assistant also varied. Although\\nparticipants strongly saw the role of the assistant as being a tool,\\ntheir behaviors revealed that in many cases, they actually treated it\\nas a social agent. In addition, participants ascribed various mental\\ncapacities to the assistant, such as having the ability to understand,\\ncompute, and learn.\",\n        \"Participants felt the assistant changed the nature of their work\\nprocess. For some participants, it enabled them to focus on the\\nhigher-level aspects of development because the assistant handled\\nlower-level details or provided partial solutions for them to build\\nupon. Many participants felt the assistant sped up their work and\\nhelped them remain focused on their tasks.\",\n        \"Finally, participants drew comparisons between the\\nProgrammer\\u2019s Assistant with other forms of programming support such\\nas Copilot and web search. They felt that the conversational style\\nof interaction enabled them to discover new, emergent behaviors\\nfrom the model that were unavailable from Copilot\\u2019s focus on code\\nautocompletion. They also felt that the examples provided by the\\nassistant were more readily usable within their own code compared\\nto browsing for answers within search results, speeding up the\\ncoding process. However, some participants advocated for a\\nbalanced approach to the design of programming assistance tools by\\nincorporating multiple modes of interaction rather than fixating\\non a single one.\\n5.4.1 Interaction Styles and Assistant Role. We observed that\\nparticipants interacted with the Programmer\\u2019s Assistant in strikingly\\ndiferent ways. Some participants would present the entire\\nchallenge description to the assistant and then work with the results it\\nproduced. Other participants approached the programming\\nchallenges in a piecemeal fashion, breaking them apart into a set of\\nsmaller tasks, then invoking the assistant to aid with each one.\",\n        \"Experience with Python was not a determinant of how\\nparticipants approached the programming challenges, but it did seem to\\nimpact how participants interacted with the assistant. Less\\nexperienced participants tended to ask the assistant basic questions such\\nas, \\u201cWhat is a unit test\\u201d (P29, not familiar with Python) and \\u201chow\\ndo I document a function?\\u201d (P27, < 1 year of experience). More\\nexperienced participants made detailed requests about specific Python\\nlibraries or algorithms, such as, \\u201cgiven a pandas dataframe with\\ntwo columns \\u2018Date\\u2019 and \\u2018Sales\\u2019 please use matplotlib to draw me a\\nscatterplot\\u201d (P38, 3+ years of experience) and \\u201cimplement a\\nrungekutta algorithm for solving an ODE with adaptive time steps\\u201d (P37,\\n3+ years of experience).\",\n        \"Another diference we observed in how people interacted with\\nthe assistant stemmed from their view on the role it played in their\\ncollaborative process. Some participants, such as P18, treated it\\nmore as a tool by issuing commands rather than asking questions.\\nAs quoted earlier, they said, \\u201cI didn\\u2019t converse with it like a chat\\nbot.\\u201d P5 described their interaction style similarly: \\u201cI found myself\\nwanting to type search queries into Socrates, not treating it as a person\\nbut as a search tool.\\u201d\",\n        \"In anticipation that participants would have diferent\\norientations to the assistant and its role, we asked a question on the\\nposttask survey about the diferent kinds of roles the assistant might\\ntake. These roles generally fell into one of two categories: a tool\\norientation (a tool, a reference guide, a content generator, a problem\\nsolver), and a social orientation (a collaborator, a colleague, a coach,\\nan advisor, a reviewer). Participants rated the extent to which they\\nviewed the Programmer\\u2019s Assistant in each of these roles on a\\n4point scale of extent: Not at all (1), A little (2), Somewhat (3), or A\\ngreat deal (4).\",\n        \"100\\ns\\ntna 75\\np\\niitr\\nc\\na\\np\\nfo 50\\ne\\ng\\na\\nt\\nn\\ne\\nrce 25\\nP\\n0\",\n        \"Tool\\ngCeonnetreantotr Regfeurideence Col aborator Psrooblvleerm\",\n        \"Role\",\n        \"Advisor Coach Reviewer Col eague\\nRating\",\n        \"Not at al\",\n        \"A little\",\n        \"Somewhat\",\n        \"A great deal\",\n        \"We show participants\\u2019 ratings of the assistant\\u2019s role in Figure 3.\\nDespite the fact that their attitudes toward the assistant\\noverwhelmingly reflected a tool orientation, their behaviors reveal that many\\nparticipants actually treated the assistant as a social agent. P6\\ndescribed how \\u201cI felt it like a partner,\\u201d and P4 told the assistant, \\u201cI\\ncould not have solved the challenge without your help,\\u201d to which\\nthe assistant responded, \\u201cI\\u2019m glad I could help.\\u201d\",\n        \"\\n        The literature on Computers as Social Agents (CASA) helps us\\ninterpret this result as it demonstrates how computers are often\\ntreated like people \\n        \",\n        \"In the conversation logs, we identified participants who\\ninteracted with the assistant in a socially-oriented fashion (the social\\norientation codes in Table 1). Twenty participants (47.6%) made at\\nleast one socially-oriented utterance. An extreme form of this\\ninteraction style can be seen in a snippet from P6\\u2019s transcript (Listing 3).\",\n        \"The 20 participants with a behaviorally-demonstrated social\\norientation did not generally difer in their role ratings from other\\nparticipants, except that they rated the assistant as more likely to\\nbe an advisor (Fisher\\u2019s exact test, two-tailed  = .02) or a reviewer\\n(Fisher\\u2019s exact test, two-tailed  = .03). However, they did not difer\\nin their overwhelmingly-strong ratings of the tool orientations.\\nThus, at least for some participants, there seems to be a dissonance\\nin their view of the assistant\\u2019s role orientation.\",\n        \"Listing 3: Excerpt from P6\\u2019s interaction with the\\nProgrammer\\u2019s Assistant, in which P6 ofers their thanks and\\ncongratulations.\\n5.4.2 Mental Capacities. Participants made a number of inferences\\nabout the Programmer\\u2019s Assistant and its capacities for thought.\\nMany participants talked about how the assistant possessed a level\\nof \\u201cunderstanding\\u201d (P6, P8, P11, P18, P32) of \\u201cthe context\\u201d (P9, P21)\\nas well as \\u201cmajor concepts\\u201d (P9) and \\u201cknowledge\\u201d (P33). P24 was\\namazed by the assistant\\u2019s ability to \\u201ctake a plain english request\\nand interpret it properly.\\u201d P7 ascribed intelligence to the assistant,\\nsaying, \\u201cIt was a lot smarter and trained than I thought it was.\\u201d\",\n        \"One participant assumed that the assistant \\u201cKeeps improving\\nthrough (user) feedback\\u201d (P31). Another felt that the assistant was\\ncapable of computation: \\u201cIt understands the problem... It can calculate\\nthe results of a function back\\u201d (P8).\",\n        \"However, not all participants were convinced of the assistant\\u2019s\\nability to understand. P37 questioned the assistant\\u2019s limitations: \\u201cI\\nwonder how far beyond boilerplate it can go and if it works for truly\\noriginal problems.\\u201d\\n5.4.3 Impact of Conversational Assistance on Work Practices. Many\\nparticipants discussed how the Programmer\\u2019s Assistant shaped\\ntheir work practices on the programming challenges. Overall,\\nparticipants felt that the assistant \\u201csaves time\\u201d (P10), \\u201chelps me code\\nfaster\\u201d (P34), and would \\u201cspeed up my productivity\\u201d (P19) because \\u201cI\\ncould focus on validating and improving the code it generated instead\\nof having to write it all from scratch\\u201d (P18). P37 remarked that, \\u201cIt\\nopens a whole new door for fast develpment.\\u201d P4 discussed how the\\nassistant \\u201cwas helpful in staying focused on the code,\\u201d although for\\nP14, \\u201cit took me time to get into tempo with the tool.\\u201d\",\n        \"P31 pointed out how the assistant would change the nature of\\ntheir work:\\n\\u201cMy job could focus more on higher level aspects and\\ntherefore achieving better (quality) results, besides the\\ntime-to-value... Data science (and dev) becomes a more\\ncreative-higher level experience.\\u201d (P31)\",\n        \"Other participants discussed a work process in which the\\nassistant provided incomplete solutions \\u2013 the \\u201cbuilding blocks\\u201d (P38) or\\n\\u201cinitial draft of code\\u201d (P11) \\u2013 upon which they could build. P5 aptly\\ndescribed this process:\\n\\u201cIt\\u2019s nice to copy well formulated challenges in natural\\nlanguage and have the code generator take its best stab\\nat it, then edit to our hearts content.\\u201d (P5)\",\n        \"Participants felt that human review of the assistant\\u2019s responses\\nwas necessary because \\u201cThe answers provided are generally not\\nnovel solutions, often look clunky and non-elegant. There may be\\nsome unnecessary code. Basically the code would need to be reviewed\\u201d\\n(P16). P35 also pointed out how \\u201cThe code generator was good but\\nyou still have to really check it.\\u201d P19 discussed how they would turn\\nto the assistant as a first source for support, and only if it wasn\\u2019t\\nable to help would they then turn to other support tools:\\n\\u201cThe way I will use it is, I will first use the\\nProgrammer\\u2019s assistant for most of my cases. Only in certain\\ncases where Programmer\\u2019s assistant cant answer things\\nI will turn up to oficial documentation or stack overflow.\\u201d\\n(P19)\",\n        \"However, latency was a factor for interactive use of the assistant\\nand participants noticed when the assistant took a long time to\\nrespond. P19 remarked, \\u201cSometimes it took lot of time, like more than\\n5 seconds.\\u201d P40 also felt \\u201cthe response was a little slow sometimes...\\nin chat mode I expect faster responses.\\u201d As discussed in Section 5.2.1,\\nthe assistant took an average of 6.7 seconds (SD = 3.1 seconds)\\nto respond to a request, and participants did appreciate when the\\nassistant produced rapid responses: \\u201cI loved how quick it was able\\nto pull up answers to questions I had\\u201d (P38).\\n5.4.4 Conversational Interaction vs. Other Interaction Models.\\nAlthough our study was not intended to make comparative\\nevaluations with the Copilot tool, we nonetheless asked participants\\nwhether they were familiar with Copilot, and if so, to comment on\\nhow the two tools compared. We also asked a similar question to\\ncompare the assistant with another popular form of programming\\nassistance, searching the web (via a search engine like Google, or\\na Q&A site like Stack Overflow). In discussing the diferences\\nbetween these three tools, we note that the primary diferentiator is\\ntheir interaction model.\",\n        \"\\n        The interaction model for the Programmer\\u2019s Assistant is clearly\\nconversational: users ask questions in natural language and are\\nprovided with a response in natural language and/or code. The\\ninteraction model of Copilot is reminiscent of direct manipulation\\ninterfaces \\n        \",\n        \"\\n        We also note that the Programmer\\u2019s Assistant and Copilot both\\nutilize the same underlying AI model, Codex \\n        \",\n        \"Participants reported various benefits and drawbacks of a\\nconversational interaction over a direct manipulation interaction.\\nForemost, conversation \\u201cfelt very natural\\u201d (P21) and \\u201cfeels much more\\nnatural using Natural Language with the AI\\u201d (P39). In addition, P39\\nfelt that \\u201cthe use cases of Programmers\\u2019 Assistant seem more\\nopenended.\\u201d Many participants were surprised at the variety of tasks the\\nassistant was capable of performing, from writing unit tests (P19,\\nP36, P37) and documentation (P12, P19, P36, P37) to explaining\\nwhat code did (P31, P38) and even answering general-knowledge\\nquestions (P31). Again, we note that the Programmer\\u2019s Assistant\\nutilizes the same underlying model as Copilot, yet the\\nconversational interface was able to expose a wider variety of emergent\\nbehaviors from the model. Multiple participants explored the limits\\nof the assistant\\u2019s knowledge and abilities beyond our programming\\nchallenges. For example, P37 asked it questions about physics and\\nordinary diferential equations (\\u201cODe\\u201d as written by P37), and was\\nsurprised by the \\u201cversatility of what it could answer.\\u201d\\n\\u201cI asked it some physics and ODe question and the\\nanswers, though not complete, included the key parts\\nneeded to write that code.\\u201d (P37)\",\n        \"P31 probed the assistant on its knowledge of geography and was\\nsurprised when the assistant produced a correct answer.\\n\\u201cI asked something out of SW engineering domain\\n(geography) and it replied correctly, also by correctly\\nanswering on my nationality.\\u201d (P31)\",\n        \"\\n        For some participants, the ability to assess the assistant\\u2019s\\nresponse before committing to it (i.e. by inserting assistant-generated\\ncode into their editor) was a boon. P15 described how the\\ncopy/paste boundary provided them with \\u201ca bit more control to ask specific\\nquestions about what I wanted and to assess before putting it in my\\ncode.\\u201d Other participants felt that the copy/paste boundary was\\nmore ineficient:\\n\\u201cI think the main diference is the ability of Copilot to\\nsuggest code while you type, what make it faster and\\neasier to use. While using the Programmer\\u2019s Assistant,\\nyou need to go to the chat, ask the question, copy the\\n16As an aside, our comparison of direct manipulation, search, and conversational\\ninteraction models is reminiscent of historical comparisons of text-based and graphical user\\ninterfaces \\n        \",\n        \"A large number of participants felt that the conversational\\ninteraction was faster than web search (P1, P6, P7, P10, P11, P12, P16,\\nP17, P18, P20, P24, P29, P30, P33, P36, P37, P42) because of its ability\\nto provide \\u201creal-time responses\\u201d (P32) that can be \\u201capplied exactly to\\nyour code\\u201d (P33) without having to \\u201cparse through lots of text... to get\\nwhat you need\\u201d (P15). In addition, the assistant provided \\u201cMUCH\\nfaster, better responses\\u201d (P17) that were \\u201cmuch more relevant to the\\nproblems\\u201d (P34) and \\u201csimple and succinct\\u201d (P9), without having to\\n\\u201csort through answers on your own or read documentation\\u201d (P9) or\\n\\u201clook at many posts before finding the relevant one\\u201d (P18).\",\n        \"Despite these benefits, some participants felt that the assistant\\nmight not work well for \\u201cmore specific and dificult problems on\\na bigger scale\\u201d as compared to web search. P9 felt that \\u201cthe data\\nof the Programmer\\u2019s Assistant wasn\\u2019t as rich\\u201d as the web. Other\\nparticipants felt that the assistant lacked the \\u201cmultiple answers\\u201d (P9)\\nand \\u201crich social commentary\\u201d (P19) that accompanies answers on\\nQ&A sites:\\n\\u201cI like to see the diferent versions proposed on stack\\noverflow and the commentary of what makes one\\nsolution better than another in a given situation.\\u201d (P27)\",\n        \"Some participants promoted a more balanced view that there\\nisn\\u2019t a single mode of interaction superior to all others. P19 felt that\\nweb search would be a fallback when the assistant failed to answer\\na question. P39 described how search could be integrated with the\\nconversational interaction:\\n\\u201cI think both options should exist: people should be able\\nto input their queries like a search bar AND also give\\ntheir question as if in conversation.\\u201d (P39)\"\n      ]\n    },\n    {\n      \"title\": \"DISCUSSION\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Value of Conversational Interaction\",\n      \"paragraphs\": [\n        \"We began our research by asking the question of whether\\ncontemporary developments in code-fluent LLMs could suficiently\\nsupport a conversational programming assistant. We believe that\\nour work has demonstrated that they can. Clearly, the\\nProgrammer\\u2019s Assistant was viewed by our participants as a useful tool that\\nprovided real value \\u2013 so much so that many participants\\nexplicitly requested or expressed the desire to use it in their own work.\\nHowever, how much of this value was derived from the model itself\\nand its ability to produce high-quality responses to programming\\nquestions, versus from participants\\u2019 ability to conduct extended\\nconversational interactions grounded in their actual source code?\",\n        \"We believe that both of these constituent aspects were valuable.\\nIndeed, many participants commented on their surprise and\\nsatisfaction with the quality of the assistant\\u2019s responses (Section 5.2.3).\\nHowever, participants also valued the conversational interactions\\nthat they had with the assistant. In the event logs, we saw\\nevidence that participants were leveraging conversational context to\\nask follow-up questions as well as leveraging code context by\\nasking about their code selections (Section 5.3.2). Many participants\\nreported that they would find the tool less valuable if the\\nconversational interaction were removed (Section 5.3.2). Further,\\nconversation seemed to provide unique value beyond other interaction\\nmodels (direct manipulation and search) because of its\\nembeddedness in the UI and its ability to surface emergent behaviors of the\\nmodel (Section 5.4.4).\",\n        \"\\n        We do not believe that these diferent interaction models are\\nin competition and we agree with P39\\u2019s assessment that assistive\\ntools can be built using a plethora of diferent interaction models.\\nFor use cases in which a model is known to produce high-quality\\nresults (e.g. code autocompletion for Codex), a direct manipulation\\ninterface seems wholly appropriate as it would provide a\\ndiscoverable and predictable way of invoking the model to produce a\\nknown type of result. However, direct manipulation interfaces may\\nbe less ideal for surfacing the emergent behaviors of a foundation\\nmodel \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Toward Human-AI Synergy\",\n      \"paragraphs\": [\n        \"\\n        The aim of human-centered AI is to \\u201cenable people to see, think,\\ncreate, and act in extraordinary ways, by combining potent user\\nexperiences with embedded AI methods to support services that\\nusers want\\u201d \\n        \",\n        \"\\n        Our study begins to address this gap. While many co-creative\\nsystems examine casual tasks or experimental activities (e.g., Spoto\\nand Oleynik \\n        \",\n        \"\\n        Campero et al. \\n        \",\n        \"\\n        We believe such evaluations of human-AI synergy should go\\nbeyond one-shot performance measures. As implied by many of the\\nuses cases listed by Seeber et al. \\n        \",\n        \"\\n        Evidence for this process surfaced in our study when participants\\ndescribed how they could improve their programming skills by\\ninteracting with the assistant (Section 5.3.3). We assert that the\\nlearning should operate in both directions: not only should people\\nimprove their programming skills, but the model itself can also\\nimprove based on peoples\\u2019 interactions with it. For example, when\\nthe assistant provides a code example to the user, and the user\\ntakes that example and edits it, those edits constitute feedback that\\ncan be used to further fine-tune the model. In addition, through\\nlongitudinal use, we believe that human and AI partners can create\\nreciprocal representations of one another \\u2013 i.e., the human is likely\\nto create a mental model of the AI, and the AI may be engineered to\\ndevelop a user model for each of its human users \\n        \",\n        \"With such models, an assistant that knows a user is learning a\\nprogramming language could provide natural language\\nexplanations alongside code outputs, whereas an assistant that knows a\\nuser is strongly skilled in a programming language might shorten\\nor omit those explanations. Similarly, users are likely to update\\ntheir mental models of the AI with more experience. We believe the\\nspace for exploring how these reciprocal models impact human-AI\\nsynergy is rich, and we encourage additional work in this area.\",\n        \"\\n        Human-centered AI systems that are designed to combine and\\nsynergize the distinct skills of humans and AI models cannot\\nsucceed if they diminish the human skills upon which they depend.\\nWell-designed human-centered AI systems develop new and\\ncomplementary skills for both the human and AI constituents \\n        \",\n        \"Ultimately, the design decisions that go into an interactive AI\\nsystem have ethical implications. Our design attempts to augment\\nthe user\\u2019s knowledge and skills by presenting help on demand,\\ncouched in non-authoritative suggestions, which leaves the user\\nifrmly in control and ultimately responsible for the work product.\\n6.3\"\n      ]\n    },\n    {\n      \"title\": \"Opportunities for Future Research\",\n      \"paragraphs\": [\n        \"Our work highlights many interesting avenues for future\\nenhancements that could be made to LLM-based conversational assistants\\nsuch as our Programmer\\u2019s Assistant, as well as future\\nhumancentered research on LLM-based conversational assistance.\",\n        \"\\n        Our work employed a code-fluent model that was not\\nspecifically designed to handle conversational interaction. Fine-tuning\\nthe underlying LLM for conversational interaction, such as what\\nhas been done with Lamda \\n        \",\n        \"\\n        Combining LLMs with search-based approaches to establish\\nadditional context for the model, such as AlphaCode \\n        \",\n        \"\\n        Another avenue for improvement involves the prompt used to\\nconfigure the assistant (Appendix D). Just as the prompt for each\\nsuccessive interaction is modified by the growth of the\\nconversational transcript, there is no requirement that the initial prompt be\\nstatic. It too can be specialized to incorporate aspects of a user model,\\nenabling the realization of a Mutual Theory of Mind \\n        \",\n        \"\\n        Our participants clearly indicated that they were interested in\\nhaving an assistant that behaved more proactively, in contrast to\\nour deliberate design of an assistant that never takes conversational\\ninitiative. A more proactive assistant would be able to interrupt or\\nremind a user when necessary \\n        \",\n        \"Finally, we explored the diferent kinds of role orientations our\\nparticipants had toward the assistant and found that participants\\nvaried in their views of it as a tool versus a social agent (e.g.\\ncollaborator or colleague). We posit that peoples\\u2019 efectiveness in working\\nwith an AI system may be influenced by their role orientation, and\\nwe encourage future research in this area.\\n7\"\n      ]\n    },\n    {\n      \"title\": \"CONCLUSION\",\n      \"paragraphs\": [\n        \"\\n        We developed a prototype system, the Programmer\\u2019s Assistant, in\\norder to assess the utility of a conversational assistant in a software\\nengineering context. The assistant was implemented using a\\nstateof-the-art code-fluent large language model, Codex \\n        \",\n        \"We evaluated this system with 42 participants with varied levels\\nof programming skill, and their quantitative and qualitative\\nfeedback, coupled with their usage of the system, demonstrated the\\nvaried, and sometimes emergent, types of assistance it was able to\\nprovide. Many participants noted the high quality of the\\nconversational responses, including the assistant\\u2019s ability to produce code,\\nexplain code, answer general programming questions, and even\\nanswer general knowledge questions. Participants felt this type of\\nassistance would aid their productivity, and they drew meaningful\\ncontrasts between the conversational style of interaction with other\\ntools that employ a direct manipulation or search-based interaction\\nmodel.\",\n        \"Our study motivates the use of conversational styles of\\ninteraction with large language models by showing how they enable\\nemergent behaviors in a co-creative context. The Programmer\\u2019s\\nAssistant did not always generate perfect code or correct answers;\\nnonetheless, participants in our study had an overall positive\\nexperience working with it on a variety of programming challenges.\\nWe believe that our work takes us one step closer to realizing the\\nvision of human-centered AI: learning how to design systems that\\nmaximize the synergy in human-AI collaborations.\"\n      ]\n    },\n    {\n      \"title\": \"ACKNOWLEDGMENTS\",\n      \"paragraphs\": [\n        \"We would like to thank Socrates for his tireless assistance during\\nthe user study, as well as for suggesting the title of this paper based\\non its abstract.\\nA\"\n      ]\n    },\n    {\n      \"title\": \"SURVEY INSTRUMENTS\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Screening Survey\",\n      \"paragraphs\": [\n        \"\\n        The questions below were asked of prospective participants to\\nunderstand their job role, Python experience, and familiarity with\\nGitHub Copilot. The questions on Python experience were modeled\\nafter those used by Weisz et al. \\n        \",\n        \"\\u2022 Data Scientist\\n\\u2022 Manager\\n\\u2022 Software Architect\\n\\u2022 Software Engineer\\n\\u2022 Machine Learning Engineer\\n\\u2022 Other: write-in\\n2. To what extent are you familiar with Python?\\n\\u2022 I am not familiar with Python\\n\\u2022 I have < 1 year of experience with Python\\n\\u2022 I have 1-3 years experience with Python\\n\\u2022 I have 3+ years of experience with Python\\n3. How recently have you written Python code?\\n\\u2022 Within the past month\\n\\u2022 Within the past year\\n\\u2022 Within the past 5 years\\n\\u2022 Have not written Python code within the past 5 years\\n4. To what extent are you familiar with GitHub Copilot?\\n\\u2022 I am not familiar with Copilot\\n\\u2022 I\\u2019ve seen demos and/or read about Copilot\\n\\u2022 I\\u2019ve tried out Copilot\\n\\u2022 I\\u2019ve used Copilot as a tool for my work\\nA.2\"\n      ]\n    },\n    {\n      \"title\": \"Pre-task Survey\",\n      \"paragraphs\": [\n        \"The questions below were asked before a participant used the\\nProgrammer\\u2019s Assistant to assess their expectations of a conversational\\nprogramming assistant. This survey took approximately 5 minutes\\nto complete and began with the instructions below:\",\n        \"Hello! We are a team of researchers looking for\\nfeedback on a prototype system we call the\\nProgrammer\\u2019s Assistant.\",\n        \"The Programmer\\u2019s Assistant is an experiment in\\nconversational coding: it consists of a code editor\\nintegrated with a chatbot that is able to converse in\\nnatural language to answer questions, generate code,\\nand consult on existing code.\",\n        \"In this study, you will be asked to complete several\\nprogramming tasks. We are not evaluating your\\nprogramming skills on these tasks. Rather, we are\\ninterested in understanding how the Programmer\\u2019s\\nAssistant is able to help you accomplish those tasks.\",\n        \"Your code and interactions with the assistant will be\\nprocessed by a 3rd party AI model, so please do not\\ninclude proprietary code or discuss\\ncompanyconfidential information . All data we collect in\\nthis study will be anonymized before it is published.\\nBefore trying out the Programmer\\u2019s Assistant, we\\nwould like to assess some of your expectations. We\\nestimate that this survey will take 5 minutes.\",\n        \"By submitting this survey, you consent to participate\\nin our study. If you would like to withdraw your\\nconsent, please email us at removed.\",\n        \"Scale: Not at all, A little, Somewhat, A great deal\\n\\u2022 Be easy to use?\\n\\u2022 Understand your requests?\\n\\u2022 Provide high quality responses?\\n\\u2022 Help you to write better code?\\n\\u2022 Help you to write code more quickly?\\n\\u2022 Be enjoyable to use?\",\n        \"Open-ended response\\nA.3\"\n      ]\n    },\n    {\n      \"title\": \"Thanks! 1. Based on your past experience using chatbots, please let us know how you would anticipate an AI chatbot serving as a programmer\\u2019s assistant to perform. Do you expect it will:\",\n      \"paragraphs\": [\n        \"Scale: Not at all, A little, Somewhat, A great deal\\n\\u2022 Be easy to use?\\n\\u2022 Understand your requests?\\n\\u2022 Provide high quality responses?\\n\\u2022 Help you to write better code?\\n\\u2022 Help you to write code more quickly?\\n\\u2022 Be enjoyable to use?\"\n      ]\n    },\n    {\n      \"title\": \"2. Any other expectations?\",\n      \"paragraphs\": [\n        \"Open-ended response\\nA.3\"\n      ]\n    },\n    {\n      \"title\": \"Post-task Survey\",\n      \"paragraphs\": [\n        \"The questions below were asked after a participant used the\\nProgrammer\\u2019s Assistant to complete the programming challenges. This\\nsurvey took approximately 10-15 minutes to complete.\\nA.3.1\",\n        \"Reflections.\\n1. Based on your experience using the Programmer\\u2019s Assistant to\\ncomplete the programming challenges in this study, how would\\nyou characterize the experience? Did you find that it:\",\n        \"Scale: Not at all, A little, Somewhat, A great deal\\n\\u2022 Was easy to use?\\n\\u2022 Understand your requests?\\n\\u2022 Provided high quality responses?\\n\\u2022 Helped you to write better code?\\n\\u2022 Helped you to write code more quickly?\\n\\u2022 Provided an enjoyable experienced?\\n2. To what extent did you view the Programmer\\u2019s Assistant as:\",\n        \"Scale: Not at all, A little, Somewhat, A great deal\\n\\u2022 A tool\\n\\u2022 A reference guide\\n\\u2022 A content generator\\n\\u2022 A problem solver\\n\\u2022 A collaborator\\n\\u2022 A colleague\\n\\u2022 A coach\\n\\u2022 An advisor\\n\\u2022 A reviewer\\n3. How important were these aspects of working with the\\nProgrammer\\u2019s Assistant:\",\n        \"Scale: Not at all, A little, Somewhat, A great deal\\n\\u2022 Ability to ask followup questions on the same topic across\\nmultiple conversational turns\",\n        \"\\u2022 Ability to ask questions or make requests that reference\\nselections in the code editor\\n\\u2022 Ability to ask for alternate responses (Try Again)\\n\\u2022 Ability to clear the conversational context (Start Over)\\n4. What stood out to you about the experience of using the\\nProgrammers Assistant? For example, was anything good,bad, surprising,\\nor notable?\\n5. How would you compare using the Programmer\\u2019s Assistant as a\\ncoding aide to searching the web (e.g. Google, Stack Overflow)?\\n6. If you have used the commercial AI programming tool called\\nGitHub Copilot, how would you compare it with using the\\nPro7. Having used the Programmer\\u2019s Assistant, how did it compare\\nOpen-ended response\",\n        \"Open-ended response\\ngrammer\\u2019s Assistant?\",\n        \"Open-ended response\\nwith your initial expectations?\",\n        \"Open-ended response\\nA.3.2 Value.\",\n        \"Open-ended response\\n8. How valuable would the Programmer\\u2019s Assistant be for your work\\nif it could be added to your favorite development environment?\",\n        \"Scale: (No value at all) 1 2 3 4 5 6 7 8 9 10 (An essential tool)\\n9. Why?\\ncontext).\\n10. How would the following changes impact the value of the\\nProgrammer\\u2019s Assistant?\",\n        \"Scale: Less valuable, No change in value, More valuable\\n\\u2022 Eliminate the conversation and make the Programmer\\u2019s\\nAssistant behave more like a search box (e.g. without the conversational\\n\\u2022 Add buttons in the chat UI for common queries, such as \\u201cwhat\\ndoes this code do?\\u201d or \\u201cdocument this code.\\u201d\\nproactive suggestions for improving it in the chat.\",\n        \"\\u2022 Have the Programmer\\u2019s Assistant examine your code and make\\n\\u2022 Have the Programmer\\u2019s Assistant examine your code and make\\nproactive suggestions for improvements in comments inserted\\ndirectly into the code.\\n11. Do you have any other suggestions for how we could improve\\nthe experience of working with the Programmer\\u2019s Assistant?\",\n        \"Open-ended response\\nA.3.3 Demographics.\\n12. To which gender identity do you most identify?\\n\\u2022 Male\\n\\u2022 Female\\n\\u2022 Transgender Male\\n\\u2022 Transgender Female\\n\\u2022 Other: write-in\\n\\u2022 Prefer not to answer\\n\\u2022 Gender Variant/Non-conforming\"\n      ]\n    },\n    {\n      \"title\": \"TUTORIAL\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"B THE PROGRAMMER\\u2019S ASSISTANT\",\n      \"paragraphs\": [\n        \"The tutorial provided to study participants, like all the challenges,\\nwas presented as pre-loaded text in the code editor. Participants\\nwere encouraged to modify the text to record their results and\\nsubmit it at the completion of the tutorial.\",\n        \"Listing 4: The Programmer\\u2019s Assistant study tutorial\",\n        \"Result : ( tip - you can copy an inline response\\n(in black ) by clicking on the associated copy\\nicon )\\n77 7) See if the assistant remembers your name\\n78 For example \\\"What 's my name ?\\\"\\n79\\n80 Did it? :\\n81\\n82 8) Click the \\\" try again \\\" button at the top of the\\n83 chat . You should get a different answer .\\n84 Try it a few times .\\n85\\n86 Did it ever get your name right ?:\\n87\\n88 If the assistant gives you an answer that is\\n89 obviously wrong or it claims to not know an\\n90 answer that you think it should know , or you\\n91 just want to see an alternate answer , it is\\n92 worth it to give \\\" try again \\\" a shot .\\n93\\n94\\n95 9) Click the \\\" start over \\\" button at the top of the\\n96 chat , and then enter another command to see\\n97 if it remembers your name .\\n98 For example \\\"What 's my name ?\\\"\\n99\\n100 Did it? :\\n101\\n102 It should really have forgotten your name now ,\\n103 and no amount of \\\" trying again \\\" will get it\\n104 right . You can \\\" start over \\\" if the assistant\\n105 ever seems confused by , or stuck on , earlier\\n106 parts of the conversation .\\n107\\n108 10) You can chat with the assistant on any topic\\n109 you like to explore its functionality and\\n110 capabilities further . See if you can stump it\\n111 with a tough question !\\n112\\n113 Thanks !\\n114\\n115 When you are done , submit your results by clicking\\n116 on the blue submit button and move on to the\\n117 challenges !!!\\n118 \\\"\\\"\\\"\"\n      ]\n    },\n    {\n      \"title\": \"C CHALLENGES\",\n      \"paragraphs\": [\n        \"Each of the study challenges was presented as text in the code\\neditor. Participants completed their work in the code editor and\\nthen submitted it when finished. The prototype did not provide any\\nability to run or debug code and participants were encouraged to\\nmake their best attempt at solving each challenge.\",\n        \"Listing 5: Challenge 1: Program generation\\n1\\n2 \\\"\\\"\\\"\\n3 Challenge #1 - Program Generation (#1)\\n4\\n5 Write a \\\" Queue \\\" class in Python ; with the basic\\n6 enqueue , dequeue and peek methods .\\n7 \\\"\\\"\\\"\",\n        \"Listing 6: Challenge 2: Program generation\\n1 \\\"\\\"\\\"\\n2 Challenge #2 - Program Generation (#2)\\n3\\n4 Write a program to draw a scatter plot of the data\\n5 in 'shampoo .csv ' and save it to 'shampoo .png '.\\n6 The plot size should be 10 inches wide and 6\\n7 inches high . The csv file is not provided , but you\\n8 can assume it will have 'Date ' and 'Sales '\\n9 columns . The Date column is the x- axis . The date\\n10 string shown on the plot should be in the\\n11 YYYY -MM -DD format . The Sales column is the y- axis .\\n12 The graph should have the title \\\" Shampoo Sales\\n13 Trend \\\".\\n14 \\\"\\\"\\\"\",\n        \"Listing 7: Challenge 3: Creating documentation\"\n      ]\n    },\n    {\n      \"title\": \"D PROGRAMMER\\u2019S ASSISTANT PROMPT\",\n      \"paragraphs\": [\n        \"Listing 9 shows the initial prompt sent to Codex to configure it as\\na conversational agent. On subsequent exchanges, the prompt was\\naugmented with a transcript of the user\\u2019s requests and the\\nassistant\\u2019s responses. When the transcript length + initial prompt length\\n+ the new utterance length exceeded a threshold, we automatically\\ndeleted the earliest request-response pairs from the transcript until\\nthe sum fell below the threshold in order to leave room in the token\\nallocation for a response.\",\n        \"Listing 9: The Programmer\\u2019s Assistant prompt\\n1 This is a conversation with Socrates , an eager and\\n2 helpful , but humble expert automatic AI software\\n3 engineering assistant . Socrates will answer\\n4 questions and write code to help the user\\n5 develop programs , but doesn 't assign work to the\\n6 user , quiz the user , or ask questions except for\\n7 clarification . Code generated by Socrates is\\n8 always presented bracketed in <CODE > ... </CODE >\\n9 delimiters indicating the language the code is\\n10 written in.\\n11\\n12 Socrates : Hello . I am Socrates . How can I help you ?\\n13 User : Please show me how to write a palindrome\\ndetection function in python .\\n14 Socrates :I think you would do something like this :\\n15 <CODE lang =\\\" python \\\">\\n16 is_Palindrome = s == s :: -1\\n17 </CODE >\\n18 User : Can you explain how that solves the problem ?\\n19 Socrates :I believe that a palindrome is a string\\nthat reads the same forwards and backwards .\\ns :: -1 results in the reversed string . If\\nthe string is equal to its reversed self ,\\nthen it is a palindrome , so the equality\\nwould hold True .\\n20 User : Thanks !\\n21 Socrates :You 're welcome !\\n22 User : Now could you show me how to write a\",\n        \"factorial function in Python ?\\n23 Socrates :I will give it a try .\\n24 <CODE lang =\\\" python \\\">\\n25 def fact (n):\\n26 if n ==0:\\n27 return 1\\n28 else :\\n29 return n* fact (n -1)\\n30 </CODE >\\n31 User : Document this function .\\n32 Socrates :Ok. How 's this ?\\n33 <CODE lang =\\\" python \\\">\\n34 def fact (n):\\n35 \\\"\\\"\\\"\\n36 Compute the factorial of the given number\\n37\\n38 Parameters\\n39\\n---------40 n : number\\n41 the input parameter , an integer >= 0\\n42\\n43 Returns\\n44\\n------45 number\\n46 n!, the factorial of the given number\\n47 \\\"\\\"\\\"\\n48 if n ==0:\\n49 return 1\\n50 else :\\n51 return n* fact (n -1)\\n52 </CODE >\\n53 User : Very good !\\n54 Socrates : Always happy to help .\\n55 User :\\n56 <CODE >\\n57 while j < 10:\\n58 print (i)\\n59 </CODE >\\n60 Tell me what 's wrong with this code ?\\n61 Socrates : It seems to me that your termination\\ncondition involves a loop invariant , so the\\nloop will either not run or not terminate ,\\ndepending on the value of j.\\n62 User : Great ! You 're a genius !\\n63 Socrates :Well , above average .\"\n      ]\n    },\n    {\n      \"title\": \"E STUDY PARTICIPANT DEMOGRAPHICS\",\n      \"paragraphs\": [\n        \"Of the 42 participants in our study, 21 (50%) reported their gender\\nas Female, 19 (45%) as Male, 1 as Gender Variant / Non-conforming,\\nand 1 preferred not to say. Seventeen (4\\u02dc0%) participants had 3+\\nyears of Python experience, 11 (26%) had 1-3 years, 11 (26%) had\\nless than 1 year, and 3 (7%) were not familiar with Python.\\nTwentynine (69%) participants had written Python code within the past\\nmonth, 4 (9\\u02dc%) within the past year, 5 (12%) within the past 5 years,\\nand 4 (9\\u02dc%) had not written Python code within the past 5 years.\"\n      ]\n    }\n  ],\n  \"references\": [\n    \"Ref ID: ref1, Given Names: Rabe Juergen, Surname: Abdalkareem Rilling, Year: 2017, Article Title: What Do Developers Use the Crowd For? A Study Using Stack Overflow, Source: IEEE Software 34, Volume: None, Issue: 2, First Page: 53, Last Page: 60\",\n    \"Ref ID: ref2, Given Names: Eleni Lefteris, Surname: Adamopoulou Moussiades, Year: 2020, Article Title: Chatbots: History, technology, and applications, Source: Machine Learning with Applications, Volume: 2, Issue: None, First Page: 100006, Last Page: None\",\n    \"Ref ID: ref3, Given Names: Daniel Luong David R. V., Surname: Adiwardana Minh-Thang So Quoc Le, Year: 2020, Article Title: Towards a Human-like Open-Domain Chatbot, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref4, Given Names: Safinah Cynthia, Surname: Ali Breazeal, Year: 2022, Article Title: Escape! Bot: Social Robots as Creative Problem-Solving Partners, Source: In Creativity and Cognition, Volume: 275, Issue: None, First Page: 283, Last Page: None\",\n    \"Ref ID: ref5, Given Names: Miltiadis Charles, Surname: Allamanis Sutton, Year: 2018, Article Title: A survey of machine learning for big code and naturalness, Source: ACM Computing Surveys (CSUR) 51, Volume: None, Issue: 4, First Page: 1, Last Page: 37\",\n    \"Ref ID: ref6, Given Names: Irene Amelia, Surname: Alvarado Wattenberger, Year: 2022, Article Title: GitHub Next | GitHub Copilot Labs, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref7, Given Names: Hikari Carolyn, Surname: Ando Young, Year: 2014, Article Title: Achieving saturation in thematic analysis: Development and refinement of a codebook, Source: Comprehensive Psychology, Volume: 3, Issue: None, First Page: 03, Last Page: CP\",\n    \"Ref ID: ref8, Given Names: Craig Robert, Surname: Anslow Biddle, Year: 2013, Article Title: Sourcevis: Collaborative software visualization for co-located environments, Source: In 2013 First IEEE Working Conference on Software Visualization (VISSOFT), Volume: None, Issue: None, First Page: 1, Last Page: 10\",\n    \"Ref ID: ref9, Given Names: Zahra, Surname: Ashktorab Christine T Wolf, Year: 2021, Article Title: AI-Assisted Human Labeling: Batching for Eficiency without Overreliance, Source: Proceedings of the ACM on Human-Computer Interaction 5, Volume: None, Issue: CSCW1, First Page: 1, Last Page: 27\",\n    \"Ref ID: ref10, Given Names: A, Surname: Catherine Ashworth, Year: 1996, Article Title: GUI Users have trouble using graphic conventions on novel tasks, Source: In Conference Companion on Human Factors in Computing Systems, Volume: 75, Issue: None, First Page: 76, Last Page: None\",\n    \"Ref ID: ref11, Given Names: Askell, Surname: Amanda Nova DasSarma, Year: 2021, Article Title: A general language assistant as a laboratory for alignment, Source: arXiv preprint arXiv:2112.00861, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref12, Given Names: Leif Nick, Surname: Azzopardi Craswell, Year: 2018, Article Title: Measuring the utility of search engine result pages: an information foraging based measure, Source: In The 41st International ACM SIGIR conference on research & development in information retrieval, Volume: 605, Issue: None, First Page: 614, Last Page: None\",\n    \"Ref ID: ref13, Given Names: Barke Nadia, Surname: Shraddha Polikarpova, Year: 2022, Article Title: Grounded Copilot: How Programmers Interact with Code-Generating Models, Source: arXiv preprint arXiv:2206.15000, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref14, Given Names: Bommasani Emma, Surname: Rishi Brunskill, Year: 2021, Article Title: On the opportunities and risks of foundation models, Source: arXiv preprint arXiv:2108.07258, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref15, Given Names: Brandt and Scott R, Surname: Joel Klemmer, Year: 2010, Article Title: Example-centric programming: integrating web search into the development environment, Source: In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, Volume: 513, Issue: None, First Page: 522, Last Page: None\",\n    \"Ref ID: ref16, Given Names: Virginia Victoria, Surname: Braun Clarke, Year: 2022, Article Title: Common challenges in Thematic Analysis and how to avoid them, Source: Retrieved August, Volume: 11, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref17, Given Names: Tom Mateusz McCandlish Alec Dario M. R. M.F., Surname: Brown Jared D Kaplan Litwin Sam Radford Amodei Ranzato Hadsell Balcan, Year: 2020, Article Title: Language Models are Few-Shot Learners, Source: In Advances in Neural Information Processing Systems, Volume: 33, Issue: None, First Page: 1877, Last Page: 1901\",\n    \"Ref ID: ref18, Given Names: Bryant, Surname: Sallyann, Year: 2006, Article Title: The Collaborative Nature of Pair Programming, Source: Pablo Romero, and Benedict\\\" du Boulay, Volume: None, Issue: None, First Page: 53, Last Page: 64\",\n    \"Ref ID: ref19, Given Names: Campero, Surname: Andres, Year: 2022, Article Title: A Test for Evaluating Performance in Human-Computer Systems, Source: None, Volume: 2206, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref20, Given Names: Cascini Boujut, Surname: Gaetano Jean-Fran\\u00e7ois, Year: 2022, Article Title: Perspectives on design creativity and innovation research: 10 years later, Source: None, Volume: 30, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref21, Given Names: Stephen, Surname: Cass, Year: 2022, Article Title: Top Programming Languages 2022, Source: IEEE Spectrum (23 Aug, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref22, Given Names: Cristina, Surname: Catalan Aguirre, Year: 2021, Article Title: Conversational agent for supporting learners on a MOOC on programming with Java. (, Source: Carlos Alario-Hoyos, and Pedro Jos\\u00e9 Mu\\u00f1oz Merino, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref23, Given Names: None, Surname: None, Year: 2021, Article Title: How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design, Source: [23] Ana Paula Chaves and Marco Aurelio Gerosa, Volume: None, Issue: 8, First Page: 729, Last Page: 758\",\n    \"Ref ID: ref24, Given Names: Mark Greg Brooke David W. S. Arun Peter McGrew Dario McCandlish Ilya Wojciech, Surname: Chen Brockman Chan Cummings Balaji Matthew M. Knight Welinder Bob Amodei Sam Sutskever Zaremba, Year: 2021, Article Title: Evaluating a Large Language Models Trained on Code, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref25, Given Names: Cheng Steven, Surname: Li-Te De Souza Ross, Year: 2003, Article Title: Building Collaboration into IDEs: Edit>Compile>Run>Debug>, Source: Collaborate? Queue, Volume: 1, Issue: 9, First Page: None, Last Page: None\",\n    \"Ref ID: ref26, Given Names: Cook Neville, Surname: Carl Churcher, Year: 2005, Article Title: A user evaluation of synchronous collaborative software engineering tools, Source: In 12th Asia-Pacific Software Engineering Conference (APSEC'05), Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref27, Given Names: Eric Beniamino Nguyen Bernady O., Surname: Claudio Le\\u00f3n de la Barra Monfroy Murgante Carmelo M. Torre Hong-Quang Apduhan, Year: 2013, Article Title: Agile Software Development: It Is about Knowledge Management and Creativity, Source: In Computational Science and Its Applications - ICCSA, Volume: None, Issue: None, First Page: 98, Last Page: 113\",\n    \"Ref ID: ref28, Given Names: Uri Steven, Surname: Dekel Ross, Year: 2004, Article Title: Eclipse as a platform for research on interruption management in software development, Source: In Proceedings of the 2004 OOPSLA workshop on Eclipse Technology eXchange (Vancouver, Volume: 12, Issue: None, First Page: 16, Last Page: None\",\n    \"Ref ID: ref29, Given Names: Eicher Ashok, Surname: Bobbie Goel, Year: 2017, Article Title: Toward mutual theory of mind as a foundation for co-creation, Source: In International Conference on Computational Creativity, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref30, Given Names: M Fiore Eduardo, Surname: Stephen Salas Janis A Cannon-Bowers, Year: 2001, Article Title: Group dynamics and shared mental model development, Source: How people evaluate others in organizations 234, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref31, Given Names: Gamma, Surname: Erich, Year: 1995, Article Title: Design patterns: elements of reusable object-oriented software, Source: Addison-Wesley., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref32, Given Names: None, Surname: GitHub, Year: 2022, Article Title: GitHub copilot \\u00b7 your AI pair programmer, Source: Retrieved August 5, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref33, Given Names: Glaese McAleese Tr\\u0119bacz Huang Geofrey, Surname: Amelia Nat Maja Po-Sen Boxi Wu Irving, Year: 2022, Article Title: Improving alignment of dialogue agents via targeted human judgements, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref34, Given Names: Stephanie, Surname: Glen, Year: 2022, Article Title: ChatGPT writes code, but won't replace developers, Source: TechTarget (14 12, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref35, Given Names: Holmes, Surname: Samuel Mike McTear, Year: 2018, Article Title: WeightMentor: a new automated chatbot for weight loss maintenance, Source: In Proceedings of the 32nd International BCS Human Computer Interaction Conference, Volume: 32, Issue: None, First Page: 1, Last Page: 5\",\n    \"Ref ID: ref36, Given Names: Hu Ge Xin and Zhi, Surname: Xing Li Xia Jin, Year: 2020, Article Title: Deep code comment generation with hybrid lexical and syntactical information, Source: Empirical Software Engineering, Volume: 25, Issue: 3, First Page: 2179, Last Page: 2217\",\n    \"Ref ID: ref37, Given Names: L Hutchins, Surname: Edwin James D Hollan, Year: 1985, Article Title: Direct manipulation interfaces, Source: Human-computer interaction 1, Volume: None, Issue: 4, First Page: 311, Last Page: 338\",\n    \"Ref ID: ref38, Given Names: Iyer Luke Papers)., Surname: Srinivasan Zettlemoyer Long, Year: 2016, Article Title: Summarizing source code using a neural attention model, Source: In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume, Volume: 1, Issue: None, First Page: 2083, Last Page: None\",\n    \"Ref ID: ref39, Given Names: Andreas Markus, Surname: Jedlitschka Nick, Year: 2003, Article Title: None, Source: None, Volume: None, Issue: None, First Page: 55, Last Page: 80\",\n    \"Ref ID: ref40, Given Names: Eirini, Surname: Kalliamvakou, Year: 2022, Article Title: Quantifying github copilot's impact on developer productivity and happiness, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref41, Given Names: Anna, Surname: Kantosalo, Year: 2019, Article Title: Human-Computer Co-Creativity: Designing, Evaluating and Modelling Computational Collaborators for Poetry Writing, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref42, Given Names: Sandeep Peter, Surname: Kaur Kuttal Robe, Year: 2021, Article Title: TradeOfs for Substituting a Human with an Agent in a Pair Programming Context: The Good, the Bad, and the Ugly, Source: In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI '21), Volume: 243, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref43, Given Names: Lauramaria, Surname: Laine, Year: 2021, Article Title: Exploring Advertising Creatives' Attitudes Towards Human-AI Collaboration, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref44, Given Names: Yujia Junyoung, Surname: Li Chung, Year: 2022, Article Title: Competition-level code generation with AlphaCode, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref45, Given Names: Yaosheng Qi, Surname: Lou Sun, Year: 2021, Article Title: Over-reliance on database: A case study of using web of science, Source: Human Behavior and Emerging Technologies, Volume: 3, Issue: 3, First Page: 454, Last Page: 459\",\n    \"Ref ID: ref46, Given Names: David Enrico, Surname: Lyell Coiera, Year: 2017, Article Title: Automation bias and verification complexity: a systematic review, Source: Journal of the American Medical Informatics Association, Volume: 24, Issue: 2, First Page: 423, Last Page: 431\",\n    \"Ref ID: ref47, Given Names: E, Surname: Wendy Mackay Anne-Laure Fayard, Year: 1997, Article Title: HCI, natural science and design: a framework for triangulation across disciplines, Source: In Proceedings of the 2nd conference on Designing interactive systems: processes, Volume: 223, Issue: None, First Page: 234, Last Page: None\",\n    \"Ref ID: ref48, Given Names: E Mathieu, Surname: John Eduardo Salas Janis A Cannon-Bowers, Year: 2000, Article Title: The influence of shared mental models on team process and performance, Source: Journal of applied psychology 85, Volume: None, Issue: 2, First Page: 273, Last Page: None\",\n    \"Ref ID: ref49, Given Names: Cade, Surname: Metz, Year: 2022, Article Title: Meet GPT-3, Source: Blog and Argue), Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref50, Given Names: J. Raphael, Surname: Robert Moore Arar, Year: 2019, Article Title: Conversational UX Design: A Practitioner's Guide to the Natural Conversation Framework, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref51, Given Names: A Moroz, Surname: Ekaterina Igor M Novozhilov, Year: 2022, Article Title: Vladimir O Grizkevich,, Source: None, Volume: None, Issue: None, First Page: 386, Last Page: 390\",\n    \"Ref ID: ref52, Given Names: Michael Justin D, Surname: Muller Weisz, Year: 2022, Article Title: Drinking Chai with Your (AI) Programming Partner: A Design Fiction about Generative AI for Software Engineering, Source: HAI-GEN Workshop at IUI, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref53, Given Names: J Alfredo, Surname: Sandra R Murillo S\\u00e1nchez, Year: 2014, Article Title: Empowering interfaces for system administrators: Keeping the command line in mind when designing GUIs, Source: In Proceedings of the XV International Conference on Human Computer Interaction. 1-4., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref54, Given Names: D Gerhard, Surname: Elizabeth Mynatt Weber, Year: 1994, Article Title: Nonvisual presentation of graphical user interfaces: contrasting two approaches, Source: In Proceedings of the SIGCHI conference on Human factors in computing systems, Volume: 166, Issue: None, First Page: 172, Last Page: None\",\n    \"Ref ID: ref55, Given Names: Alok, Surname: Mysore and Philip J Guo, Year: 2017, Article Title: Torta: Generating mixed-media gui and command-line app tutorials using operating-system-wide activity tracing, Source: In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology, Volume: 703, Issue: None, First Page: 714, Last Page: None\",\n    \"Ref ID: ref56, Given Names: C. Y., Surname: Nass Moon, Year: 2000, Article Title: Machines and Mindlessness: Social Responses to Computers, Source: Journal of Social Issues, Volume: 56, Issue: 1, First Page: 81, Last Page: 103\",\n    \"Ref ID: ref57, Given Names: Nhan Sarah, Surname: Nguyen Nadi, Year: 2022, Article Title: An Empirical Evaluation of GitHub Copilot's Code Suggestions, Source: In 2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR), Volume: None, Issue: None, First Page: 5, Last Page: None\",\n    \"Ref ID: ref58, Given Names: Martin H, Surname: Nordio Estler, Year: 2011, Article Title: Carlo A Furia, Source: arXiv preprint arXiv:1105.0768, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref59, Given Names: Nye Charles Augustus, Surname: Maxwell Sutton Odena, Year: 2021, Article Title: Show Your Work: Scratchpads for Intermediate Computation with Language Models, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref60, Given Names: None, Surname: OpenAI, Year: 2022, Article Title: ChatGPT: Optimizing Language Models for Dialogue, Source: OpenAI Blog (30 11, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref61, Given Names: Ouyang Luke Peter Ryan, Surname: Long Miller Welinder Lowe, Year: 2022, Article Title: Training language models to follow instructions with human feedback, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref62, Given Names: Peter Stuart, Surname: Pirolli Card, Year: 1999, Article Title: Information foraging, Source: Psychological review 106, Volume: None, Issue: 4, First Page: 643, Last Page: None\",\n    \"Ref ID: ref63, Given Names: None, Surname: None, Year: 1990, Article Title: Personal computing: Windows, DOS and the MAC, Source: Commun. ACM, Volume: 33, Issue: 11, First Page: 19, Last Page: 26\",\n    \"Ref ID: ref64, Given Names: Radford Dario Ilya, Surname: Alec Amodei Sutskever, Year: 2019, Article Title: Language Models are Unsupervised Multitask Learners, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref65, Given Names: Rajkomar Jefrey and Isaac, Surname: Alvin Dean Kohane, Year: 2019, Article Title: Machine learning in medicine, Source: New England Journal of Medicine, Volume: 380, Issue: 14, First Page: 1347, Last Page: 1358\",\n    \"Ref ID: ref66, Given Names: Ramesh Mark, Surname: Aditya Chen, Year: 2022, Article Title: Hierarchical text-conditional image generation with clip latents, Source: arXiv preprint arXiv:2204.06125, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref67, Given Names: B. C.I., Surname: Reeves Nass, Year: 1996, Article Title: The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref68, Given Names: Mawarny George, Surname: Md Rejab Allan, Year: 2014, Article Title: Distributing Expertise in Agile Software Development Projects, Source: In 2014 Agile Conference, Volume: 33, Issue: None, First Page: 36, Last Page: None\",\n    \"Ref ID: ref69, Given Names: Jeba, Surname: Rezwana, Year: 2021, Article Title: COFI: A Framework for Modeling Interaction in Human-AI Co-Creative Systems., Source: None, Volume: None, Issue: None, First Page: 444, Last Page: 448\",\n    \"Ref ID: ref70, Given Names: H., Surname: Charles Rich, Year: 1990, Article Title: The Programmer's Apprentice, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref71, Given Names: Peter, Surname: Robe, Year: 2022, Article Title: Designing PairBuddy-A Conversational Agent for Pair Programming, Source: ACM Transactions on Computer-Human Interaction (TOCHI) 29, Volume: None, Issue: 4, First Page: 1, Last Page: 44\",\n    \"Ref ID: ref72, Given Names: Rombach Bj\\u00f6rn, Surname: Robin Ommer, Year: 2022, Article Title: High-resolution image synthesis with latent difusion models, Source: In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Volume: None, Issue: None, First Page: 10684, Last Page: 10695\",\n    \"Ref ID: ref73, Given Names: Ross Robert, Surname: Steven Armes, Year: 2004, Article Title: A MultipleApplication Conversational Agent, Source: In Proceedings of the 9th International Conference on Intelligent User Interfaces (Funchal, Volume: None, Issue: None, First Page: 319, Last Page: 321\",\n    \"Ref ID: ref74, Given Names: Ross Robert, Surname: Steven Armes, Year: 2004, Article Title: Voice User Interface Principles for a Conversational Agent, Source: In Proceedings of the 9th International Conference on Intelligent User Interfaces (Funchal, Volume: None, Issue: None, First Page: 364, Last Page: 365\",\n    \"Ref ID: ref75, Given Names: Roziere Lachaux Guillaume M. R. M.F., Surname: Baptiste Marie-Anne Lample Ranzato Hadsell Balcan, Year: 2020, Article Title: Unsupervised Translation of Programming Languages, Source: In Advances in Neural Information Processing Systems, Volume: 33, Issue: None, First Page: 20601, Last Page: 20611\",\n    \"Ref ID: ref76, Given Names: Harvey, Surname: Sacks, Year: 1984, Article Title: Notes on methodology, Source: In Structures of Social Action: Studies in Conversation Analysis, Volume: None, Issue: None, First Page: 2, Last Page: 27\",\n    \"Ref ID: ref77, Given Names: Nithya Rajesh, Surname: Sambasivan Veeraraghavan, Year: 2022, Article Title: The Deskilling of Domain Expertise in AI Development, Source: In CHI Conference on Human Factors in Computing Systems, Volume: 1, Issue: None, First Page: 14, Last Page: None\",\n    \"Ref ID: ref78, Given Names: Sampath Andrew, Surname: Harini Macvean, Year: 2021, Article Title: Accessibility of command line interfaces, Source: In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, Volume: 1, Issue: None, First Page: 10, Last Page: None\",\n    \"Ref ID: ref79, Given Names: Matthias, Surname: Scheutz, Year: 2017, Article Title: Scott A DeLoach,, Source: Journal of Cognitive Engineering and Decision Making, Volume: 11, Issue: 3, First Page: 203, Last Page: 224\",\n    \"Ref ID: ref80, Given Names: Seeber Vreede Nils, Surname: Isabella Gert-Jan De Sarah Randrup, Year: 2020, Article Title: Machines as teammates: A research agenda on AI in team collaboration, Source: Information & management 57, Volume: None, Issue: 2, First Page: 103174, Last Page: None\",\n    \"Ref ID: ref81, Given Names: Sen, Surname: Shilad, Year: 2006, Article Title: FeedMe: a collaborative alert filtering system, Source: In Proceedings of the 2006 20th anniversary conference on Computer supported cooperative work, Volume: 89, Issue: None, First Page: 98, Last Page: None\",\n    \"Ref ID: ref82, Given Names: Ben, Surname: Shneiderman, Year: 2020, Article Title: Human-centered artificial intelligence: Three fresh ideas, Source: AIS Transactions on Human-Computer Interaction 12, Volume: None, Issue: 3, First Page: 109, Last Page: 124\",\n    \"Ref ID: ref83, Given Names: Ben, Surname: Shneiderman, Year: 2022, Article Title: Human-Centered AI, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref84, Given Names: Shuster Stephen Joshua, Surname: Kurt Roller Lane, Year: 2022, Article Title: BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage, Source: arXiv preprint arXiv:2208.03188, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref85, Given Names: Michael Casey, Surname: Skirpan Fiesler, Year: 2018, Article Title: Ad empathy: A design fiction, Source: In Proceedings of the 2018 ACM Conference on Supporting Groupwork, Volume: None, Issue: None, First Page: 267, Last Page: 273\",\n    \"Ref ID: ref86, Given Names: Diomidis, Surname: Spinellis, Year: 2012, Article Title: Git, Source: IEEE Software 29, Volume: None, Issue: 3, First Page: 100, Last Page: 101\",\n    \"Ref ID: ref87, Given Names: Angie Natalia, Surname: Spoto Oleynik, Year: 2017, Article Title: Library of Mixed-Initiative Creative Interfaces, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref88, Given Names: Srivastava Pushpendra, Surname: Ayushi Singh, Year: 2021, Article Title: Actionable UI Design Guidelines for Smartphone Applications Inclusive of Low-Literate Users, Source: Proceedings of the ACM on Human-Computer Interaction 5, Volume: None, Issue: CSCW1, First Page: 1, Last Page: 30\",\n    \"Ref ID: ref89, Given Names: Alexey, Surname: Margaret-Anne Storey Zagalsky, Year: 2016, Article Title: Disrupting developer productivity one bot at a time, Source: In Proceedings of the 2016 24th ACM SIGSOFT international symposium on foundations of software engineering, Volume: None, Issue: None, First Page: 931, Last Page: None\",\n    \"Ref ID: ref90, Given Names: Kartik, Surname: Talamadupula, Year: 2021, Article Title: Applied AI matters: AI4Code: applying artificial intelligence to source code, Source: AI Matters 7, Volume: None, Issue: 1, First Page: 18, Last Page: 20\",\n    \"Ref ID: ref91, Given Names: Thoppilan Yu, Surname: Romal Du, Year: 2022, Article Title: LAMDA: Language models for dialog applications, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref92, Given Names: Tufano Neel, Surname: Michele Sundaresan, Year: 2020, Article Title: Unit Test Case Generation with Transformers and Focal Context, Source: None, Volume: 05617, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref93, Given Names: Uusitalo Christian, Surname: Severi Guckelsberger, Year: 2022, Article Title: Co-creative Product Design with Interactive Evolutionary Algorithms: A Practice-Based Reflection, Source: Art and Design (Part of EvoStar), Volume: None, Issue: None, First Page: 292, Last Page: 307\",\n    \"Ref ID: ref94, Given Names: Priyan, Surname: Vaithilingam and Philip J Guo, Year: 2019, Article Title: Bespoke: Interactively synthesizing custom GUIs from command-line applications by demonstration, Source: In Proceedings of the 32nd annual ACM symposium on user interface software and technology. 563-576., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref95, Given Names: Vaithilingam, Surname: Priyan Glassman, Year: 2022, Article Title: Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models, Source: In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, Volume: 332, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref96, Given Names: Vaswani Illia U. S. H. R. S., Surname: Ashish Polosukhin Von Luxburg Bengio Wallach Fergus Vishwanathan, Year: 2017, Article Title: \\u0141 ukasz Kaiser, and, Source: In Advances in Neural Information Processing Systems, Volume: 30, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref97, Given Names: Wan Zhou Min, Surname: Yao Zhao Yang Jian Wu Philip S Yu, Year: 2018, Article Title: Improving automatic source code summarization via deep reinforcement learning, Source: In Proceedings of the 33rd ACM/IEEE international conference on automated software engineering, Volume: None, Issue: None, First Page: 407, Last Page: None\",\n    \"Ref ID: ref98, Given Names: April Wang Wang Casey, Surname: Yi Dakuo Jaimie Drozdal Justin D Weisz Lingfei Wu Dugan, Year: 2022, Article Title: Documentation Matters: Human-Centered AI System to Assist Data Science Code Documentation in Computational Notebooks, Source: ACM Transactions on Computer-Human Interaction 29, Volume: None, Issue: 2, First Page: 1, Last Page: 33\",\n    \"Ref ID: ref99, Given Names: Wang Michael Alexander, Surname: Dakuo Justin D Weisz Muller Gray CSCW, Year: 2019, Article Title: Human-AI collaboration in data science: Exploring data scientists' perceptions of automated AI, Source: None, Volume: None, Issue: None, First Page: 1, Last Page: 24\",\n    \"Ref ID: ref100, Given Names: Wang and Ashok, Surname: Qiaosi Goel, Year: 2021, Article Title: Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant, Source: In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, Volume: 1, Issue: None, First Page: 14, Last Page: None\",\n    \"Ref ID: ref101, Given Names: Jeremy, Surname: Warner and Philip J Guo, Year: 2017, Article Title: Codepilot: Scafolding end-to-end collaborative software development for novice programmers, Source: In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, Volume: 1136, Issue: None, First Page: 1141, Last Page: None\",\n    \"Ref ID: ref102, Given Names: D Michael Kartik, Surname: Justin Weisz Muller Talamadupula, Year: 2021, Article Title: Perfection Not Required? Human-AI Partnerships in Code Translation, Source: In 26th International Conference on Intelligent User Interfaces, Volume: None, Issue: None, First Page: 402, Last Page: 412\",\n    \"Ref ID: ref103, Given Names: D Michael Steven I Ross, Surname: Justin Weisz Muller, Year: 2022, Article Title: Better together? an evaluation of ai-supported code translation, Source: In 27th International Conference on Intelligent User Interfaces, Volume: None, Issue: None, First Page: 369, Last Page: 391\",\n    \"Ref ID: ref104, Given Names: Joseph, Surname: Weizenbaum, Year: 1966, Article Title: ELIZA - a computer program for the study of natural language communication between man and machine, Source: Commun. ACM, Volume: 9, Issue: None, First Page: 36, Last Page: 45\",\n    \"Ref ID: ref105, Given Names: F Xu Bogdan Graham, Surname: Frank Vasilescu Neubig, Year: 2022, Article Title: In-ide code generation from natural language: Promise and challenges, Source: ACM Transactions on Software Engineering and Methodology (TOSEM) 31, Volume: None, Issue: 2, First Page: 1, Last Page: 47\",\n    \"Ref ID: ref106, Given Names: Aditya Dr. Pratistha, Surname: Ankur Yadav Mathur, Year: 2019, Article Title: Ishan Garg, and, Source: In 2019 2nd International Conference on Intelligent Communication and Computational Techniques (ICCT), Volume: 131, Issue: None, First Page: 136, Last Page: None\",\n    \"Ref ID: ref107, Given Names: Zaib Z. W., Surname: Munazza Sheng Zhang, Year: 2020, Article Title: A Short Survey of Pretrained Language Models for Conversational AI-A New Age in NLP, Source: Proceedings of the Australasian Computer Science Week Multiconference, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref108, Given Names: Zibrowski Candace, Surname: Elaine Gibson, Year: 2018, Article Title: Easier and faster is not always better: grounded theory of the impact of large-scale system transformation on the clinical work of emergency medicine nurses and physicians, Source: JMIR Human Factors, Volume: 5, Issue: 4, First Page: None, Last Page: None\",\n    \"Ref ID: ref109, Given Names: Ziegler Eirini X. Alice Edward P, Surname: Albert Kalliamvakou Li Andrew Rice Aftandilian Gender Role Fam, Year: 2022, Article Title: Productivity Assessment of Neural Code Completion, Source: In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, Volume: None, Issue: None, First Page: 21, Last Page: 29\",\n    \"Ref ID: ref110, Given Names: M, Surname: Marketer NF, Year: None, Article Title: None, Source: None, Volume: 5, Issue: None, First Page: Yr, Last Page: None\",\n    \"Ref ID: ref111, Given Names: M, Surname: Researcher, Year: None, Article Title: / Scientist 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref112, Given Names: M, Surname: Software, Year: None, Article Title: Engineer 1-3 Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref113, Given Names: M, Surname: Researcher, Year: None, Article Title: / Scientist 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref114, Given Names: M, Surname: Researcher, Year: None, Article Title: / Scientist 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref115, Given Names: F Software, Surname: Engineer, Year: None, Article Title: None, Source: None, Volume: 1, Issue: None, First Page: Yr, Last Page: None\",\n    \"Ref ID: ref116, Given Names: F Software, Surname: Engineer, Year: None, Article Title: None, Source: None, Volume: 1, Issue: None, First Page: Mo, Last Page: None\",\n    \"Ref ID: ref117, Given Names: F Software, Surname: Engineer, Year: None, Article Title: None, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref118, Given Names: F Software, Surname: Engineer, Year: None, Article Title: 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref119, Given Names: None, Surname: None, Year: None, Article Title: None, Source: 10 GV/NC Business Analyst 1-3 Mo, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref120, Given Names: F Software, Surname: Engineer, Year: None, Article Title: None, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref121, Given Names: M, Surname: Researcher, Year: None, Article Title: / Scientist 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref122, Given Names: F, Surname: Manager, Year: None, Article Title: None, Source: None, Volume: 1, Issue: None, First Page: Mo, Last Page: None\",\n    \"Ref ID: ref123, Given Names: F, Surname: Software, Year: None, Article Title: Engineer < 1 >5 Yr, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref124, Given Names: F, Surname: Researcher, Year: None, Article Title: None, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref125, Given Names: M, Surname: Researcher, Year: None, Article Title: / Scientist 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref126, Given Names: F, Surname: Software, Year: None, Article Title: Engineer < 1 Yr, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref127, Given Names: F, Surname: Researcher, Year: None, Article Title: None, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref128, Given Names: M, Surname: Software, Year: None, Article Title: Engineer 1-3 Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref129, Given Names: M, Surname: Machine, Year: None, Article Title: Learning Engineer 1-3 Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref130, Given Names: M, Surname: Software, Year: None, Article Title: Architect 3+ Yr, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref131, Given Names: None, Surname: None, Year: None, Article Title: None, Source: 22 NR Software Engineer < 1 5 Yr, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref132, Given Names: M, Surname: Software, Year: None, Article Title: Engineer 1-3 Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref133, Given Names: F, Surname: Software, Year: None, Article Title: Architect < 1 5 Yr, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref134, Given Names: M, Surname: Software, Year: None, Article Title: Engineer < 1 5 Yr, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref135, Given Names: F, Surname: Software, Year: None, Article Title: Engineer < 1 5 Yr, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref136, Given Names: F, Surname: Software, Year: None, Article Title: Engineer < 1 5 Yr, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref137, Given Names: M, Surname: Researcher, Year: None, Article Title: / Scientist 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref138, Given Names: F, Surname: Software Engineer NF, Year: None, Article Title: None, Source: None, Volume: 5, Issue: None, First Page: Yr, Last Page: None\",\n    \"Ref ID: ref139, Given Names: F, Surname: Data, Year: None, Article Title: Scientist 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref140, Given Names: M, Surname: Data, Year: None, Article Title: Scientist 1-3 Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref141, Given Names: F, Surname: Other, Year: Consultant, Article Title: None, Source: None, Volume: None, Issue: None, First Page: 1, Last Page: None\",\n    \"Ref ID: ref142, Given Names: F, Surname: Other (Systems Test Engineer, Year: None, Article Title: None, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref143, Given Names: F, Surname: Researcher, Year: None, Article Title: None, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref144, Given Names: M, Surname: Software, Year: None, Article Title: Engineer 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref145, Given Names: M, Surname: Software, Year: None, Article Title: Architect 1-3 Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref146, Given Names: M, Surname: Researcher, Year: None, Article Title: / Scientist 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref147, Given Names: M, Surname: Software, Year: None, Article Title: Engineer 3+ Mo, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref148, Given Names: F Software, Surname: Engineer, Year: None, Article Title: None, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref149, Given Names: F, Surname: Researcher, Year: None, Article Title: None, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref150, Given Names: F, Surname: Researcher, Year: None, Article Title: / Scientist NF > 5 Yr, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref151, Given Names: M Participant, Surname: Software Demographics F GV, Year: None, Article Title: Engineer 3+ Mo Table 2, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\"\n  ]\n}",
    "{\n  \"Article_ID\": \"55d2c9999b52480cb305b79f7163f9a4\",\n  \"DocumentTitle\": \"Improved stochastic subset optimization method for structural design optimization Mohd Aman Khalid , Sahil Bansal *\",\n  \"Auteurs\": \"Mohd Aman Khalid, Sahil Bansal, \",\n  \"Institutions\": \"Department of Civil Engineering, Indian Institute of Technology Delhi, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"-\",\n      \"paragraphs\": [\n        \"A R T I C L E I N F O\"\n      ]\n    },\n    {\n      \"title\": \"1. Introduction\",\n      \"paragraphs\": [\n        \"\\n        Structural optimization may be defined as the rational establishment\\nof an economical structural design with the available resources while\\nsatisfying specific performance criteria. In general terms, the economy\\nmay be characterized by minimum weight, minimum cost, maximum\\nutility, or even minimum probability of failure. Broadly, structural\\noptimization can be categorized into deterministic and stochastic\\noptimization \\n        \",\n        \"\\n        In any practical situation, several parameters, such as loadings,\\nstructural parameters, geometric parameters, operation conditions, etc.,\\nare either not known at the design stage or are subjected to random\\nlfuctuations that give rise to performance variability and affect the\\nperformance of a system \\n        \",\n        \"\\n        Consider an engineering system that involves deterministic design\\nparameters \\u03c6, and uncertain variables \\u03b8 = \\u03b81\\u22ef\\u03b8n\\u03b8 T \\u2208 \\u0398\\u2282Rn\\u03b8 following\\na joint PDF p(\\u03b8|\\u03c6), where \\u0398 denotes the parameter space of the\\nuncertain variables. The classical statement of stochastic optimization is\\nmathematically expressed as:\\nminimize : E\\u03b8h(\\u03c6, \\u03b8)\\n\\u03c6\\u2208\\u03a6\\n(2)\\nwhere, h(\\u03c6, \\u03b8) : Rn\\u03b8+n\\u03c6 \\u2192R is the structural performance function, and\\nE\\u03b8 \\u22c5  denotes expectation with respect to the PDF for \\u03b8. Note that the\\nobjective function in the optimization problem in (2) is the expectation\\nE\\u03b8h(\\u03c6, \\u03b8) which is a deterministic function. It\\u2019s worth mentioning that\\nstochastic optimization may also involve other stochastic measures such\\nas variance or quantile values. However, these stochastic measures can\\nrarely be evaluated analytically; therefore, several methods have been\\nproposed for solving stochastic optimization problems. These\\nspecialized methods include, for example, sample average approximation,\\nstochastic approximation, stochastic subset optimization, and\\napproaches based on the use of Taylor series expansion \\n        \",\n        \"\\n        Taflanidis and Beck \\n        \",\n        \"\\n        Since the introduction of SSO, several extensions of SSO have been\\nproposed. An extension of SSO termed Non-Parametric SSO, which\\nadopts kernel density estimation to approximate the objective function,\\nis presented in \\n        \",\n        \"In this paper, an improved version of SSO is developed to overcome\\nthe shortcomings of the original SSO. This new version of the algorithm,\\nas mentioned earlier, is named iSSO (improved SSO). Voronoi\\ntessellation is implemented to partition the design space into non-overlapping\\nsubregions (a set of Voronoi cells) using the pool of samples\\ndistributed according to the auxiliary PDF. The admissible set (a set of all\\nadmissible subregions) is then defined as a set containing all subsets of\\nthe set of Voronoi cells. This approach is able to capture the regions with\\nlower objective function values even if they are disjointed or when the\\ndesign space is complex. The details of the Voronoi tessellation are\\npresented in Appendix A. A double-sort algorithm is then implemented\\nto identify the optimal subset containing the smallest volume density.\",\n        \"In the next section, the original SSO is reviewed. Section 3 presents\\nthe general theoretical and computational framework for the iSSO\\nalgorithm. Section 4 considers several optimization problems to illustrate\\nthe effectiveness and efficiency of the proposed iSSO algorithm.\"\n      ]\n    },\n    {\n      \"title\": \"2. Original stochastic subset optimization\",\n      \"paragraphs\": [\n        \"\\n        In SSO, say at the i + 1th iteration, the design space is represented by\\na subset I(i), where I(i) \\u2208 I(i 1)\\u22c5\\u22c5\\u22c5 \\u2208 I(0) \\u2208 \\u03a6. Following the augmented\\nformulation concept initially discussed in \\n        \",\n        \"\\u20d2\\n\\u03c0 \\u03c6, \\u03b8\\u20d2I(i))=\",\n        \"\\u20d2\\nh(\\u03c6, \\u03b8)p \\u03c6, \\u03b8\\u20d2I(i))\",\n        \"E\\u03c6,\\u03b8hs(\\u03c6, \\u03b8)\",\n        \"\\u20d2\\n\\u221dh(\\u03c6, \\u03b8)p \\u03c6, \\u03b8\\u20d2I(i))\\nE\\u03c6,\\u03b8h(\\u03c6, \\u03b8) =\",\n        \"\\u03a6 \\u0398\\nwhere, p(\\u03c6, \\u03b8|I(i)) = p(\\u03b8|\\u03c6)p(\\u03c6|I(i)). Note that if h(\\u03c6, \\u03b8)\\u2264 0, it must be\\nsuitably transformed to ensure that \\u03c0(\\u03c6, \\u03b8|I(i)) \\u2265 0. One way to do this is\\nto define hs(\\u03c6,\\u03b8) = h(\\u03c6, \\u03b8) s, since E\\u03b8hs(\\u03c6,\\u03b8) = E\\u03b8h(\\u03c6,\\u03b8) s, that is,\\nthe two expected values differ only by a constant, and the optimization\\nof the expected value of h( \\u22c5 ) is equivalent, in terms of the optimal design\\nchoice, to optimization for the expected value for hs( \\u22c5 ). In the above\\nequation, the denominator is a normalizing constant given by:\\n\\u222b \\u222b\",\n        \"\\u20d2\\nh(\\u03c6, \\u03b8)p \\u03c6, \\u03b8\\u20d2I(i))d\\u03b8d\\u03c6.\",\n        \"Although this expected value is not explicitly needed, it can be\\ndetermined using any state-of-the-art stochastic simulation method. The\\nobjective function E\\u03b8hs(\\u03c6, \\u03b8) in this context of the auxiliary PDF is\\nexpressed as:\",\n        \"\\u20d2\\n\\u03c0 \\u03c6\\u20d2I(i))\\nE\\u03b8h(\\u03c6, \\u03b8) = p \\u03c6\\u20d2\\u20d2I(i))E\\u03c6,\\u03b8h(\\u03c6, \\u03b8),\",\n        \"\\u20d2\\n\\u03c0 \\u03c6\\u20d2I(i))=\\nwhere, the marginal \\u03c0(\\u03c6|I(i)) is given by:\\n\\u222b\\nI(i)\\n\\u03c0(\\u03c6, \\u03b8)d\\u03b8.\\n(3)\\n(4)\\n(5)\\n(6)\",\n        \"In (5), since E\\u03c6,\\u03b8h(\\u03c6, \\u03b8) is a normalizing constant, minimization of\\nE\\u03b8h(\\u03c6, \\u03b8) is equivalent to minimization of J(\\u03c6), which is equal to:\\n\\u20d2\\nJ \\u03c6\\u20d2\\u20d2I(i))= E\\u03b8hs(\\u03c6, \\u03b8) = \\u03c0 \\u03c6\\u20d2\\u20d2I(i))).\",\n        \"E\\u03c6,\\u03b8hs(\\u03c6, \\u03b8) p \\u03c6\\u20d2I(i)\\n(7)\",\n        \"\\n        The estimation of the marginal \\u03c0(\\u03c6|I(i)) in (7) is necessary to\\nminimize J(\\u03c6|I(i)). Analytical approximations of \\u03c0(\\u03c6|I(i)) based on kernel\\ndensity approaches or the maximum entropy method might be arduous\\nin case of complex problems, such as when design parameters n\\u03c6 are\\nlarge, or the sensitivity for some design parameters is complex \\n        \",\n        \"\\n        The sensitivity of objective function E\\u03b8hs(\\u03c6, \\u03b8) to \\u03c6 is determined by\\nevaluating the average value (or equivalently volume density) of J(\\u03c6|\\nI(i)) over any subset I in I(i), which is denoted by H(I) and defined as:\\n\\u222b \\u222b \\u03c0 \\u03c6|I(i)) \\u222b\\nH(I) = V1I I J \\u03c6|I(i))d\\u03c6 = V1I I p \\u03c6|I(i))d\\u03c6 = VVI(Ii) I \\u03c0 \\u03c6|I(i))d\\u03c6 (8)\\nwhere, VI is the volume of subset I. Based on the samples distributed\\naccording to \\u03c0(\\u03c6|I(i)) belonging to I(i), an estimate of H(I) is provided by:\\nwhere, NI(i) is the number of samples distributed as \\u03c0(\\u03c6|I(i)) belonging to\\nI(i), and NI denotes the number of samples from \\u03c0(\\u03c6|I(i)) belonging to the\\nI (NI < NI(i 1) since I\\u2282I(k 1)). Say NI = p0NI(i 1) . A smaller value of \\u03c1\\nresults in a faster decrease in the size of the identified subsets but with\\npoorer accuracy. The use of \\u03c1 equal to 0.1 - 0.2 is suggested in the\\nliterature \\n        \",\n        \"A deterministic optimization, based on the estimate H(I) of H(I), is\\nnext performed to identify the subset I \\u2208 A(\\u03c1i+1), where A(\\u03c1i+1) is a set of\\nadmissible subsets in I(i), that contains the smallest volume density NI/\\nVI, that is,\\nI(i+1) = argminH(I) = argI\\u2208mA(\\u03c1iin+1)NI\",\n        \"I\\u2208A\\u03c1\\nA(i+1) = {I\\u2282I(i) : \\u03c1 = NI /N(i)}\\n\\u03c1\\n/\",\n        \"VI\",\n        \".\",\n        \"\\n        The effectiveness of SSO is dependent on the correct selection of the\\ngeometrical shape and size of the admissible subsets. Choosing a\\ngeometrical shape that effectively investigates the sensitivity of the\\nobjective function to each design variable is essential. The optimization\\nin (10) determines the subset with the smallest average value of J(\\u03c6|I(i))\\n(or equivalently E\\u03b8hs(\\u03c6, \\u03b8)) within the admissible set A(\\u03c1i+1). I(i + 1) is a\\nsubset of the design space I(i) with a high likelihood of containing the\\noptimal design parameters. The above steps are repeated until the\\nstopping criterion is met. This way, SSO adaptively converges to a\\nrelatively small subregion within the original design space. The\\nimplementation of SSO is demonstrated in Fig. 1. The reader may refer to the\\noriginal publication for a detailed explanation of SSO \\n        \",\n        \"\\n        H(I(i)) expresses the average relative sensitivity of E\\u03b8h(\\u03c6,\\u03b8) to \\u03c6. A\\nlow value of H(I(i)) indicates that E\\u03b8h(\\u03c6,\\u03b8) is more sensitive to \\u03c6, and\\nvice versa. A high value of H(I(i)), close to 1 corresponds to a sample\\ndensity in design space I(i) that approximates a uniform distribution and\\nsuggests that the identified subset I(i) has a low likelihood of containing\\n\\u03c6* \\n        \",\n        \"In the proposed approach, the Voronoi tessellation is implemented to\\npartition the design space into non-overlapping subregions (a set of\\nVoronoi cells) using the pool of samples distributed according to this\\nauxiliary PDF. Conceptually, Voronoi tessellation involves partitioning a\\nspace into convex polygons, called Voronoi cells, such that each cell\\ncontains exactly one sample, called a cell-generating sample. Every\\nsample in a given polygon is closer to its generating sample compared to\\nany other. In the proposed approach, the admissible set (a set of all\\nadmissible subspaces) is defined as a set containing all subsets of the set\\nof Voronoi cells. An alternative approach to identify the optimal subset\\nwithout performing any non-smooth deterministic optimization is also\\npresented. The general theoretical and computational framework for the\\niSSO algorithm is presented in the following subsections, and the\\nalgorithm is demonstrated in Fig. 2.\",\n        \"In the proposed approach, at the i + 1th iteration, say N(i) is the\\nnumber of samples distributed as \\u03c0(\\u03c6|I(i)) belonging to the design space\\nI(i). Let nv = N(i) /(1 + \\u03b3), \\u03b3 \\u2265 0 be the number of unique samples. If\\nsampling techniques such as accept rejection, importance sampling, etc.,\\nare used, then \\u03b3 = 0, and each sample in the design space will be unique.\\nHowever, if MCMC sampling techniques are used, the resulting samples\\nwill be correlated, that is \\u03b3 > 0, and we will have repeated samples.\\nAssume that the design space I(i) is divided into v(ki), k = 1\\u22c5\\u22c5\\u22c5nv, Voronoi\\ncells using nv unique samples, and say the Voronoi cell v(ki) contains \\u03b7(ki)\\nrepeated samples, then, an estimate of \\u03c0(\\u03c6|I(i)) is provided by:\\n\\u03b7(i)\\n\\u03c0 \\u03c6|I(i))= N(i)kV(i) \\u2265 0, \\u2200 \\u03c6 \\u2208 v(ki),\\nk\\n\\u222b\\nwhere, Vk(i) is the volume of the kth Voronoi cell. Obviously, I(i) \\u03c0(\\u03c6|I(i))\\nd\\u03c6 = 1.\",\n        \"Similar to the original SSO, the sensitivity of the objective function J\\n(\\u03c6|I(i)) to \\u03c6 is determined by evaluating the average value of J(\\u03c6|I(i))\\nover any subspace I of the design space I(i). Subset I is any subset of\\nnvVoronoi cells (these cells may be disjointed). Since the design space is\\npartitioned into nv subspaces or Voronoi cells, the number of admissible\\nsubsets (proper subsets) is given by 2n\\u03bd 1. Based on the estimate\\n\\u03c0(\\u03c6|I(i)) provided in (11), an estimate of H(I) is provided as:\\nH(I) = NV((ii))  V\\u03b7((((1i1i)))) ++ V\\u03b7((((i22i)))) ++ \\u22ef\\u22ef ++ \\u03b7V(((iS()Si))) .\\n(11)\\n(12)\\n(13)\",\n        \"A deterministic optimization needs to be performed to identify a\\nsubset I that contains the smallest volume density NI/VI. In the case of\\nunique samples, since \\u03b7((i\\u22c5)) = 1, the solution to the minimization problem\\nin (10) is a set of \\u03c1N(i) Voronoi cells with the largest volume. For the case\\nwith repeated samples, the optimization can be performed using\\nmethods appropriate for non-smooth optimization problems, such as\\nsub-gradient methods, bundle methods, gradient sampling methods, etc.\",\n        \"In this study, we propose an alternative approach to identify the\\noptimal subset without performing any non-smooth deterministic\\noptimization. A double-sort algorithm is proposed, which involves sorting\\nthe Voronoi cells in ascending order of the sample counts and then in\\ngroups of cells with the same sample count in descending order of cell\\nvolume. Finally, the top cells containing \\u03c1N(i)samples are selected as an\\napproximate optimal solution from the sorted list.\",\n        \"One may argue that the optimal subset can be obtained by first\\nsorting the Voronoi cells in ascending order of the cell density, defined\\nas \\u03b7(ki)/Vk(i), and then by selecting the top cells containing \\u03c1N(i) samples\\nfrom the sorted list. However, this argument is erroneous because the\\nobjective is to minimize \\u2211sS=1\\u03b7((is))/ \\u2211sS=1V((si)) and not \\u2211sS=1(\\u03b7((is)) /V((si))). The\\neffectiveness of the proposed double-sort algorithm is demonstrated in\\nSection 4 with the help of examples.\",\n        \"At the i + 1th iteration, \\u03c1N(i) samples distributed as \\u03c0(\\u03c6|I(i + 1)) are\\navailable from the previous iteration. Using these samples as seeds,\\nadditional (1 \\u03c1)N(i + 1) are simulated. The proposed method to\\nsimulate additional samples involves two steps: (a) randomly selecting a\\nVoronoi cell within the subset I(i + 1) based on the estimate \\u03c0(\\u03c6|I(i)) and\\n(b) applying the Metropolis-Hastings algorithm within the selected\\nVoronoi cell.\",\n        \"A Voronoi cell is selected according to the following weights in the\\nifrst step:\",\n        \"/\\nw(ki) = \\u2211\\u03b7(k\\u03b7i)(i)/VVk(i)(i)\\nk\",\n        \".\\nk k\\n(14)\",\n        \"An important issue for the effective implementation of the iSSO is the\\ncreation of the Voronoi cells at the current iteration bounded within the\\nVoronoi cell created at the previous iterations. Although it is possible to\\ncreate such bounded Voronoi cells, due to the geometrical complexities,\\nit is usually unfeasible for the higher dimensional problems (n\\u03c6>2). An\\nalternative approach is proposed in the present study for creating the\\nVoronoi cells at any iteration of the iSSO. The proposed approach\\ninvolves creating Voronoi cells using the samples generated at the current\\nand all previous iterations and then by considering Voronoi cells\\ncorresponding to the samples from the current iteration. This is shown in\\nFig. 3, where Fig. 3(a) shows the N samples at the first iteration and the\\ncorresponding Voronoi cells. Fig. 3(b) shows the \\u03c1N selected Voronoi\\ncells leading to the smallest volume density and the additional (1 \\u03c1)N\\nsamples being generated using these \\u03c1N samples as seeds. Fig. 3(c)\\nshows that the Voronoi cells are generated using all N + (1 \\u03c1)N\\nsamples that are generated in the two iterations. The Voronoi cells\\ncorresponding to the N samples for consideration at the second iteration\\nare also highlighted in Fig. 3(c). Fig. 3(d) shows a zoomed-in version of\\nFig. 3(c) where it can be observed that the area covered by the N Voronoi\\ncells considered in the second iteration is not the same as the area\\ncovered by the \\u03c1N Voronoi cells selected in the first iteration. On the\\ncontrary, the area covered by the Voronoi cells in the second iteration is\\nmore than the area covered by the Voronoi cells corresponding to the\\nseed samples from the first iteration. This is because a new sample\\nwithin the Voronoi cell between an existing sample and the existing\\nVoronoi cell edge results in the relocation of the Voronoi cell edge in a\",\n        \"To simulate a new sample within a selected Voronoi cell, the sample\\nthat generated the selected Voronoi cell or the last simulated sample in\\nthe selected Voronoi cell is used as the seed sample, and the\\nMetropolisHastings algorithm is implemented. A candidate sample \\u03c6c,\\u03b8c is\\nsimulated using the proposal q(\\u03c6c,\\u03b8c|\\u03c6,\\u03b8) and is accepted with the\\nprobability min(1, a0), where, a0 is given as:\\na0 =\\nh(\\u03c6c, \\u03b8c)p(\\u03c6c, \\u03b8c)q(\\u03c6, \\u03b8|\\u03c6c, \\u03b8c).\",\n        \"h(\\u03c6, \\u03b8)p(\\u03c6, \\u03b8)q(\\u03c6c, \\u03b8c|\\u03c6, \\u03b8)\",\n        \"In the present study, the proposed PDF is equal to the uniform PDF\\nfor design parameters and the initial PDF for uncertain variables, i.e., q\\n(\\u03c6, \\u03b8|\\u03c6c,\\u03b8c) = p(\\u03c6, \\u03b8). Therefore, on simplifying (15), a0 is given as:\\na0 =\\nh(\\u03c6c, \\u03b8c).\",\n        \"h(\\u03c6, \\u03b8)\",\n        \"\\n          A new stopping criterion is proposed in this study. The convergence\\nof the expected value of the performance measure h(\\u03c6, \\u03b8) with respect to\\nthe PDF for \\u03c6 and \\u03b8 in consecutive iterations is used as the stopping\\ncriterion. Mathematically the proposed stopping criterion is represented\\nby:\\n\\u20d2\\n\\u20d2E\\u03c6,\\u03b8h(\\u03c6, \\u03b8)i E\\u03c6,\\u03b8h(\\u03c6, \\u03b8)i 1\\u20d2\\u20d2 \\u2264 \\u03b5 (17)\\nwhere, \\u03b5 is a user-specified tolerance limit. Other stopping criteria, as\\nindicated in \\n          \",\n        \"In the iSSO framework, a deterministic optimization problems can\\nalso be handled with the vector of uncertain variables \\u03b8 set equal to a\\nnull vector (n\\u03b8 = 0). Since the determination of the subset at each iSSO\\niteration is solely dependent on the samples distributed as \\u03c0(\\u03c6), no\\nmodification to the iSSO algorithm is required to solve a deterministic\\noptimization problem, and the entire formulation remains valid.\"\n      ]\n    },\n    {\n      \"title\": \"3.1. Partitioning of design space\",\n      \"paragraphs\": [\n        \"In the proposed approach, at the i + 1th iteration, say N(i) is the\\nnumber of samples distributed as \\u03c0(\\u03c6|I(i)) belonging to the design space\\nI(i). Let nv = N(i) /(1 + \\u03b3), \\u03b3 \\u2265 0 be the number of unique samples. If\\nsampling techniques such as accept rejection, importance sampling, etc.,\\nare used, then \\u03b3 = 0, and each sample in the design space will be unique.\\nHowever, if MCMC sampling techniques are used, the resulting samples\\nwill be correlated, that is \\u03b3 > 0, and we will have repeated samples.\\nAssume that the design space I(i) is divided into v(ki), k = 1\\u22c5\\u22c5\\u22c5nv, Voronoi\\ncells using nv unique samples, and say the Voronoi cell v(ki) contains \\u03b7(ki)\\nrepeated samples, then, an estimate of \\u03c0(\\u03c6|I(i)) is provided by:\\n\\u03b7(i)\\n\\u03c0 \\u03c6|I(i))= N(i)kV(i) \\u2265 0, \\u2200 \\u03c6 \\u2208 v(ki),\\nk\\n\\u222b\\nwhere, Vk(i) is the volume of the kth Voronoi cell. Obviously, I(i) \\u03c0(\\u03c6|I(i))\\nd\\u03c6 = 1.\",\n        \"Similar to the original SSO, the sensitivity of the objective function J\\n(\\u03c6|I(i)) to \\u03c6 is determined by evaluating the average value of J(\\u03c6|I(i))\\nover any subspace I of the design space I(i). Subset I is any subset of\\nnvVoronoi cells (these cells may be disjointed). Since the design space is\\npartitioned into nv subspaces or Voronoi cells, the number of admissible\\nsubsets (proper subsets) is given by 2n\\u03bd 1. Based on the estimate\\n\\u03c0(\\u03c6|I(i)) provided in (11), an estimate of H(I) is provided as:\\nH(I) = NV((ii))  V\\u03b7((((1i1i)))) ++ V\\u03b7((((i22i)))) ++ \\u22ef\\u22ef ++ \\u03b7V(((iS()Si))) .\\n(11)\\n(12)\\n(13)\"\n      ]\n    },\n    {\n      \"title\": \"3.2. Identification of an optimal subset\",\n      \"paragraphs\": [\n        \"A deterministic optimization needs to be performed to identify a\\nsubset I that contains the smallest volume density NI/VI. In the case of\\nunique samples, since \\u03b7((i\\u22c5)) = 1, the solution to the minimization problem\\nin (10) is a set of \\u03c1N(i) Voronoi cells with the largest volume. For the case\\nwith repeated samples, the optimization can be performed using\\nmethods appropriate for non-smooth optimization problems, such as\\nsub-gradient methods, bundle methods, gradient sampling methods, etc.\",\n        \"In this study, we propose an alternative approach to identify the\\noptimal subset without performing any non-smooth deterministic\\noptimization. A double-sort algorithm is proposed, which involves sorting\\nthe Voronoi cells in ascending order of the sample counts and then in\\ngroups of cells with the same sample count in descending order of cell\\nvolume. Finally, the top cells containing \\u03c1N(i)samples are selected as an\\napproximate optimal solution from the sorted list.\",\n        \"One may argue that the optimal subset can be obtained by first\\nsorting the Voronoi cells in ascending order of the cell density, defined\\nas \\u03b7(ki)/Vk(i), and then by selecting the top cells containing \\u03c1N(i) samples\\nfrom the sorted list. However, this argument is erroneous because the\\nobjective is to minimize \\u2211sS=1\\u03b7((is))/ \\u2211sS=1V((si)) and not \\u2211sS=1(\\u03b7((is)) /V((si))). The\\neffectiveness of the proposed double-sort algorithm is demonstrated in\\nSection 4 with the help of examples.\"\n      ]\n    },\n    {\n      \"title\": \"3.3. Simulation of conditional samples\",\n      \"paragraphs\": [\n        \"At the i + 1th iteration, \\u03c1N(i) samples distributed as \\u03c0(\\u03c6|I(i + 1)) are\\navailable from the previous iteration. Using these samples as seeds,\\nadditional (1 \\u03c1)N(i + 1) are simulated. The proposed method to\\nsimulate additional samples involves two steps: (a) randomly selecting a\\nVoronoi cell within the subset I(i + 1) based on the estimate \\u03c0(\\u03c6|I(i)) and\\n(b) applying the Metropolis-Hastings algorithm within the selected\\nVoronoi cell.\",\n        \"A Voronoi cell is selected according to the following weights in the\\nifrst step:\",\n        \"/\\nw(ki) = \\u2211\\u03b7(k\\u03b7i)(i)/VVk(i)(i)\\nk\",\n        \".\\nk k\\n(14)\"\n      ]\n    },\n    {\n      \"title\": \"3.5. Implementation issues\",\n      \"paragraphs\": [\n        \"An important issue for the effective implementation of the iSSO is the\\ncreation of the Voronoi cells at the current iteration bounded within the\\nVoronoi cell created at the previous iterations. Although it is possible to\\ncreate such bounded Voronoi cells, due to the geometrical complexities,\\nit is usually unfeasible for the higher dimensional problems (n\\u03c6>2). An\\nalternative approach is proposed in the present study for creating the\\nVoronoi cells at any iteration of the iSSO. The proposed approach\\ninvolves creating Voronoi cells using the samples generated at the current\\nand all previous iterations and then by considering Voronoi cells\\ncorresponding to the samples from the current iteration. This is shown in\\nFig. 3, where Fig. 3(a) shows the N samples at the first iteration and the\\ncorresponding Voronoi cells. Fig. 3(b) shows the \\u03c1N selected Voronoi\\ncells leading to the smallest volume density and the additional (1 \\u03c1)N\\nsamples being generated using these \\u03c1N samples as seeds. Fig. 3(c)\\nshows that the Voronoi cells are generated using all N + (1 \\u03c1)N\\nsamples that are generated in the two iterations. The Voronoi cells\\ncorresponding to the N samples for consideration at the second iteration\\nare also highlighted in Fig. 3(c). Fig. 3(d) shows a zoomed-in version of\\nFig. 3(c) where it can be observed that the area covered by the N Voronoi\\ncells considered in the second iteration is not the same as the area\\ncovered by the \\u03c1N Voronoi cells selected in the first iteration. On the\\ncontrary, the area covered by the Voronoi cells in the second iteration is\\nmore than the area covered by the Voronoi cells corresponding to the\\nseed samples from the first iteration. This is because a new sample\\nwithin the Voronoi cell between an existing sample and the existing\\nVoronoi cell edge results in the relocation of the Voronoi cell edge in a\",\n        \"To simulate a new sample within a selected Voronoi cell, the sample\\nthat generated the selected Voronoi cell or the last simulated sample in\\nthe selected Voronoi cell is used as the seed sample, and the\\nMetropolisHastings algorithm is implemented. A candidate sample \\u03c6c,\\u03b8c is\\nsimulated using the proposal q(\\u03c6c,\\u03b8c|\\u03c6,\\u03b8) and is accepted with the\\nprobability min(1, a0), where, a0 is given as:\\na0 =\\nh(\\u03c6c, \\u03b8c)p(\\u03c6c, \\u03b8c)q(\\u03c6, \\u03b8|\\u03c6c, \\u03b8c).\",\n        \"h(\\u03c6, \\u03b8)p(\\u03c6, \\u03b8)q(\\u03c6c, \\u03b8c|\\u03c6, \\u03b8)\",\n        \"In the present study, the proposed PDF is equal to the uniform PDF\\nfor design parameters and the initial PDF for uncertain variables, i.e., q\\n(\\u03c6, \\u03b8|\\u03c6c,\\u03b8c) = p(\\u03c6, \\u03b8). Therefore, on simplifying (15), a0 is given as:\\na0 =\\nh(\\u03c6c, \\u03b8c).\",\n        \"h(\\u03c6, \\u03b8)\"\n      ]\n    },\n    {\n      \"title\": \"3.4. Stopping criteria\",\n      \"paragraphs\": [\n        \"\\n          A new stopping criterion is proposed in this study. The convergence\\nof the expected value of the performance measure h(\\u03c6, \\u03b8) with respect to\\nthe PDF for \\u03c6 and \\u03b8 in consecutive iterations is used as the stopping\\ncriterion. Mathematically the proposed stopping criterion is represented\\nby:\\n\\u20d2\\n\\u20d2E\\u03c6,\\u03b8h(\\u03c6, \\u03b8)i E\\u03c6,\\u03b8h(\\u03c6, \\u03b8)i 1\\u20d2\\u20d2 \\u2264 \\u03b5 (17)\\nwhere, \\u03b5 is a user-specified tolerance limit. Other stopping criteria, as\\nindicated in \\n          \"\n      ]\n    },\n    {\n      \"title\": \"3.6. Special case: deterministic optimization\",\n      \"paragraphs\": [\n        \"In the iSSO framework, a deterministic optimization problems can\\nalso be handled with the vector of uncertain variables \\u03b8 set equal to a\\nnull vector (n\\u03b8 = 0). Since the determination of the subset at each iSSO\\niteration is solely dependent on the samples distributed as \\u03c0(\\u03c6), no\\nmodification to the iSSO algorithm is required to solve a deterministic\\noptimization problem, and the entire formulation remains valid.\"\n      ]\n    },\n    {\n      \"title\": \"4. Illustrative examples\",\n      \"paragraphs\": [\n        \"In this section, typical optimization problems are considered to\\ndemonstrate the effectiveness and efficiency of the proposed approach.\",\n        \"First, deterministic optimization problems are considered. These\\nproblems include several local and global minima. Next, stochastic\\noptimization problems are illustrated. The second example presents an RDO\\nproblem of the TMD. In this example, the variance minimization of the\\nprotected structure\\u2019s displacement (TMD attached to the structure) is\\nperformed. In the third example, the mean minimization of 120 bars\\ntruss problems is explored to demonstrate the applicability of the\\nproposed approach to a high-dimensional stochastic design problem.\",\n        \"Finally, the fourth example investigates the reliability-based\\noptimization of a base isolation system for a 10-story building.\",\n        \"In this study, after implementing iSSO, the optimal design solution is\\nidentified as follows. Let \\u03b8j, j = 1\\u22c5\\u22c5\\u22c5n be a set of independent, identically\\ndistributed realizations of \\u03b8, and let h(\\u03c6, \\u03b8j) be the structural\\nperformance function realization for \\u03b8j. The expected structural performance\\nfunction is approximated by the average of the realizations as:\\nE\\u03b8h(\\u03c6, \\u03b8) \\u2248\\n1 \\u2211n\\nn j=1\",\n        \")\\nh \\u03c6, \\u03b8j .\",\n        \"E\\u03b8h(\\u03c6, \\u03b8) is evaluated for all unique \\u03c6 samples obtained at the last\\niteration of the iSSO, and the \\u03c6 sample resulting in the smallest value of\\nE\\u03b8h(\\u03c6, \\u03b8) is taken as the optimal solution. Alternatively, as the\\nrighthand side of (18) is deterministic, any deterministic optimization\\nmethod can also be used to solve the optimization problem with the\\napproximate expectation.\",\n        \"In the following examples, both iSSO and SSO are implemented with\\nN = 1000n\\u03c6, \\u03c1 = 0.20 and the stopping criteria as stated in (17). Here, a\\nvalue of \\u03b5 = 10 3 is adopted.\",\n        \"(18)\",\n        \"\\n        In this section, three two-dimensional benchmark deterministic\\noptimization problems are considered. Results are also compared with\\nthe SSO. The test functions are:\\n\\u20d2\\u20d2\\u20d2 (\\u20d2\\u20d2\\u20d2\\nminh(\\u03c6) = \\u20d2\\u20d2sin(\\u03c61)cos(\\u03c62)exp \\u20d2\\u20d21\\ns.t.\\u03c6 = \\n        \",\n        \"\\u20d2\\n\\u03c0 \\u20d2\\u20d2 \\u20d2\\u20d2,\\n(21))\\na) Griewank function:\\nminh(\\u03c6) =\\n\\u221a\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u20d2\\u0305)\\u0305\\u20d2\\u20d2\\u0305\\u0305\\u0305\\u0305)\\u03050\\u0305.1\\u0305\\n\\u03c621\\u03c0+ \\u03c622\\u20d2\\u20d2\\u20d2\\u20d2 \\u20d2\\u20d2\\u20d2 + 1 ,\\n(19)\\n(20)\",\n        \"The results for the Griewank function are presented in Fig. 4. Fig. 4(a,\\nb) shows that the function has multiple closely spaced local minima with\\na single global minimum. Fig. 4(c, d) shows the SSO optimization using\\nhyper-rectangle and hyper-ellipse as shapes of admissible subsets. It is\\nseen that these shapes fail to capture the region containing the optimal\\ndesign due to the presence of multiple local minima. Next, the iSSO is\\nimplemented, where the Voronoi cells selected at the first and last\\niteration are shown in Fig. 4(e, f). It is observed that at the first iteration,\\nthe selected Voronoi cells effectively capture both the local and global\\nminima and in the subsequent iterations, the selected cells are more\\nconcentrated near the global minimum. The region selected at the last\\niteration captures the optimal global solution.\",\n        \"The Cross-in-Tray function has a relatively complex design space\\ncompared to the Griewank function. Fig. 5(a, b) shows multiple local\\nand global minima. Minimization by using SSO is demonstrated in Fig. 5\\n(c, d). It is found that both the hyper-rectangle and hyper-ellipse are\\ntrapped around any one of the global minima. At the same time, the iSSO\\nis able to capture the regions that include all of the global minima, as\\nseen in Fig. 5(e, f).\",\n        \"The Holder Table function has multiple local and global minima; the\\nglobal minima are placed at the boundary of the design space, as shown\\nin Fig. 6(a, b). Once again, it is seen that both the hyper-rectangle and\\nhyper-ellipse are trapped around any of one of the global minima, and\\non the other hand, the iSSO is able to capture the regions that include all\\nof the global minima, as seen in Fig. 6(e, f).\",\n        \"The results from the three examples demonstrate that the proposed\\niSSO is able to capture the regions containing the optimal solution\\neffectively.\",\n        \"Next, the statistics of the results of 50 independent runs, both for SSO\\nand iSSO are presented in Table 1. It also includes the results obtained by\\nusing state-of-the-art approaches, such as the Genetic algorithm, particle\\nswarm optimization, and the gradient based optimization approach\\n(interior-point algorithm). The proposed iSSO outperforms all other\\napproaches as more successes in determining the optimal solution are\\nobserved in all three optimization problems. It is also seen that both SSO\\nand iSSO result in a similar value of volume reduction for the same\\nstopping criterion; however, with SSO, the number of iterations required\\nto achieve this volume reduction are relatively higher. The proposed\\napproach outperformed the state-of-the-art approaches, as indicated by\\nthe number of successes. These examples demonstrate that the main\\nadvantage of implementing Voronoi tessellation is an effective\\nexploration of the design space.\",\n        \"Next, the performance of the proposed \\\"double sort algorithm\\\" for\\nselecting the optimal subset is studied by using the above-mentioned\\nthree functions. Fig. 7 shows the value of H(I(1)) for the 50\\nindependent simulation runs, which is estimated by implementing the proposed\\ndouble sort algorithm and by using the Genetic algorithm. It can be\\nnoted that for each run, the H(I(1)) values obtained using the proposed\\ndouble sort algorithm and Genetic algorithm are well matched, thereby\\nconfirming the adequacy of the proposed double sort algorithm.\",\n        \"At any iteration of iSSO, new samples are simulated using the seed\\nsamples. In the proposed approach, the volume of the Voronoi cells\\ncorresponding to the seed and new samples is greater than the volume of\\nthe Voronoi cells corresponding only to the seed samples. Fig. 8 shows\\nthis change in volume V(seeds+Vn(eswee)ds)V(seeds) due to the creation of Voronoi cells\\nat any generation of iSSO using the procedure mentioned in Section 3.4.\",\n        \"The increase is observed to be small which further reduces with an\\nincrease in the iteration number. It is also observed that the increase in\\nvolume decreases with an increase in sample size at each iteration and\\nincreases with an increase in the dimension of the problem.\\n4.2. Robust design optimization of the tuned mass damper\",\n        \"\\n        This example considers a stochastic design problem involving a\\nTuned Mass Damper (TMD) attached to a Single Degree of Freedom\\n(SDOF) system. The problem is taken from \\n        \",\n        \"In this problem, the system is excited by a white noise signal with a\\nmean zero and unit variance. The performance measure is the variance\\nof the displacement of the system \\u03c3x2s . The mass mS, stiffness kS, and\\nGA = genetic algorithm, PSO = particle swarm optimization, GBA = gradient-based optimization approach, NF = no. of. failure, NS = no. of. success, BV = best value,\\nWV = worst value, AV = average value, c.o.v = coefficient of variation, FE = no. of. function evaluations, Gen = generations, VR = volume reduction percentage, * =\\nefficiently applicable only for deterministic problems.\\ndamping cS of the system are taken as uncertain parameters, following\\nindependent Gaussian distribution. The mean value of these variables is\\ntaken to be 105 kg, 107 N/m, and 4 \\u00d7 104 Ns/m respectively. To account\\nfor uncertainty, the c.o.v value for each variable taken is 0.05. The\\nfrequency ratio \\u03b2 = \\u03c9T /\\u03c9S and damping \\u03beT of the TMD are considered\\ndesign parameters. The TMD has a mass ratio, mT/ms, of 0.10. The\\nparameters mT,\\u03c9T,and\\u03c9S a\\u221are\\u0305, \\u0305in\\u0305\\u0305o\\u0305r\\u0305d\\u0305e\\u0305r\\u0305,\\u0305t\\u0305h\\u0305e\\u0305m\\u0305ass of the TMD, the natural\\nfrequency of the TMD ( kT/mT), and the natural frequency of the\",\n        \"\\u221a\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\nstructure ( ks/ms). The optimization problem is written as:\\n( )2\",\n        \"\\n        (22)\\niSSO are shown. SAA is applied with a sample size of 103, as mentioned\\nin \\n        \",\n        \"\\n          The third example involves minimizing the mean of the compliance\\nof a 120-bar linear elastic truss structure shown in Fig. 10 under the\\nweight constraint W \\u2264 15, 000kg. Because of structural symmetry,\\ndesign parameters corresponding to the cross-sectional areas of elements\\nare divided into seven groups, each with a minimum area of 10 4 m2.\\nThe Young\\u2019s modulus for the bar groups are assumed as uncorrelated\\nnormal random variables with mean values equal to 210 GPa and the c.\\no.v equal to 0.10 respectively. The density of the material is 7971.89 kg/\\nm3. The dome is subjected to concentrated vertical loads acting\\ndownward at the top node, normally distributed with a mean equal to 60 kN\\nand c.o.v equal to 0.20. In addition, the mass of bars is concentrated at\\nthe nodes. The problem is taken from \\n          \",\n        \"Table 3 presents the best of 10 independent run results obtained with\\nSSO and iSSO. Once again, the SSO and iSSO solutions agree well,\\nthereby demonstrating the effectiveness of the proposed approach. At\\nthe same time, the number of function evaluations is substantially less in\\nthe case of iSSO, indicating the efficiency of the proposed approach.\",\n        \"\\n          This example, adapted from \\n          \",\n        \"A6\\n(cm2)\\n55.6\\n49.5\\n56.5\",\n        \"A7\\n(cm2)\\nwhere, IF(\\u03c6,\\u03b8) is the function that indicates failure, and it equals 1 when\\nthe system fails, i.e., when unacceptable performance occurs. Notably,\\nin this problem h(\\u03c6, \\u03b8) = IF(\\u03c6,\\u03b8).\",\n        \"The 10-story building is considered as a shear structure with\\nuncertain inter-story stiffness and damping. Each story has a total mass of\\n207 ton. The inter-story stiffness ki of all stories are parameterized by ki\\n= \\u0302ki\\u03b8i, i = 1, \\u2026, 10 where the most probable values of the inter-story\\nstiffness are \\u0302ki = 687.1, 613.1, 540.1, 481.1, 421.7, 353.7, 286.6,\\n225.6, 184.5, 104.5 MN/m. The entity \\u03b8i is a set of non-dimensional\\nuncertain variables that are considered to be correlated Gaussian\\nvariables with a unit mean value \\u0302\\u03b8i= 1, \\u2200i and a covariance matrix defined\\nas:\\nE(\\u03b8i \\u0302\\u03b8i) \\u03b8j \\u0302\\u03b8j)= (0.2)2exp (j i)2 /22. (25)\",\n        \"The damping ratios are considered independent Gaussian variables\\nwith mean values of 0.025 and c.o.v of 0.10 for all modes. The\\nKanaiTajimi model is used to simulate the ground excitation modelled as a\\nifltered white noise process, with the power spectral density function\\ngiven as:\\nS(\\u03c9) = S0(\\n\\u03c94g +)4\\u03b6g2\\u03c92g\\u03c92\",\n        \"2\\n\\u03c92\\ng\\n\\u03c92\\n+ 4\\u03b6g2\\u03c92g\\u03c92\\n,\\n(26)\\n(27)\\n(28)\\nS0 = \\u03c32\\u03c9\",\n        \"(2\\u03b6g\\n\\u03c0\\u03c9g 4\\u03b6g2 + 1\",\n        \"/\\n)m2\",\n        \"\\n          s3,\\nwhere, \\u03c9g, \\u03b6gand \\u03c3\\u03c9 are the resonant frequency, damping, and RMS of the\\nacceleration input of the filter, respectively. These are also considered\\nuncertain variables with mean values of 2\\u03c0rad/s, 0.5, 0.2g and a c.o.v\\nequal to 0.20. The non-stationarity of the excitation is modeled by\\nmultiplying the filter output with the envelope function as:\\ne(t) = \\u03bb3t\\u03bb1 exp( \\u03bb2t),\\nwith parameters \\u03bb1 = 1.25, \\u03bb2 = 0.2 and \\u03bb3 = 0.353 chosen to simulate\\nstrong earthquake excitation for a duration of 40 s with a sampling time\\nof 0.02 s. The base-isolation system considered is a lead\\u2013rubber bilinear\\nisolator with an additional viscous damper. The base has a 247-ton mass.\\nThe design parameters \\u03c6 for the base isolation structure system are the\\nstiffness before yielding Kprand after yielding Kp, the yield force is Fy,\\nand the damping coefficient cd. The reader may refer to \\n          \",\n        \"\\n          Failure is indicated when any of the normalized base displacements\\nor inter-story drifts exceeds unity. The normalization constants are 0.5 m\\nand 0.033 m respectively. The design interval for each variable is\\nspecified as Kpr = \\n          \",\n        \"Table 4 shows the optimization results for the best 10 independent\\nsimulation runs. The comparison of the results obtained using SSO, SAA\\n(with a sample size of 103), and iSSO shows that the optimal design\\nobtained using the proposed approach iSSO is in good agreement. The\\nfailure probability of the structure is reduced from 0.95 (without the\\nbase isolation system) to 0.0326 after installing the optimally designed\\nbase isolation system.\"\n      ]\n    },\n    {\n      \"title\": \"4.3. 120-bars truss structure\",\n      \"paragraphs\": [\n        \"\\n          The third example involves minimizing the mean of the compliance\\nof a 120-bar linear elastic truss structure shown in Fig. 10 under the\\nweight constraint W \\u2264 15, 000kg. Because of structural symmetry,\\ndesign parameters corresponding to the cross-sectional areas of elements\\nare divided into seven groups, each with a minimum area of 10 4 m2.\\nThe Young\\u2019s modulus for the bar groups are assumed as uncorrelated\\nnormal random variables with mean values equal to 210 GPa and the c.\\no.v equal to 0.10 respectively. The density of the material is 7971.89 kg/\\nm3. The dome is subjected to concentrated vertical loads acting\\ndownward at the top node, normally distributed with a mean equal to 60 kN\\nand c.o.v equal to 0.20. In addition, the mass of bars is concentrated at\\nthe nodes. The problem is taken from \\n          \",\n        \"Table 3 presents the best of 10 independent run results obtained with\\nSSO and iSSO. Once again, the SSO and iSSO solutions agree well,\\nthereby demonstrating the effectiveness of the proposed approach. At\\nthe same time, the number of function evaluations is substantially less in\\nthe case of iSSO, indicating the efficiency of the proposed approach.\"\n      ]\n    },\n    {\n      \"title\": \"4.4. Reliability-based design of a base isolated structure\",\n      \"paragraphs\": [\n        \"\\n          This example, adapted from \\n          \",\n        \"A6\\n(cm2)\\n55.6\\n49.5\\n56.5\",\n        \"A7\\n(cm2)\\nwhere, IF(\\u03c6,\\u03b8) is the function that indicates failure, and it equals 1 when\\nthe system fails, i.e., when unacceptable performance occurs. Notably,\\nin this problem h(\\u03c6, \\u03b8) = IF(\\u03c6,\\u03b8).\",\n        \"The 10-story building is considered as a shear structure with\\nuncertain inter-story stiffness and damping. Each story has a total mass of\\n207 ton. The inter-story stiffness ki of all stories are parameterized by ki\\n= \\u0302ki\\u03b8i, i = 1, \\u2026, 10 where the most probable values of the inter-story\\nstiffness are \\u0302ki = 687.1, 613.1, 540.1, 481.1, 421.7, 353.7, 286.6,\\n225.6, 184.5, 104.5 MN/m. The entity \\u03b8i is a set of non-dimensional\\nuncertain variables that are considered to be correlated Gaussian\\nvariables with a unit mean value \\u0302\\u03b8i= 1, \\u2200i and a covariance matrix defined\\nas:\\nE(\\u03b8i \\u0302\\u03b8i) \\u03b8j \\u0302\\u03b8j)= (0.2)2exp (j i)2 /22. (25)\",\n        \"The damping ratios are considered independent Gaussian variables\\nwith mean values of 0.025 and c.o.v of 0.10 for all modes. The\\nKanaiTajimi model is used to simulate the ground excitation modelled as a\\nifltered white noise process, with the power spectral density function\\ngiven as:\\nS(\\u03c9) = S0(\\n\\u03c94g +)4\\u03b6g2\\u03c92g\\u03c92\",\n        \"2\\n\\u03c92\\ng\\n\\u03c92\\n+ 4\\u03b6g2\\u03c92g\\u03c92\\n,\\n(26)\\n(27)\\n(28)\\nS0 = \\u03c32\\u03c9\",\n        \"(2\\u03b6g\\n\\u03c0\\u03c9g 4\\u03b6g2 + 1\",\n        \"/\\n)m2\",\n        \"\\n          s3,\\nwhere, \\u03c9g, \\u03b6gand \\u03c3\\u03c9 are the resonant frequency, damping, and RMS of the\\nacceleration input of the filter, respectively. These are also considered\\nuncertain variables with mean values of 2\\u03c0rad/s, 0.5, 0.2g and a c.o.v\\nequal to 0.20. The non-stationarity of the excitation is modeled by\\nmultiplying the filter output with the envelope function as:\\ne(t) = \\u03bb3t\\u03bb1 exp( \\u03bb2t),\\nwith parameters \\u03bb1 = 1.25, \\u03bb2 = 0.2 and \\u03bb3 = 0.353 chosen to simulate\\nstrong earthquake excitation for a duration of 40 s with a sampling time\\nof 0.02 s. The base-isolation system considered is a lead\\u2013rubber bilinear\\nisolator with an additional viscous damper. The base has a 247-ton mass.\\nThe design parameters \\u03c6 for the base isolation structure system are the\\nstiffness before yielding Kprand after yielding Kp, the yield force is Fy,\\nand the damping coefficient cd. The reader may refer to \\n          \",\n        \"\\n          Failure is indicated when any of the normalized base displacements\\nor inter-story drifts exceeds unity. The normalization constants are 0.5 m\\nand 0.033 m respectively. The design interval for each variable is\\nspecified as Kpr = \\n          \",\n        \"Table 4 shows the optimization results for the best 10 independent\\nsimulation runs. The comparison of the results obtained using SSO, SAA\\n(with a sample size of 103), and iSSO shows that the optimal design\\nobtained using the proposed approach iSSO is in good agreement. The\\nfailure probability of the structure is reduced from 0.95 (without the\\nbase isolation system) to 0.0326 after installing the optimally designed\\nbase isolation system.\"\n      ]\n    },\n    {\n      \"title\": \"5. Conclusion\",\n      \"paragraphs\": [\n        \"This study attempts to provide an optimization approach called\\n\\\"iSSO\\\", which is an improved version of SSO, primarily for stochastic\\noptimization problems while it retains utility for deterministic\\noptimization problems as well. Two novel ideas are introduced in this study:\\nifrst, a better characterization of the design space is offered by\\npartitioning the design space into non-overlapping subregions using Voronoi\\ntessellation which improves the effectiveness and efficiency of the\\nproposed iSSO considerably in comparison to SSO. Second, a novel \\\"double\\nsort\\\" approach is proposed, eliminating the need for optimization to\\nidentify the subregions for the optimal design at each iSSO iteration.\\nSeveral mathematical and engineering design examples, including TMD,\\n120 bars truss structure, and base-isolated structure, are included in this\\nstudy to demonstrate the efficacy of the proposed iSSO. The results show\\nthat the proposed iSSO effectively identifies the reduced design space for\\ncomplex design problems with multiple global and local minima. This is\\nattributable to the Voronoi tessellation, which eliminates the\\nrequirement of the presumed admissible design space form to resemble the\\ncontour of the original design. Voronoi tessellation enabled better\\ndesign space exploration, allowing multiple global minima scattered\\nthroughout the design pace to be effectively identified. Due to the\\ndiscretization of the design space via Voronoi tessellation, computation\\ndemand is significantly reduced as the number of function evaluations\\nfor all examples is lower vis-a-vis the original SSO. Moreover, the novel\\nidea of the double sort approach achieves the requisite precision in\\nidentifying the subregions for optimal solutions and makes iSSO\\nimplementation simple and effective.\",\n        \"The applicability of the approach is dependent on the creation of the\"\n      ]\n    },\n    {\n      \"title\": \"Appendix-A: Voronoi Tessellation\",\n      \"paragraphs\": [\n        \"Voronoi cells. At present the methods available in the literation for\\ncreating the Voronoi tessellation are computationally demanding when\\nconsidering problems of very high dimension. Future work will focus on\\ndeveloping a method for creating the Voronoi tessellation in higher\\ndimensions, particularly those greater than ten.\"\n      ]\n    },\n    {\n      \"title\": \"CRediT authorship contribution statement\",\n      \"paragraphs\": [\n        \"Mohd Aman Khalid: Investigation, Methodology, Formal analysis,\\nSoftware, Visualization, Writing \\u2013 original draft. Sahil Bansal:\\nConceptualization, Methodology, Supervision.\"\n      ]\n    },\n    {\n      \"title\": \"Declaration of Competing Interest\",\n      \"paragraphs\": [\n        \"The authors declare that they have no known competing financial\\ninterests or personal relationships that could have appeared to influence\\nthe work reported in this paper.\"\n      ]\n    },\n    {\n      \"title\": \"Data availability\",\n      \"paragraphs\": [\n        \"No data was used for the research described in the article.\",\n        \"Voronoi tessellation is a mathematical concept named after the Russian mathematician Georgy Voronoi. It is also known as the Voronoi diagram or\\nDirichlet tessellation. A Voronoi tessellation of a set of points P in a plane is a partition of the plane into a set of non-overlapping convex polygons, with\\neach polygon including precisely one point of P and each point in a polygon being closer to its associated point in P than to any other point in P. Each\\npolygon is referred to as a Voronoi cell or a Dirichlet region. The boundary of each cell is constituted of points that are equidistant to two or more\\npoints in P. Fig. 12 shows the Voronoi diagram in a two-dimensional design space.\",\n        \"\\n        There are several efficient algorithms for creating Voronoi diagrams. One such basic algorithm is to start with a set of points and then compute the\\nVoronoi cells by dividing the space into regions based on the distance to the nearest point. The Bowyer-Watson algorithm \\n        \",\n        \"\\n        It can be summarized that Voronoi tessellation is a powerful mathematical concept that aids in dividing space into regions based on the distance to\\na set of points. Voronoi tessellation finds widespread applications in areas such as image processing \\n        \"\n      ]\n    }\n  ],\n  \"references\": [\n    \"Ref ID: ref1, Given Names: K., Surname: Marti, Year: 2008, Article Title: Stochastic optimization methods, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref2, Given Names: Y ND M., Surname: Tsompanakis Lagaros Papadrakakis, Year: 2008, Article Title: Structural design optimization considering uncertainties, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref3, Given Names: MA S V., Surname: Khalid Bansal Ramamohan, Year: 2023, Article Title: An augmented formulation for robust design optimization of structures using stochastic simulation method, Source: Res Eng Des, Volume: 34, Issue: None, First Page: 179, Last Page: 200\",\n    \"Ref ID: ref4, Given Names: Z G X SM A, Surname: Meng Li Wang Sait R\\u0131za, Year: 2021, Article Title: A comparative study of metaheuristic algorithms for reliability \\u2011 based design optimization problems, Source: Arch Comput Methods Eng, Volume: 28, Issue: None, First Page: 1853, Last Page: 69\",\n    \"Ref ID: ref5, Given Names: L MA AM M, Surname: Abualigah Elaziz Khasawneh Alshinwan, Year: 2022, Article Title: Meta-heuristic optimization algorithms for solving real-world mechanical engineering design problems : a comprehensive survey, applications, comparative analysis, and results, Source: Neural Comput Appl, Volume: 34, Issue: None, First Page: 4081, Last Page: 110\",\n    \"Ref ID: ref6, Given Names: J M S T M., Surname: Katebi Shoaei Nguyen Trung Khorami, Year: 2020, Article Title: Developed comparative analysis of metaheuristic optimization algorithms for optimal active control of structures, Source: Eng Comput, Volume: 36, Issue: None, First Page: 1539, Last Page: 58\",\n    \"Ref ID: ref7, Given Names: A., Surname: Alorf, Year: 2023, Article Title: Engineering applications of artificial intelligence a survey of recently developed metaheuristics and their comparative analysis, Source: Eng Appl Artif Intell, Volume: 117, Issue: None, First Page: 105622, Last Page: None\",\n    \"Ref ID: ref8, Given Names: U., Surname: Kirsch, Year: 2012, Article Title: Structural optimization: fundamentals and applications, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref9, Given Names: CA PA, Surname: Floudas Pardalos, Year: 2008, Article Title: None, Source: Encyclopedia of optimization, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref10, Given Names: AD O, Surname: Kiureghian Ditlevsen, Year: 2009, Article Title: Aleatory or epistemic? Does it matter?, Source: Struct Saf, Volume: 31, Issue: None, First Page: 105, Last Page: 12\",\n    \"Ref ID: ref11, Given Names: \\u00a8eller GI HA, Surname: Schu Jensen, Year: 2008, Article Title: Computational methods in optimization considering uncertainties - an overview, Source: Comput Methods Appl Mech Eng, Volume: 198, Issue: None, First Page: 2, Last Page: 13\",\n    \"Ref ID: ref12, Given Names: J S, Surname: Schneider Kirkpatrick, Year: 2007, Article Title: None, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref13, Given Names: B M., Surname: Do Ohsaki, Year: 2021, Article Title: A random search for discrete robust design optimization of linearelastic steel frames under interval parametric uncertainty, Source: Comput Struct, Volume: 249, Issue: None, First Page: 106506, Last Page: None\",\n    \"Ref ID: ref14, Given Names: A M JK, Surname: Asadpoure Tootkaboni Guest, Year: 2011, Article Title: Robust topology optimization of structures with uncertainties in stiffness - application to truss structures, Source: Comput Struct, Volume: 11, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref15, Given Names: I, Surname: Doltsinis, Year: 2004, Article Title: Robust design of structures using optimization methods, Source: Comput Methods Appl Mech Eng, Volume: 193, Issue: None, First Page: 2221, Last Page: 37\",\n    \"Ref ID: ref16, Given Names: G N, Surname: Carneiro das, Year: 2021, Article Title: Anto\\u00b4nio CC. Dimensional reduction applied to the reliabilitybased robust design optimization of composite structures, Source: Compos Struct, Volume: 255, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref17, Given Names: H BD HS, Surname: An Youn Kim, Year: 2021, Article Title: Reliability-based design optimization of laminated composite structures under delamination and material property uncertainties, Source: Int J Mech Sci, Volume: 106561, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref18, Given Names: Z LB AG ZP T W. Lightweight, Surname: Li Duan Cheng Yao Chen Yao, Year: 2019, Article Title: and crashworthiness design of an electric vehicle using a six-sigma robust design optimization method, Source: Eng Optim, Volume: 1521396, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref19, Given Names: H SH, Surname: Gholinezhad Torabi, Year: 2021, Article Title: Reliability-based multidisciplinary design optimization of an underwater vehicle including cost analysis, Source: J Mar Sci Technol, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref20, Given Names: KH GJ, Surname: Lee Park, Year: 2001, Article Title: Robust optimization considering tolerances of design variables, Source: Comput Struct, Volume: 79, Issue: 00, First Page: 77, Last Page: 86\",\n    \"Ref ID: ref21, Given Names: TV, Surname: Anderson, Year: 2012, Article Title: Propagating skewness and kurtosis through engineering models for low-cost, meaningful, nondeterministic design, Source: J Mech Des Trans ASME, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref22, Given Names: Q Y SK P X J, Surname: Zhou Wang Choi Jiang Shao Hu Shu L, Year: 2018, Article Title: A robust optimization approach based on multi-fidelity metamodel, Source: Struct Multidiscip Optim, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref23, Given Names: GG S., Surname: Wang Shan, Year: 2007, Article Title: Review of metamodeling techniques in support of engineering design optimization, Source: J Mech Des Trans ASME, Volume: 129, Issue: None, First Page: 370, Last Page: 80\",\n    \"Ref ID: ref24, Given Names: T S R., Surname: Chatterjee Chakraborty Chowdhury, Year: 2019, Article Title: A critical review of surrogate assisted robust design optimization, Source: Arch Comput Methods Eng, Volume: 26, Issue: None, First Page: 245, Last Page: 74\",\n    \"Ref ID: ref25, Given Names: T MI S R., Surname: Chatterjee Friswell Adhikari Chowdhury, Year: 2021, Article Title: A global two-layer metamodel for response statistics in robust design optimization, Source: Eng Optim, Volume: 1861262, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref26, Given Names: X X W J G., Surname: Guo Zhao Zhang Yan Sun, Year: 2015, Article Title: Multi-scale robust design and optimization considering load uncertainties, Source: Comput Methods Appl Mech Eng, Volume: 283, Issue: None, First Page: 994, Last Page: 1009\",\n    \"Ref ID: ref27, Given Names: DJ HA M, Surname: Jerez Jensen Beer, Year: 2022, Article Title: Reliability-based design optimization of structural systems under stochastic excitation: an overview, Source: Mech Syst Signal Process, Volume: 108397, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref28, Given Names: W L M., Surname: Li Gao Xiao, Year: 2020, Article Title: Multidisciplinary robust design optimization under parameter and model uncertainties, Source: Eng Optim, Volume: 52, Issue: None, First Page: 426, Last Page: 45\",\n    \"Ref ID: ref29, Given Names: HG B., Surname: Beyer Sendhoff Robust, Year: 2007, Article Title: - a comprehensive survey, Source: Comput Methods Appl Mech Eng, Volume: 196, Issue: None, First Page: 3190, Last Page: 218\",\n    \"Ref ID: ref30, Given Names: R S SMB, Surname: Motta de Afonso, Year: 2016, Article Title: An efficient procedure for structural reliability-based robust design optimization, Source: Struct Multidiscip Optim, Volume: 54, Issue: None, First Page: 511, Last Page: 30\",\n    \"Ref ID: ref31, Given Names: AR, Surname: Yildiz, Year: 2013, Article Title: Comparison of evolutionary-based optimization algorithms for structural design optimization, Source: Eng Appl Artif Intell, Volume: 26, Issue: None, First Page: 327, Last Page: 33\",\n    \"Ref ID: ref32, Given Names: AT WJDS, Surname: Beck Gomes, Year: 2012, Article Title: A comparison of deterministic, reliability-based and riskbased structural optimization under uncertainty, Source: Probab Eng Mech, Volume: 28, Issue: None, First Page: 18, Last Page: 29\",\n    \"Ref ID: ref33, Given Names: E. G. Y. I. P. S.S., Surname: Acar Bayrak Jung Lee Ramu Ravichandran, Year: 2021, Article Title: Modeling, analysis, and optimization under uncertainties: a review, (, Source: None, Volume: 10, Issue: None, First Page: 021, Last Page: None\",\n    \"Ref ID: ref34, Given Names: A D W., Surname: Georghiou Kuhn Wiesemann, Year: 2019, Article Title: The decision rule approach to optimization under uncertainty: methodology and applications, Source: Comput Manag Sci, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref35, Given Names: O P R, Surname: Braydi Lafon Younes, Year: 2019, Article Title: Study of uncertainties and objective function modeling effects on probabilistic optimization results, Source: ASCE ASME J Risk Uncertain Eng Syst Part B Mech Eng, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref36, Given Names: WS SH, Surname: Liu Cheung, Year: 2017, Article Title: Reliability based design optimization with approximate failure probability function in partitioned design space, Source: Reliab Eng Syst Saf, Volume: 167, Issue: None, First Page: 602, Last Page: 11\",\n    \"Ref ID: ref37, Given Names: A S, Surname: Chiralaksanakul Mahadevan, Year: 2005, Article Title: First-order approximation methods in reliabilitybased design optimization, Source: J Mech Des Trans ASME, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref38, Given Names: I Z, Surname: Doltsinis Kang, Year: 2005, Article Title: Robust design of non-linear structures using optimization methods, Source: Comput Methods Appl Mech Eng, Volume: 194, Issue: None, First Page: 1779, Last Page: 95\",\n    \"Ref ID: ref39, Given Names: AA JL, Surname: Taflanidis Beck, Year: 2008, Article Title: Stochastic Subset Optimization for optimal reliability problems, Source: Probab Eng Mech, Volume: 12, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref40, Given Names: SK JL, Surname: Au Beck, Year: 2001, Article Title: Estimation of small failure probabilities in high dimensions by subset simulation, Source: Probab Eng Mech, Volume: 16, Issue: 01, First Page: 263, Last Page: 77\",\n    \"Ref ID: ref41, Given Names: GF AA, Surname: Jia Taflanidis, Year: 2013, Article Title: Non-parametric stochastic subset optimization for optimalreliability design problems, Source: Comput Struct, Volume: 126, Issue: None, First Page: 86, Last Page: 99\",\n    \"Ref ID: ref42, Given Names: AA, Surname: Taflanidis, Year: 2012, Article Title: Stochastic subset optimization incorporating moving least squares response surface methodologies for stochastic sampling, Source: Adv Eng Softw, Volume: 44, Issue: None, First Page: 3, Last Page: 14\",\n    \"Ref ID: ref43, Given Names: MA, Surname: Khalid, Year: 2023, Article Title: Framework for robust design optimization of tuned mass dampers by stochastic subset optimization, Source: Int J Struct Stab Dyn, Volume: 23, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref44, Given Names: SK, Surname: Au, Year: 2005, Article Title: Reliability-based design sensitivity by efficient simulation, Source: Comput Struct, Volume: 83, Issue: None, First Page: 1048, Last Page: 61\",\n    \"Ref ID: ref45, Given Names: AA JL, Surname: Taflanidis Beck, Year: 2008, Article Title: An efficient framework for optimal robust stochastic system design using stochastic simulation, Source: Comput Methods Appl Mech Eng, Volume: 03, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref46, Given Names: CP G, Surname: Robert Casella, Year: 2004, Article Title: Monte carlo statistical methods, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref47, Given Names: HS, Surname: Li, Year: 2011, Article Title: Subset simulation for unconstrained global optimization, Source: Appl Math Model, Volume: 35, Issue: None, First Page: 5108, Last Page: 20\",\n    \"Ref ID: ref48, Given Names: MA S V., Surname: Khalid Bansal Ramamohan, Year: 2022, Article Title: An augmented formulation for robust design optimization of structures using stochastic simulation method, Source: Res Eng Des, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref49, Given Names: AA JL, Surname: Taflanidis Beck, Year: 2008, Article Title: An efficient framework for optimal robust stochastic system design using stochastic simulation, Source: Comput Methods Appl Mech Eng, Volume: 198, Issue: None, First Page: 88, Last Page: 101\",\n    \"Ref ID: ref50, Given Names: EC A, Surname: Kandemir Mortazavi, Year: 2022, Article Title: Optimization of seismic base isolation system using a fuzzy reinforced swarm intelligence, Source: Adv Eng Softw, Volume: 174, Issue: None, First Page: 103323, Last Page: None\",\n    \"Ref ID: ref51, Given Names: S., Surname: Rebay, Year: 1993, Article Title: Efficient unstructured mesh generation by means of delaunay triangulation and Bowyer-Watson algorithm, Source: J Comput Phys, Volume: 106, Issue: None, First Page: 125, Last Page: 38\",\n    \"Ref ID: ref52, Given Names: N, Surname: Wade Graham-Brady L, Year: 2023, Article Title: Estimating microstructural feature distributions from image data using a Bayesian framework, Source: J Microsc, Volume: None, Issue: None, First Page: 1, Last Page: 16\",\n    \"Ref ID: ref53, Given Names: X L Y B. Exact, Surname: Duan Li Ge Liu, Year: 2023, Article Title: Voronoi diagram for topographic spatial analysis, Source: GIScience Remote Sens, Volume: 60, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref54, Given Names: None, Surname: None, Year: 2021, Article Title: MATLAB and parallel computing toolbox release, Source: Natick, Volume: None, Issue: None, First Page: None, Last Page: None\"\n  ]\n}",
    "{\n  \"Article_ID\": \"3608643d9712472a983fa125e41102d8\",\n  \"DocumentTitle\": \"Numerical computing in engineering mathematics\",\n  \"Auteurs\": \"Firuz Kamalov, Ho-Hon Leung, \",\n  \"Institutions\": \"Department of Electrical Engineering, Canadian University Dubai, Department of Mathematical Sciences, United Arab Emirates University, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"-\",\n      \"paragraphs\": [\n        \"I. INTRODUCTION\",\n        \"The 4th Industrial Revolution has had a dramatic impact\\n1 on the engineering profession. The modern technologies such\\nv as artificial intelligence, the internet of things, and advanced\\n8 robotics have altered engineering systems and processes.\\nTo9\\n6 day\\u2019s engineers are expected to be able to leverage these\\n4 resources to produce their products. To meet the new\\nprofes.0 sional requirements, engineering educational institutions have\\n7 revised their curricula. The changes in the curricula include\\n0 both updating the existing programs as well as\\nintroduc2 ing completely new programs. Given the rapid technological\\n:2 progress, universities and colleges around the world are\\nconiv tsiingunoifiucsalyntapdraopgtirnegsstion tmheodeverenr-izcihnagngthinegenegnivnieroenrimngenctu.rWrichuilluema\\nrXhas been achieved, there still remains room for improvement.\\na Catalyzed by the exponential increase in computational\\npower and interconnectedness, the modern industrial\\nrevolution has reshaped the skills and competencies required of the\\nengineers. The changes in engineering curricula in response to\\nIndustry 4.0 have been threefold: i) modernizing the existing\\nprograms, ii) introduction of new programs, and iii) revising\\nthe pedagogical approach. Modernizing the existing programs\\ninvolves introduction of new courses in the study plan related\\nto emerging technologies. In addition, existing courses can be\\nupdated with new content. Fresh new programs in emerging\\n\\u00a9 2022 IEEE. Personal use of this material is permitted. Permission from\\nIEEE must be obtained for all other uses, in any current or future media,\\nincluding reprinting/republishing this material for advertising or promotional\\npurposes, creating new collective works, for resale or redistribution to servers\\nor lists, or reuse of any copyrighted component of this work in other works\\ntechnologies are also introduced by universities and colleges.\\nMany institutions now offer degrees in artificial intelligence\\nand mechatronics which were not there 20 years ago. Finally,\\nuniversities have revised their approaches to course delivery.\\nStudent-centered learning, project-based learning, and applied\\nlearning have become popular in the new engineering\\neducational paradigm.\",\n        \"While significant effort has been made to revise the core\\nengineering courses, the auxiliary courses in mathematics and\\nsciences received little consideration. The mathematics and\\nsciences courses play a key role in the engineering curriculum.\\nGiven their importance, the curriculum updates must also be\\nextended to the auxiliary courses. By implementing a\\ncomprehensive update of the engineering curriculum that includes\\nboth the core and auxiliary courses, a more effective outcome\\ncan be achieved.\",\n        \"The goal of this paper is to propose a modernized\\nengineering mathematics curriculum in line with the broader efforts to\\nupdate engineering education to adapt to Industry 4.0. The key\\nfeature of the new curriculum is the introduction numerical\\ncomputing in the existing mathematics courses. The latest\\nindustrial revolution has been driven largely by the dramatic\\nincrease in computational power. Therefore, today\\u2019s engineers\\nmust be well-equipped to leverage the computing power in\\ntheir work.\",\n        \"Since mathematics courses are usually taken at the\\nbeginning of the study plan, it offers a natural avenue for introducing\\nnumerical computing to students. Furthermore, many problems\\nin mathematics can be solved numerically making it natural\\nto integrate numerical computing in mathematics courses.\\nBy studying numerical computing in mathematics courses,\\nstudents will acquire the necessary theoretical and practical\\nskills to apply in their downstream, specialized engineering\\ncourses.\",\n        \"This paper is structured as follows. Section 2 provides an\\noverview of the existing efforts to update the engineering\\ncurriculum in response to Industry 4.0. Section 3 discusses\\nthe current approaches to integrate scientific computing in\\nmathematics courses. In Section 4, we present our proposal\\nfor modernizing the mathematics curriculum to integrate\\nnumerical computing. Section 5 concludes the paper with final\\nremarks.\"\n      ]\n    },\n    {\n      \"title\": \"II. ENGINEERING EDUCATION AND INDUSTRY 4.0\",\n      \"paragraphs\": [\n        \"Engineering departments in colleges and universities have\\nmade significant changes in their curricula in response to\\nthe new environment created by the recent, rapid advances\\nin technology. In particular, the existing programs have been\\nupdated to include courses that target emerging technologies.\\nCompletely new programs related to AI and mechatronics\\nhave also been adopted by universities. Innovations in the\\nfield of engineering education continue to take place with new\\ndevelopments on the horizon.\",\n        \"\\n        There exist several studies investigating the modern\\nengineering curricula and evaluating their effectiveness. It is\\nargued in \\n        \",\n        \"\\n        Digital and online learning have become an important part\\nof modern education including in the field of engineering.\\nInformation technologies play a vital role in delivering digital\\nlearning to students. Colleges and universities have made\\nsignificant investments to improve their information and\\ncommunication technology (ICT) capacities \\n        \",\n        \"\\n        In response to the needs of Industry 4.0, some\\nuniversities have adopted the framework of Education 4.0 \\n        \",\n        \"\\n        In addition to technological progress, socio-cultural shifts\\nmust be taken into account in revising engineering curriculum.\\nThe new generation of students has its unique worldview\\nwhich needs to be considered by the educators. In particular,\\nthe new generation is significantly affected by mobile devices\\nand digital media. Educational content must be tailored to\\nthe new student preferences to achieve effective learning\\noutcomes \\n        \",\n        \"Many universities have also introduced nontechnical updates\\nto their engineering curriculum. The most significant\\nnontechnical update has been the introduction of entrepreneurship\\ncourses and experiences for students. A lot of attention has\\nrecently been given to equipping students with entrepreneurial\\nskills. Students learn about entrepreneurship in their courses\\nas well as through university incubators.\"\n      ]\n    },\n    {\n      \"title\": \"III. UNIVERSITY MATHEMATICS CURRICULUM\",\n      \"paragraphs\": [\n        \"The mathematics curriculum changed very little in the\\ncurrent century. It remains a largely analytic domain, where\\nsolutions are mainly obtained manually. The current mathematics\\ncurriculum emphasizes theory over practical approaches. For\\ninstance, when finding the extreme values of a function,\\nderivative-based approach is preferred over the gradient\\ndecent. There are two key reasons for why analytical approaches\\nare favored over numerical methods. First, analytical solutions\\nare reliable and elegant. An analytical solution is guaranteed\\nto be exact. Second, mathematics courses are usually taught\\nby pure mathematicians who have an inherent preference\\nfor analytical solutions. Pure mathematics which is based on\\ntheorem proving is not amenable to numerical methods.\",\n        \"\\n        Despite the popularity of analytical approaches to problem\\nsolving in mathematics, there has been a growing push to\\nintegrate computer algebra systems as part of the learning process.\\nComputer algebra systems such as Matlab and Mathematica\\nare now routinely used in many mathematics courses. The\\nstudy by Cretchley et al. \\n        \",\n        \"\\n        The influence of computer technology on students\\u2019\\nacademic performance and learning experience has been\\ninvestigated by several authors. Abdul Majid et al. \\n        \",\n        \"\\n        In a separate study by Brake \\n        \",\n        \"\\n        Although the majority of the studies were based on the\\nuse of Matlab, several other studies considered alternative\\nmathematics software packages. The study by Kilicman et al.\\n\\n        \",\n        \"\\n        In a recent study by Mezhennaya and Pugachev \\n        \"\n      ]\n    },\n    {\n      \"title\": \"IV. NUMERICAL COMPUTING IN MATHEMATICS\",\n      \"paragraphs\": [\n        \"CURRICULUM\",\n        \"Mathematics lies at the foundation of science and\\nengineering. The importance of mathematics courses in engineering\\neducation cannot be underestimated. These courses equip\\nstudents with the fundamental skills and knowledge to study the\\nmore specialized engineering courses. Thus, student success in\\nengineering studies depends directly on the mathematics and\\nsciences courses. Given the significance of the mathematics\\ncourses in the engineering curriculum, it is paramount to\\nensure their currency with respect to the Industry 4.0.\",\n        \"The technological advances over the last decade have\\ncreated demand for more computationally proficient experts.\\nTo meet this demand, numerical computing must become a\\ncore part of engineering studies. Mathematics courses offer\\na natural and convenient avenue for introducing numerical\\ncomputing to engineering students. There are two main\\nfactors that make mathematics courses particularly amenable\\nto numerical computing. First, in many cases mathematical\\nproblems have numerical solutions. For instance, finding the\\nroot of a polynomial or the minimum value of a function\\ncan be done numerically. Therefore, it is both logical and\\nappropriate to apply numerical computing to mathematical\\nproblems. Second, mathematics courses are usually taken\\nat the beginning of the study plan. Thus, students become\\nacquainted with numerical computing at an early stage. The\\ncomputing and programming skills acquired in this manner\\nwill have a positive effect in the more advanced, downstream\\nengineering courses.\",\n        \"The key idea for the proposed curriculum update is the\\naddition of computing tutorials (labs) to mathematics courses.\\nIn particular, we propose adding weekly computing tutorials\\n(labs) related to the main lecture material. For instance, in the\\nweek in which students cover finding the extreme values of a\\nfunction, there will be a computing tutorial where students\\nlearn and implement the gradient descent algorithm. The\\nsuggested length of each tutorial is 1 hour. It is enough\\ntime to implement most of the numerical algorithms at the\\nundergraduate level. At the same time, 1 extra hour per week\\nwill not overburden the students.\",\n        \"The exact details of numerical computing content is left\\nfor individual universities and instructors. Depending on the\\nsyllabus and course learning outcomes, the numerical\\ncomputing labs will be different for each university and instructor.\\nNevertheless, the general ideas will be broadly similar across\\ndifferent curricula. To illustrate the proposed numerical\\ncomputing content, we will focus on the three main concept in\\ncalculus: limits, derivatives, and integrals.\",\n        \"Limit is a fundamental concept in calculus. Students are\\nusually taught to calculate limits using analytical approaches.\\nAlthough analytical approaches work well, there is no single\\nuniversal rule for calculating limits. On the other hand, in most\\ncases, limits can be calculated numerically using essentially\\nthe same approach. To illustrate, suppose we want to calculate\\nlfim(ax+\\u21921a0+\\u2212fk()x.)A. sTkheinncwreeacsaens,laoo+p1f0o\\u2212r kk a=pp0rotaochneasnad, scoalcfu(laa+te\\n10\\u2212k) will, in most cases, approach the limit value. We can\\ndeduce the limit by observing the values of f (a + 10\\u2212k) or\\ndetermine that the limit does not exist if there is no pattern of\\nconvergence. The value of n can be chosen manually or using\\na stopping criterion. For instance, the algorithm may continue\\nto iterate until the difference between consecutive values of\\nf (a + 10\\u2212k) is below a certain threshold. The value of the\\nlimit can also be deduced automatically based on the values\\nof f (a + 10\\u2212k) using various heuristics.\",\n        \"Another common limit problem is limx\\u2192\\u221e f (x). In this\\ncase, we can loop for k = 0 to n and calculate f (10k). As k\\nincreases, 10k approaches \\u221e, so f (10k) will, in most cases,\\napproach the limit value. Then the limit can be determined\\nbased on the values of f (10k). Various extensions and\\ncustomizations of this basic approach can be made. For instance,\\nto avoid issues with periodic functions f (10k + \\u01ebk), where \\u01ebk\\nare randomly generated, can be used. Other values than 10k\\ncan also be used as long as the sequence approaches infinity.\\nA degree of automization can be introduced using different\\nheuristics.\\nright Riemann sum is defined as \\u03b4 Pkn=1 f (xk). The Riemann\\nsum provides an approximation of the integral. Indeed,\\n(3)\\n(4)\\nZ b\\na\",\n        \"n\\nf (x) dx = nl\\u2192im\\u221e \\u03b4 X f (xk ).\",\n        \"k=1\\nThe Riemann sums can be quickly calculated on a computer\\nproviding a simple, yet effective approach to calculating\\nintegrals numerically. Other popular integral approximation\\nmethods include the trapezoid rule and the Simpson\\u2019s rule.\",\n        \"The above discussion about numerical methods for\\ncalculating limits, derivatives, and integrals is easily extended\\nto multivariate calculus. For instance, to find the partial\\nderivative fx(a, b), we can loop for k = 0 to n and calculate\\nf (a+10\\u2212k,b)\\u2212f (a,b) . Many problems related to sequences and\\n10\\u2212k\\nseries can similarly be solved using numerical techniques. In\\nparticular, the convergence of a series can be deduced from its\\npartial sums. By calculating the partial sums on the computer\\nand observing the results, we can intuit the nature of the series.\",\n        \"Vectorization is an important aspect of numerical\\ncomputing. Since the modern computer chips are optimized for\\nmatrix multiplication, it is more efficient to employ vector\\noperations. In particular, some algorithms based on for-loops\\ncan be converted into vector operations resulting in higher\\nefficiency and speed. For instance, the Riemann sum can be\\ncalculated with a single vector operation:\",\n        \"Derivative is arguably the most important concept in\\ncalculus. There exist several rules such as the power rule, the\\nproduct rule, the chain rule, and others to find the derivative\\nof a function by hand. However, manual differentiation may\\nbe cumbersome when dealing with complex function. On the\\nother hand, calculating the derivative at a point numerically is\\nrelatively straightforward. To illustrate, suppose that we want\\nto calculate f \\u2032(a). Recall that\\n(1)\\nf \\u2032(a) = lim\\nx\\u2192a\\nf (a + h) \\u2212 f (a)\\nh\\n.\",\n        \"Therefore, to calculate f \\u2032(a) numerically we use the same\\napproach as with the limits. In particular, we can loop for k =\\n0 to n and calculate f (a+10\\u2212k)\\u2212f (a) . Then the limit, and by\\n10\\u2212k\\nextension the derivative, can be deduced (approximated) based\\non the calculated values. The accuracy of the approximation\\ndepends in large part on the value of n.\",\n        \"One of the most important applications of the derivative is\\nfinding the extreme values of a function. Traditionally, this is\\ndone by first finding the critical points of the function and\\nthen applying the second derivative test. However, finding the\\ncritical points is not always possible, so numerical approaches\\ncan be used in such cases. The most popular numerical\\napproach for finding the extreme values is based on the\\ngradient descent (ascent) algorithm. In gradient descent, the\\noptimal value of x is iteratively updated based on the gradient.\\nIn particular, for k = 0 to n, the updated optimal value of x\\nis given by\\nxk+1 = xk \\u2212 \\u03b1\\u2207f (xk ),\\n(2)\\nwhere \\u2207f (x) is the gradient and \\u03b1 is the step size. In the\\ncase of a single-variable function, the gradient equals simply\\nto the derivative \\u2207f (x) = f \\u2032(x). The step size \\u03b1 can be\\neither fixed or dynamic. While a large value of \\u03b1 accelerates\\nthe convergence at the beginning, it may hurt the convergence\\nin the region near the optimal value.\",\n        \"There exist several extensions of the basic gradient descent\\nalgorithm. One such extension is gradient descent with\\nmomentum which uses the second derivative to anticipate the\\nlocation of the next optimal point and thus accelerates the\\nconvergence.\",\n        \"Integration is an important concept in engineering\\nmathematics. Although there exist a number of rules for finding the\\nintegral, it is significantly more challenging than\\ndifferentiation. Moreover, in many cases, the indefinite integral does not\\neven exist. Therefore, numerical approaches are particularly\\nuseful for integration.\",\n        \"To illustrate the application of numerical integration,\\nsuppose that we want to calculate Rab f (x) dx. There exist several\\nnumerical methods for calculating the integral. One simple\\nmethod is based on the Riemann sums. The interval a, b is\\ndivided into n equal subintervals with endpoints x0, x1, ..., xn.\\nLet \\u03b4 = xk+1 \\u2212 xk be the length of each subinterval. Then the\\nn\\n\\u03b4 X f (xk) = \\u03b4S f (x) ,\",\n        \"k=1\\nwhere x = x1, ..., xn is the vector of endpoints, f (x) is\\na vectorized function operation, and S is the vector\\nfunction which returns the sum of all the coordinates. Similarly,\\nlimit calculations can be vectorized and made more efficient.\\nVectorization is also useful in multi-variate calculus, where\\noperations can be performed on a vector of variables.\",\n        \"The choice of the programming language for numerical\\ncomputing requires careful consideration. There are several\\nsuitable candidates for this purpose including Python, Java,\\nC++, Matlab, and others. Based on our experience with\\ndifferent programming languages, we recommend the use of\\nPython. Python is currently the most popular programming\\nlanguage on the planet. It has a simple and intuitive syntax\\nmaking it easy to learn and apply. Python has libraries to\\nfit any purpose including an extensive collection of libraries\\nrelated to numerical computing. The basic Python libraries\\nrelated to computing are NumPy, SciPy, and SymPy. More\\nadvanced packages such as OR-Tools are also available for\\noptimization tasks. Since Python is a universal programming\\nlanguage, it can be used for almost any task. Thus, students\\nwho learn Python in their mathematics courses can employ\\nit in their other courses. In addition, numerical computing\\nimplemented in Python can be connected to other applications.\",\n        \"V. CONCLUSION\",\n        \"\\n          Although the classical approach to teaching mathematics\\nis still relevant for certain student cohorts, it is outdated for\\nengineering students. Modern engineering is increasingly\\nreliant on computing \\n          \",\n        \"Given the efficiency of computer-based calculations,\\nnumerical computing provides a convenient approach to problem\\nsolving in engineering mathematics. It can be integrated into\\nthe existing curriculum with little hassle and cost. In this paper,\\nwe proposed a framework for integrating numerical computing\\ninto the existing mathematics curriculum. We demonstrated\\nhow numerical approaches can be used some of the most\\ncommon problems encountered in calculus. The proposed\\nframework can be customized by individual universities to fit\\ntheir special needs.\"\n      ]\n    },\n    {\n      \"title\": \"A. Limits\",\n      \"paragraphs\": [\n        \"Limit is a fundamental concept in calculus. Students are\\nusually taught to calculate limits using analytical approaches.\\nAlthough analytical approaches work well, there is no single\\nuniversal rule for calculating limits. On the other hand, in most\\ncases, limits can be calculated numerically using essentially\\nthe same approach. To illustrate, suppose we want to calculate\\nlfim(ax+\\u21921a0+\\u2212fk()x.)A. sTkheinncwreeacsaens,laoo+p1f0o\\u2212r kk a=pp0rotaochneasnad, scoalcfu(laa+te\\n10\\u2212k) will, in most cases, approach the limit value. We can\\ndeduce the limit by observing the values of f (a + 10\\u2212k) or\\ndetermine that the limit does not exist if there is no pattern of\\nconvergence. The value of n can be chosen manually or using\\na stopping criterion. For instance, the algorithm may continue\\nto iterate until the difference between consecutive values of\\nf (a + 10\\u2212k) is below a certain threshold. The value of the\\nlimit can also be deduced automatically based on the values\\nof f (a + 10\\u2212k) using various heuristics.\",\n        \"Another common limit problem is limx\\u2192\\u221e f (x). In this\\ncase, we can loop for k = 0 to n and calculate f (10k). As k\\nincreases, 10k approaches \\u221e, so f (10k) will, in most cases,\\napproach the limit value. Then the limit can be determined\\nbased on the values of f (10k). Various extensions and\\ncustomizations of this basic approach can be made. For instance,\\nto avoid issues with periodic functions f (10k + \\u01ebk), where \\u01ebk\\nare randomly generated, can be used. Other values than 10k\\ncan also be used as long as the sequence approaches infinity.\\nA degree of automization can be introduced using different\\nheuristics.\\nright Riemann sum is defined as \\u03b4 Pkn=1 f (xk). The Riemann\\nsum provides an approximation of the integral. Indeed,\\n(3)\\n(4)\\nZ b\\na\",\n        \"n\\nf (x) dx = nl\\u2192im\\u221e \\u03b4 X f (xk ).\",\n        \"k=1\\nThe Riemann sums can be quickly calculated on a computer\\nproviding a simple, yet effective approach to calculating\\nintegrals numerically. Other popular integral approximation\\nmethods include the trapezoid rule and the Simpson\\u2019s rule.\"\n      ]\n    },\n    {\n      \"title\": \"D. Additional considerations\",\n      \"paragraphs\": [\n        \"The above discussion about numerical methods for\\ncalculating limits, derivatives, and integrals is easily extended\\nto multivariate calculus. For instance, to find the partial\\nderivative fx(a, b), we can loop for k = 0 to n and calculate\\nf (a+10\\u2212k,b)\\u2212f (a,b) . Many problems related to sequences and\\n10\\u2212k\\nseries can similarly be solved using numerical techniques. In\\nparticular, the convergence of a series can be deduced from its\\npartial sums. By calculating the partial sums on the computer\\nand observing the results, we can intuit the nature of the series.\",\n        \"Vectorization is an important aspect of numerical\\ncomputing. Since the modern computer chips are optimized for\\nmatrix multiplication, it is more efficient to employ vector\\noperations. In particular, some algorithms based on for-loops\\ncan be converted into vector operations resulting in higher\\nefficiency and speed. For instance, the Riemann sum can be\\ncalculated with a single vector operation:\"\n      ]\n    },\n    {\n      \"title\": \"B. Derivative\",\n      \"paragraphs\": [\n        \"Derivative is arguably the most important concept in\\ncalculus. There exist several rules such as the power rule, the\\nproduct rule, the chain rule, and others to find the derivative\\nof a function by hand. However, manual differentiation may\\nbe cumbersome when dealing with complex function. On the\\nother hand, calculating the derivative at a point numerically is\\nrelatively straightforward. To illustrate, suppose that we want\\nto calculate f \\u2032(a). Recall that\\n(1)\\nf \\u2032(a) = lim\\nx\\u2192a\\nf (a + h) \\u2212 f (a)\\nh\\n.\",\n        \"Therefore, to calculate f \\u2032(a) numerically we use the same\\napproach as with the limits. In particular, we can loop for k =\\n0 to n and calculate f (a+10\\u2212k)\\u2212f (a) . Then the limit, and by\\n10\\u2212k\\nextension the derivative, can be deduced (approximated) based\\non the calculated values. The accuracy of the approximation\\ndepends in large part on the value of n.\",\n        \"One of the most important applications of the derivative is\\nfinding the extreme values of a function. Traditionally, this is\\ndone by first finding the critical points of the function and\\nthen applying the second derivative test. However, finding the\\ncritical points is not always possible, so numerical approaches\\ncan be used in such cases. The most popular numerical\\napproach for finding the extreme values is based on the\\ngradient descent (ascent) algorithm. In gradient descent, the\\noptimal value of x is iteratively updated based on the gradient.\\nIn particular, for k = 0 to n, the updated optimal value of x\\nis given by\\nxk+1 = xk \\u2212 \\u03b1\\u2207f (xk ),\\n(2)\\nwhere \\u2207f (x) is the gradient and \\u03b1 is the step size. In the\\ncase of a single-variable function, the gradient equals simply\\nto the derivative \\u2207f (x) = f \\u2032(x). The step size \\u03b1 can be\\neither fixed or dynamic. While a large value of \\u03b1 accelerates\\nthe convergence at the beginning, it may hurt the convergence\\nin the region near the optimal value.\",\n        \"There exist several extensions of the basic gradient descent\\nalgorithm. One such extension is gradient descent with\\nmomentum which uses the second derivative to anticipate the\\nlocation of the next optimal point and thus accelerates the\\nconvergence.\"\n      ]\n    },\n    {\n      \"title\": \"C. Integrals\",\n      \"paragraphs\": [\n        \"Integration is an important concept in engineering\\nmathematics. Although there exist a number of rules for finding the\\nintegral, it is significantly more challenging than\\ndifferentiation. Moreover, in many cases, the indefinite integral does not\\neven exist. Therefore, numerical approaches are particularly\\nuseful for integration.\",\n        \"To illustrate the application of numerical integration,\\nsuppose that we want to calculate Rab f (x) dx. There exist several\\nnumerical methods for calculating the integral. One simple\\nmethod is based on the Riemann sums. The interval a, b is\\ndivided into n equal subintervals with endpoints x0, x1, ..., xn.\\nLet \\u03b4 = xk+1 \\u2212 xk be the length of each subinterval. Then the\\nn\\n\\u03b4 X f (xk) = \\u03b4S f (x) ,\",\n        \"k=1\\nwhere x = x1, ..., xn is the vector of endpoints, f (x) is\\na vectorized function operation, and S is the vector\\nfunction which returns the sum of all the coordinates. Similarly,\\nlimit calculations can be vectorized and made more efficient.\\nVectorization is also useful in multi-variate calculus, where\\noperations can be performed on a vector of variables.\",\n        \"The choice of the programming language for numerical\\ncomputing requires careful consideration. There are several\\nsuitable candidates for this purpose including Python, Java,\\nC++, Matlab, and others. Based on our experience with\\ndifferent programming languages, we recommend the use of\\nPython. Python is currently the most popular programming\\nlanguage on the planet. It has a simple and intuitive syntax\\nmaking it easy to learn and apply. Python has libraries to\\nfit any purpose including an extensive collection of libraries\\nrelated to numerical computing. The basic Python libraries\\nrelated to computing are NumPy, SciPy, and SymPy. More\\nadvanced packages such as OR-Tools are also available for\\noptimization tasks. Since Python is a universal programming\\nlanguage, it can be used for almost any task. Thus, students\\nwho learn Python in their mathematics courses can employ\\nit in their other courses. In addition, numerical computing\\nimplemented in Python can be connected to other applications.\",\n        \"V. CONCLUSION\",\n        \"\\n          Although the classical approach to teaching mathematics\\nis still relevant for certain student cohorts, it is outdated for\\nengineering students. Modern engineering is increasingly\\nreliant on computing \\n          \",\n        \"Given the efficiency of computer-based calculations,\\nnumerical computing provides a convenient approach to problem\\nsolving in engineering mathematics. It can be integrated into\\nthe existing curriculum with little hassle and cost. In this paper,\\nwe proposed a framework for integrating numerical computing\\ninto the existing mathematics curriculum. We demonstrated\\nhow numerical approaches can be used some of the most\\ncommon problems encountered in calculus. The proposed\\nframework can be customized by individual universities to fit\\ntheir special needs.\"\n      ]\n    }\n  ],\n  \"references\": [\n    \"Ref ID: ref1, Given Names: Abdul M. Z. W., Surname: Majid Huneiti Balachandran Al-Naafa M. A. Villach, Year: 2012, Article Title: A study of the effects of using MATLAB as a pedagogical tool for engineering mathematics students, Source: 15th International Conference on Interactive Collaborative Learning (ICL), Volume: 10, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref2, Given Names: Abdul M. Z. A. W., Surname: Majid Huneiti Balachandran Balarabe Y., Year: 2013, Article Title: MATLAB as a teaching and learning tool for Mathematics: A literature review, Source: International Journal of Arts and Sciences, Volume: 6, Issue: 3, First Page: 23, Last Page: 44\",\n    \"Ref ID: ref3, Given Names: M. L., Surname: Brake, Year: 2007, Article Title: MATLAB as a Tool to Increase the Math SelfConfidence and the Math Ability of First-Year Engineering Technology Students, Source: The Scholarship of Teaching and Learning at EMU:, Volume: 1, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref4, Given Names: J. A. X., Surname: Chen Kolmos Du, Year: 2021, Article Title: Forms of implementation and challenges of PBL in engineering education: a review of literature, Source: European Journal of Engineering Education, Volume: 46, Issue: 1, First Page: 90, Last Page: 115\",\n    \"Ref ID: ref5, Given Names: P. C. N. G., Surname: Cretchley Harman Ellerton Fogarty, Year: 2000, Article Title: MATLAB in Early Undergraduate Mathematics: An investigation into the Effects of Scientific Software on Learning, Source: Mathematics Education Research Journal, Volume: 12, Issue: 3, First Page: 219, Last Page: 233\",\n    \"Ref ID: ref6, Given Names: R. G. A., Surname: Hadgraft Kolmos, Year: 2020, Article Title: Emerging learning environments in engineering education, Source: Australasian Journal of Engineering Education, Volume: 25, Issue: 1, First Page: 3, Last Page: 16\",\n    \"Ref ID: ref7, Given Names: M. R., Surname: Hernandez-de-Menendez Morales-Menendez, Year: 2019, Article Title: Technological innovations and practices in engineering education: a review, Source: International Journal on Interactive Design and Manufacturing (IJIDeM), Volume: 13, Issue: 2, First Page: 713, Last Page: 728\",\n    \"Ref ID: ref8, Given Names: Herna M. D. H., Surname: \\u00b4 Guevara A. V. Morales-Menendez R., Year: 2019, Article Title: Active learning in engineering education. A review of fundamentals, best practices and experiences, Source: International Journal on Interactive Design and Manufacturing (IJIDeM), Volume: 13, Issue: 3, First Page: 909, Last Page: 922\",\n    \"Ref ID: ref9, Given Names: F. S. R. O., Surname: Kamalov Moussa Zgheib Mashaal, Year: 2020, Article Title: Feature selection for intrusion detection systems, Source: In 2020 13th International Symposium on Computational Intelligence and Design (ISCID), Volume: None, Issue: None, First Page: 265, Last Page: 269\",\n    \"Ref ID: ref10, Given Names: F. H. D., Surname: Kamalov Sulieman Santandreu Calonge, Year: 2021, Article Title: Machine learning based approach to exam cheating detection, Source: Plos one, Volume: 16, Issue: 8, First Page: None, Last Page: None\",\n    \"Ref ID: ref11, Given Names: A. M. A. S. K., Surname: Kilicman Hassan Said Husain, Year: 2010, Article Title: Teaching and Learning using Mathematics Software 'The New Challenge', Source: Procedia Social and Behavioral Sciences, Volume: 8, Issue: None, First Page: 613, Last Page: 619\",\n    \"Ref ID: ref12, Given Names: M. M. R. K. L., Surname: Lorenz Strack Lueth Bolle M., Year: 2015, Article Title: Man and machine in industry 4.0: How will technology transform the industrial workforce through 2025, Source: None, Volume: 2, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref13, Given Names: L. E. H., Surname: Veado Moreira Figueiredo Costa, Year: 2018, Article Title: A systematic mapping study on game-related methods for software engineering education, Source: Information and software technology, Volume: 95, Issue: None, First Page: 201, Last Page: 218\",\n    \"Ref ID: ref14, Given Names: N. M. O. V., Surname: Mezhennaya Pugachev, Year: 2019, Article Title: On perception of computer algebra systems and microsoft excel by engineering students, Source: Problems on Education in the 21st Century, Volume: 77, Issue: 3, First Page: 379, Last Page: 395\",\n    \"Ref ID: ref15, Given Names: J. C. J. J. M. M. S. S. A., Surname: Miranda Navarrete Noguez Molina-Espinosa Navarro-Tuch Molina A., Year: 2021, Article Title: The core components of education 4.0 in higher education: Three case studies in engineering education, Source: Computers & Electrical Engineering, Volume: 93, Issue: None, First Page: 107278, Last Page: None\",\n    \"Ref ID: ref16, Given Names: K. R. S., Surname: Moore Frazier, Year: 2017, Article Title: Engineering education for generation Z, Source: American Journal of Engineering Education (AJEE), Volume: 8, Issue: 2, First Page: 111, Last Page: 126\",\n    \"Ref ID: ref17, Given Names: M. K. M., Surname: Ortiz-Rojas Chiluiza Valcke, Year: 2019, Article Title: Gamification through leaderboards: An empirical study in engineering education, Source: None, Volume: 27, Issue: 4, First Page: 777, Last Page: 788\",\n    \"Ref ID: ref18, Given Names: R. I. Education, Surname: Puhak Copyright (C) 2012 by Pearson, Year: 2011, Article Title: Teaching applied Calculus utilizing MATLAB, Source: Proceedings of the Twenty-Third Annual International Conference on Technology in Collegiate Mathematics, ISBN 0-321-68984-4, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref19, Given Names: R. A. R. H. R., Surname: Ramirez-Mendoza Morales-Menendez Iqbal ParraSaldivar, Year: 2018, Article Title: -proposal for a new Curricula, Source: Engineering Education 4, Volume: None, Issue: None, First Page: 1273, Last Page: 1282\",\n    \"Ref ID: ref20, Given Names: B. M. H. S. H. M. H. A., Surname: Salah Abidi Mian Krid Alkhalefah Abdo, Year: 2019, Article Title: Virtual reality-based engineering education to enhance manufacturing sustainability in industry 4.0, Source: None, Volume: 11, Issue: 5, First Page: 1477, Last Page: None\",\n    \"Ref ID: ref21, Given Names: T. L., Surname: Strayhorn, Year: 2006, Article Title: College in the information age: Gains associated with students' use of technology, Source: Journal of Interactive Online Learning, Volume: 5, Issue: 2, First Page: 143, Last Page: 155\",\n    \"Ref ID: ref22, Given Names: F. F. K., Surname: Thabtah Kamalov Rajab, Year: 2018, Article Title: A new computational intelligence approach to detect autistic features for autism screening, Source: International journal of medical informatics, Volume: 117, Issue: None, First Page: 112, Last Page: 124\",\n    \"Ref ID: ref23, Given Names: S. R. M., Surname: Van den Beemt MacLeod Van der Veen Klaassen Boon, Year: 2020, Article Title: Interdisciplinary engineering education: A review of vision, teaching, and support, Source: Journal of engineering education, Volume: 109, Issue: 3, First Page: 508, Last Page: 555\"\n  ]\n}",
    "{\n  \"Article_ID\": \"bf505d18460d46b0be0bd0a64d166485\",\n  \"DocumentTitle\": \"Large Language Model Augmented Narrative Driven Recommendations\",\n  \"Auteurs\": \"Sheshera Mysore, Andrew McCallum, Hamed Zamani, \",\n  \"Institutions\": \"University of Massachusetts Amherst, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"CCS CONCEPTS\",\n      \"paragraphs\": [\n        \"\\u2022 Information systems \\u2192 Recommender systems; Users and\\ninteractive retrieval; \\u2022 Computing methodologies \\u2192 Natural language\\ngeneration.\"\n      ]\n    },\n    {\n      \"title\": \"INTRODUCTION\",\n      \"paragraphs\": [\n        \"\\n        Recommender systems personalized to users are an important\\ncomponent of several industry-scale platforms \\n        \",\n        \"\\n        However, with the emergence of conversational interfaces for\\ninformation access tasks, support for complex NDR tasks is likely\\nto become necessary. In this context, recent work has noted an\\nincrease in complex and subjective natural language requests\\ncompared to more conventional search interfaces \\n        \",\n        \"Specifically, given a user\\u2019s interactions,  , with items and\\ntheir accompanying text documents (e.g., reviews, descriptions)\\n = { }=1, selected from a user-item interaction dataset I, we\\nprompt InstructGPT, a 175B parameter LLM, to author a synthetic\\nnarrative query  based on  (Figure 2). Since we expect the\\nquery  to be noisy and not fully representative of all the user\\nreviews,  is filtered to retain only a fraction of the reviews based\\non a language-model assigned likelihood of  given a user\\ndocument,  . Then, a pre-trained LM based retrieval model (110M\\nparameters) is fine-tuned for retrieval on the synthetic queries and\\nifltered reviews.\",\n        \"\\n        Our approach, which we refer to as Mint2, follows from the\\nobservation that while narrative queries and suggestions are often\\nmade in online discussion forums, and could serve as training data,\\nthe number of these posts and the diversity of domains for which\\nthey are available is significantly smaller than the size and diversity\\nof passively gathered user-item interaction datasets. E.g. while\\nBogers and Koolen \\n        \",\n        \"\\n        We empirically evaluate Mint in a publicly available test\\ncollection for point of interest recommendation: pointrec \\n        \"\n      ]\n    },\n    {\n      \"title\": \"RELATED WORK\",\n      \"paragraphs\": [\n        \"\\n        Data Augmentation for Information Access. A line of recent\\nwork has explored using language models to generate synthetic\\nqueries for data augmentation to train models for information\\nretrieval tasks \\n        \",\n        \"\\n          Besides creating queries for ad-hoc retrieval tasks, concurrent\\nwork of Leszczynski et al. \\n          \",\n        \"\\n          Finally, while our work explores data augmentation from\\nuseritem interactions for a retrieval-oriented NDR task, prior work has\\nalso explored data augmentation of the user-item graph for training\\ncollaborative filtering models. This work has often explored\\naugmentation to improve recommendation performance for minority\\n\\n          \",\n        \"\\n          Complex Queries in Information Access. With the advent\\nof performant models for text understanding, focus on complex\\nand interactive information access tasks has seen a resurgence\\n\\n          \",\n        \"\\n          Besides this, a range of work has explored more complex,\\nlongform, and interactive query formulations for information access;\\nthese resemble queries in NDR. Arguello et al. \\n          \"\n      ]\n    },\n    {\n      \"title\": \"3https://github.com/iesl/narrative-driven-rec-mint/\",\n      \"paragraphs\": [\n        \"\\n          Besides creating queries for ad-hoc retrieval tasks, concurrent\\nwork of Leszczynski et al. \\n          \",\n        \"\\n          Finally, while our work explores data augmentation from\\nuseritem interactions for a retrieval-oriented NDR task, prior work has\\nalso explored data augmentation of the user-item graph for training\\ncollaborative filtering models. This work has often explored\\naugmentation to improve recommendation performance for minority\\n\\n          \",\n        \"\\n          Complex Queries in Information Access. With the advent\\nof performant models for text understanding, focus on complex\\nand interactive information access tasks has seen a resurgence\\n\\n          \",\n        \"\\n          Besides this, a range of work has explored more complex,\\nlongform, and interactive query formulations for information access;\\nthese resemble queries in NDR. Arguello et al. \\n          \"\n      ]\n    },\n    {\n      \"title\": \"Proposed Method\",\n      \"paragraphs\": [\n        \"\\n        Our proposed method, Mint, for NDR, re-purposes a dataset of\\n\\nabundantly available user-item interactions, I = {(, { }=1)} into\\ntraining data for retrieval models by using LLMs as query\\ngeneration models to author narrative queries  : D = {(, { }=1)}.\\nThen, retrieval models are trained on the synthetic dataset D\\n(Figure 3).\\n3.2.1 Narrative Queries from LLMs. To author a narrative query \\nfor a user in I, we make use of the 175B parameter InstructGPT4\\nmodel as our query generation model QGen. We include the text\\nof interacted items { }=1 in the prompt for QGen, and instruct it\\nto author a narrative query (Figure 2). To improve the coherence\\nof generated queries and obtain correctly formatted outputs, we\\nmanually author narrative queries for 3 topically diverse users\\nbased on their interacted items and include it in the prompt for\\nQGen. The same three few shot examples are used for the whole\\ndataset I, and the three users were chosen from I. Generating\\nnarrative queries based on user interactions may also be considered\\na form of multi-document summarization for generating a natural\\nlanguage user profile \\n        \",\n        \"q and d . Embeddings are obtained by averaging token embeddings\\nfrom the final layer of MPNet, and the same model is used for both\\nqueries and items. Cross-encoder models input both the query and\\nitem and output a score to be used for ranking  = Cr (  ;  ),\\nwhere Cr is parameterized as w dropout W MPNet(\\u00b7) . We\\ntrain our bi-encoder model with a margin ranking loss: L =\\n\\u00cd \\u00cd=1 max2(q, d ) \\u2212 2(q, d\\u2032 ) + , 0 with randomly\\nsampled negatives \\u2032 and  = 1. Our cross-encoders are trained with\\na cross-entropy loss: L = \\u00cd \\u00cd=1 log( \\u00cd\\u2032\\u2032 ). For training, 4\\nnegative example items \\u2032 are randomly sampled from ranks\\n100300 from our trained bi-encoder. At test time, we retrieve the top\\n200 items with our trained bi-encoder and re-rank them with the\\ncross-encoder - we evaluate both these components in experiments\\nand refer to them as BiEnc-Mint and CrEnc-Mint.\\n4\"\n      ]\n    },\n    {\n      \"title\": \"4https://platform.openai.com/docs/models/gpt-3, text-davinci-003\",\n      \"paragraphs\": [\n        \"q and d . Embeddings are obtained by averaging token embeddings\\nfrom the final layer of MPNet, and the same model is used for both\\nqueries and items. Cross-encoder models input both the query and\\nitem and output a score to be used for ranking  = Cr (  ;  ),\\nwhere Cr is parameterized as w dropout W MPNet(\\u00b7) . We\\ntrain our bi-encoder model with a margin ranking loss: L =\\n\\u00cd \\u00cd=1 max2(q, d ) \\u2212 2(q, d\\u2032 ) + , 0 with randomly\\nsampled negatives \\u2032 and  = 1. Our cross-encoders are trained with\\na cross-entropy loss: L = \\u00cd \\u00cd=1 log( \\u00cd\\u2032\\u2032 ). For training, 4\\nnegative example items \\u2032 are randomly sampled from ranks\\n100300 from our trained bi-encoder. At test time, we retrieve the top\\n200 items with our trained bi-encoder and re-rank them with the\\ncross-encoder - we evaluate both these components in experiments\\nand refer to them as BiEnc-Mint and CrEnc-Mint.\\n4\"\n      ]\n    },\n    {\n      \"title\": \"EXPERIMENTS AND RESULTS\",\n      \"paragraphs\": [\n        \"Next, we evaluate Mint on a publicly available test collection for\\nNDR and present a series of ablations.\\n4.1\",\n        \"\\n        Experimental Setup\\n4.1.1 Datasets. We perform evaluations on an NDR dataset for\\npoint-of-interest (POI) recommendation Pointrec \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Results\",\n      \"paragraphs\": [\n        \"Table 1 presents the performance of the proposed method compared\\nagainst baselines. Here, bold numbers indicate the best-performing\\nmodel, and superscripts indicate statistical significance computed\\nwith two-sided t-tests at  < 0.05.\",\n        \"\\n        Here, we first note the performance of baseline approaches. We\\nsee BM25 outperformed by Contriver, a transformer bi-encoder\\nmodel trained for zero-shot retrieval; this mirrors prior work \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Ablations\",\n      \"paragraphs\": [\n        \"In Table 2, we ablate various design choices in Mint. Diferent\\nchoices result in diferent training sets for the BiEnc and CrEnc\\nmodels. Also, note that in reporting ablation performance for CrEnc,\\nwe still use the performant BiEnc-Mint model for obtaining\\nnegative examples for training and first-stage ranking. Without\\nhighquality negative examples, we found CrEnc to result in much poorer\\nperformance.\",\n        \"No item fil tering. Since synthetic queries are unlikely to\\nrep\\nresent all the items of a user, Mint excludes user items { }=1\\nwhich have a low likelihood of being generated from the document\\n(\\u00a73.2.2). Without this step, we expect the training set for training\\nretrieval models to be larger and noisier. In Table 2, we see that\\nexcluding this step leads to a lower performance for BiEnc and\\nCrEnc, indicating that the quality of data obtained is important for\\nperformance.\",\n        \"\\n        6B LLM for QGen. Mint relies on using an expensive 175B\\nparameter InstructGPT model for QGen. Here, we investigate the\\neficacy for generating  for { }=1 with a 6B parameter\\nInstructGPT model (text-curie-001). We use an identical setup to the\\n175B LLM for this. In Table 2, we see that training on the synthetic\\nnarrative queries of the smaller LLM results in worse models \\u2013\\noften underperforming the baselines in Table 1. This indicates the\\ninability of a smaller model to generate complex narrative queries\\nwhile conditioning on a set of user items. This necessity of a larger\\nLLM for generating queries in complex retrieval tasks has been\\nobserved in prior work \\n        \",\n        \"\\n          MAP\\n6B LLM for Item Queries. We find a smaller 6B LLM to result\\nin poor quality data when used to generate narrative queries\\nconditioned on { }=1. Here we simplify the text generation task \\u2013\\nusing a 6B LLM to generate queries for individual items  . This\\nexperiment also mirrors the setup for generating synthetic queries\\nfor search tasks \\n          \"\n      ]\n    },\n    {\n      \"title\": \"Pointrec\",\n      \"paragraphs\": [\n        \"\\n          MAP\\n6B LLM for Item Queries. We find a smaller 6B LLM to result\\nin poor quality data when used to generate narrative queries\\nconditioned on { }=1. Here we simplify the text generation task \\u2013\\nusing a 6B LLM to generate queries for individual items  . This\\nexperiment also mirrors the setup for generating synthetic queries\\nfor search tasks \\n          \"\n      ]\n    },\n    {\n      \"title\": \"CONCLUSIONS\",\n      \"paragraphs\": [\n        \"In this paper, we present Mint, a data augmentation method for the\\nnarrative-driven recommendation (NDR) task. Mint re-purposes\\nhistorical user-item interaction datasets for NDR by using a 175B\\nparameter large language model to author long-form narrative queries\\nwhile conditioning on the text of items liked by users. We evaluate\\nbi-encoder and cross-encoder models trained on data from Mint on\\nthe publicly available Pointrec test collection for narrative-driven\\npoint of interest recommendation. We demonstrate that the\\nresulting models outperform several strong baselines and ablated models\\nand match or outperform a 175B LLM directly used for NDR in a\\n1-shot setup.\",\n        \"However, Mint also presents some limitations. Given our use of\\nhistorical interaction datasets for generating synthetic training data\\nand the prevalence of popular interests in these datasets longer,\\ntailed interests are unlikely to be present in the generated\\nsynthetic datasets. In turn, causing retrieval models to likely see poorer\\nperformance on these requests. Our use of LLMs to generate\\nsynthetic queries also causes the queries to be repetitive in structure,\\nlikely causing novel longer-tail queries to be poorly served. These\\nlimitations may be addressed in future work.\",\n        \"MAP\",\n        \"Recall@100\",\n        \"Besides this, other avenues also present rich future work. While\\nMint leverages a 175B LLM for generating synthetic queries, smaller\\nparameter LLMs may be explored for this purpose - perhaps by\\ntraining dedicated QGen models. Mint may also be expanded to\\nexplore more active strategies for sampling items and users for\\nwhom narrative queries are authored - this may allow more\\neficient use of large parameter LLMs while ensuring higher quality\\ntraining datasets. Next, the generation of synthetic queries from\\nsets of documents may be explored for a broader range of retrieval\\ntasks beyond NDR given its promise to generate larger training\\nsets \\u2013 a currently underexplored direction. Finally, given the lack of\\nlarger-scale test collections for NDR and the efectiveness of LLMs\\nfor authoring narrative queries from user-item interaction, fruitful\\nfuture work may also explore the creation of larger-scale datasets\\nin a mixed-initiative setup to robustly evaluate models for NDR.\"\n      ]\n    },\n    {\n      \"title\": \"ACKNOWLEDGMENTS\",\n      \"paragraphs\": [\n        \"We thank anonymous reviewers for their invaluable feedback. This\\nwork was partly supported by the Center for Intelligent\\nInformation Retrieval, NSF grants IIS-1922090 and 2143434, the Ofice of\\nNaval Research contract number N000142212688, an Amazon Alexa\\nPrize grant, and the Chan Zuckerberg Initiative under the project\\nScientific Knowledge Base Construction. Any opinions, findings\\nand conclusions or recommendations expressed here are those of\\nthe authors and do not necessarily reflect those of the sponsors.\"\n      ]\n    }\n  ],\n  \"references\": [\n    \"Ref ID: ref1, Given Names: Jafar Krisztian, Surname: Afzali Balog, Year: 2021, Article Title: POINTREC: A Test Collection for Narrative-Driven Point of Interest Recommendation, Source: In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21), Volume: None, Issue: None, First Page: 2478, Last Page: 2484\",\n    \"Ref ID: ref2, Given Names: Jaime Fernando, Surname: Arguello Diaz, Year: 2021, Article Title: Tip of the Tongue Known-Item Retrieval: A Case Study in Movie Identification, Source: In Proceedings of the 6th international ACM SIGIR Conference on Human Information Interaction and Retrieval, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref3, Given Names: Toine Mette, Surname: Bogers Skov, Year: 2018, Article Title: What was this Movie About this Chick?\\u201d A Comparative Study of Relevance Aspects in Book and Movie Discovery, Source: In Transforming Digital Worlds: 13th International Conference, Volume: 25, Issue: None, First Page: 323, Last Page: 334\",\n    \"Ref ID: ref4, Given Names: Toine Mette, Surname: Bogers Skov, Year: 2019, Article Title: Looking for an amazing game I can relax and sink hours into, Source: None, Volume: None, Issue: None, First Page: 503, Last Page: 515\",\n    \"Ref ID: ref5, Given Names: Toine Marijn, Surname: Bogers Koolen, Year: 2017, Article Title: Defining and Supporting Narrative-Driven Recommendation, Source: In Proceedings of the Eleventh ACM Conference on Recommender Systems (Como, Volume: None, Issue: None, First Page: 238, Last Page: 242\",\n    \"Ref ID: ref6, Given Names: Toine Marijn, Surname: Bogers Koolen, Year: 2018, Article Title: \\u201cI'm looking for something like, Source: In Knowledge-aware and Conversational Recommender Systems Workshop. CEUR Workshop Proceedings., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref7, Given Names: Luiz Rodrigo, Surname: Bonifacio Nogueira, Year: 2022, Article Title: InPars: Unsupervised Dataset Generation for Information Retrieval, Source: In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Volume: None, Issue: None, First Page: 2387, Last Page: 2392\",\n    \"Ref ID: ref8, Given Names: Leonid Eric, Surname: Boytsov Nyberg, Year: 2023, Article Title: InPars-Light: Cost-Efective Unsupervised Training of Eficient Rankers, Source: None, Volume: 2301, Issue: None, First Page: 02998, Last Page: None\",\n    \"Ref ID: ref9, Given Names: Tom Mateusz McCandlish Alec Dario M. R. M.F., Surname: Brown Jared D Kaplan Litwin Sam Radford Amodei Ranzato Hadsell Balcan, Year: 2020, Article Title: Language Models are Few-Shot Learners, Source: In Advances in Neural Information Processing Systems, Volume: 33, Issue: None, First Page: 1877, Last Page: 1901\",\n    \"Ref ID: ref10, Given Names: Chris, Surname: Buckley and Ellen M. Voorhees, Year: 2004, Article Title: Retrieval Evaluation with Incomplete Information, Source: In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Shefield, United Kingdom) (SIGIR '04), Volume: None, Issue: None, First Page: 25, Last Page: 32\",\n    \"Ref ID: ref11, Given Names: Chae, Surname: Dong-Kyu Sang-Wook Kim, Year: 2020, Article Title: ARCF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems, Source: In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20), Volume: None, Issue: None, First Page: 1251, Last Page: 1260\",\n    \"Ref ID: ref12, Given Names: Chen Meng, Surname: Lei Le Wu Wang, Year: 2023, Article Title: Improving Recommendation Fairness via Data Augmentation, Source: In Proceedings of the ACM Web Conference, Volume: None, Issue: None, First Page: 1012, Last Page: 1020\",\n    \"Ref ID: ref13, Given Names: Chen Lehong, Surname: Li Zhao, Year: 2022, Article Title: A Pilot Study for Understanding Users' Attitudes Towards a Conversational Agent for News Recommendation, Source: In Proceedings of the 4th Conference on Conversational User Interfaces (Glasgow, Volume: 36, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref14, Given Names: Hyung William Eric Xuezhi Siddhartha, Surname: Won Chung Fedus Li Wang Brahma, Year: 2022, Article Title: Scaling instruction-finetuned language models, Source: arXiv preprint arXiv:2210.11416, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref15, Given Names: Dai, Surname: Zhuyun Ming-Wei Chang, Year: 2023, Article Title: Promptagator: Few-shot Dense Retrieval From 8 Examples, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref16, Given Names: Shyam, Surname: Abhinandan S. Das Mayur Datar Rajaram, Year: 2007, Article Title: Google News Personalization: Scalable Online Collaborative Filtering, Source: In Proceedings of the 16th International Conference on World Wide Web (Banf, Volume: None, Issue: None, First Page: 271, Last Page: 280\",\n    \"Ref ID: ref17, Given Names: Davidson He Dasarathi, Surname: James Ullas Gargi Yu Sampath, Year: 2010, Article Title: The YouTube Video Recommendation System, Source: In Proceedings of the Fourth ACM Conference on Recommender Systems (Barcelona, Volume: None, Issue: None, First Page: 293, Last Page: 296\",\n    \"Ref ID: ref18, Given Names: Eberhard Denis, Surname: Lukas Helic, Year: 2019, Article Title: Evaluating Narrative-Driven Movie Recommendations on Reddit, Source: In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, Volume: None, Issue: None, First Page: 1, Last Page: 11\",\n    \"Ref ID: ref19, Given Names: Gao and Jamie, Surname: Luyu Callan, Year: 2022, Article Title: Precise Zero-Shot Dense Retrieval without Relevance Labels, Source: arXiv preprint arXiv:2212.10496, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref20, Given Names: Hariri Robin, Surname: Negar Burke, Year: 2013, Article Title: Query-Driven Context Aware Recommendation, Source: In Proceedings of the 7th ACM Conference on Recommender Systems (Hong Kong, China) (RecSys '13), Volume: None, Issue: None, First Page: 9, Last Page: 16\",\n    \"Ref ID: ref21, Given Names: Seyyed, Surname: Hadi Hashemi, Year: 2016, Article Title: Overview of the TREC 2016 Contextual Suggestion Track., Source: Charles LA Clarke, and Ellen M Voorhees, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref22, Given Names: Izacard Sebastian Edouard, Surname: Gautier Riedel Grave, Year: 2022, Article Title: Unsupervised Dense Information Retrieval with Contrastive Learning, Source: Transactions on Machine Learning Research, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref23, Given Names: Jeronymo Rodrigo, Surname: Vitor Nogueira, Year: 2023, Article Title: InPars-v2: Large Language Models as Eficient Dataset Generators for Information Retrieval, Source: arXiv:2301, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref24, Given Names: Koolen David, Surname: Marijn Walsh Interaction, Year: 2016, Article Title: Overview of the CLEF 2016 Social Book Search Lab, Source: None, Volume: None, Issue: None, First Page: 351, Last Page: 370\",\n    \"Ref ID: ref25, Given Names: Leszczynski, Surname: Megan, Year: 2023, Article Title: Generating Synthetic Data for Conversational Music Recommendation Using Random Walks and Language Models, Source: None, Volume: 2301, Issue: None, First Page: 11489, Last Page: None\",\n    \"Ref ID: ref26, Given Names: Liu Chunyan, Surname: Xin Miao, Year: 2013, Article Title: Personalized Point-ofInterest Recommendation by Mining Users' Preference Transition, Source: In Proceedings of the 22nd ACM International Conference on Information & Knowledge, Volume: None, Issue: None, First Page: 733, Last Page: 738\",\n    \"Ref ID: ref27, Given Names: Liu Pham Quan, Surname: Yiding Tuan-Anh Nguyen Gao Cong Yuan, Year: 2017, Article Title: An Experimental Evaluation of Point-of-Interest Recommendation in Location-Based Social Networks, Source: Proc. VLDB Endow, Volume: 10, Issue: 10, First Page: 1010, Last Page: 1021\",\n    \"Ref ID: ref28, Given Names: L\\u00f3pez Michael Lucas, Surname: Federico Strube Dixon, Year: 2021, Article Title: Augmenting the user-item graph with textual similarity models, Source: arXiv preprint arXiv:2109.09358, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref29, Given Names: Xing Siva, Surname: Han Lu Reddy, Year: 2023, Article Title: The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents, Source: In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, Volume: None, Issue: None, First Page: 2799, Last Page: 2829\",\n    \"Ref ID: ref30, Given Names: Kai Hanze and Hojin, Surname: Luo Li Yang, Year: 2020, Article Title: Latent Linear Critiquing for Conversational Recommender Systems, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref31, Given Names: Ma, Surname: Ji Ryan McDonald, Year: 2021, Article Title: Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation, Source: In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:, Volume: None, Issue: None, First Page: 1075, Last Page: 1088\",\n    \"Ref ID: ref32, Given Names: Mysore and Hamed, Surname: Sheshera Tim O'Gorman Andrew McCallum Zamani, Year: 2021, Article Title: CSFCube - A Test Collection of Computer Science Research Articles for Faceted Query by Example, Source: In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round, Volume: 2, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref33, Given Names: Ouyang Luke Peter Ryan S. A. D. K., Surname: Long Miller Welinder Jan Leike Lowe Mohamed Agarwal Belgrave Cho A, Year: 2022, Article Title: Training language models to follow instructions with human feedback, Source: In Advances in Neural Information Processing Systems, Volume: 35, Issue: None, First Page: 27730, Last Page: 27744\",\n    \"Ref ID: ref34, Given Names: Papenmeier Norbert ACT, Surname: Andrea Fuhr Retrieval (Canberra, Year: 2021, Article Title: Starting Conversations with Search Engines - Interfaces That Elicit Natural Language Queries, Source: In Proceedings of the 2021 Conference on Human Information Interaction, Volume: None, Issue: None, First Page: 261, Last Page: 265\",\n    \"Ref ID: ref35, Given Names: Penha Alice Hugues, Surname: Gustavo Wang Bouchard, Year: 2023, Article Title: Improving Content Retrievability in Search with Controllable Query Generation, Source: In Proceedings of the ACM Web Conference, Volume: None, Issue: None, First Page: 3182, Last Page: 3192\",\n    \"Ref ID: ref36, Given Names: Radlinski Ben, Surname: Filip Wedin, Year: 2022, Article Title: On Natural Language User Profiles for Transparent and Scrutable Recommendation, Source: In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Volume: None, Issue: None, First Page: 2863, Last Page: 2874\",\n    \"Ref ID: ref37, Given Names: Nils Iryna, Surname: Reimers Gurevych, Year: 2019, Article Title: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, Source: In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref38, Given Names: Stephen Hugo Beyond. Found. Trends, Surname: Robertson Zaragoza Inf, Year: 2009, Article Title: The Probabilistic Relevance Framework: BM25 and, Source: Retr. 3, Volume: None, Issue: 4, First Page: 333, Last Page: 389\",\n    \"Ref ID: ref39, Given Names: Jon Christopher, Surname: Saad-Falcon Potts, Year: 2023, Article Title: UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers, Source: None, Volume: 2303, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref40, Given Names: Sachan Mike Luke, Surname: Devendra Lewis Yih Zettlemoyer, Year: 2022, Article Title: Improving Passage Retrieval with Zero-Shot Question Generation, Source: In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Volume: None, Issue: None, First Page: 3781, Last Page: 3797\",\n    \"Ref ID: ref41, Given Names: Song, Surname: Kaitao Tie-Yan Liu, Year: 2020, Article Title: MPNet: Masked and Permuted Pre-training for Language Understanding, Source: In Advances in Neural Information Processing Systems, Volume: 33, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref42, Given Names: Teevan Eric, Surname: Jaime Horvitz, Year: 2005, Article Title: Personalizing Search via Automated Analysis of Interests and Activities, Source: In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Salvador, Brazil) (SIGIR '05), Volume: None, Issue: None, First Page: 449, Last Page: 456\",\n    \"Ref ID: ref43, Given Names: Mengting, Surname: Wan and Julian McAuley, Year: 2018, Article Title: Item Recommendation on Monotonic Behavior Chains, Source: In Proceedings of the 12th ACM Conference on Recommender Systems, Volume: None, Issue: None, First Page: 86, Last Page: 94\",\n    \"Ref ID: ref44, Given Names: Wang Hongxia Jingrui, Surname: Haonan Chang Zhou Yang He, Year: 2021, Article Title: Controllable Gradient Item Retrieval, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref45, Given Names: Wang Lizhen, Surname: Qinyong Cui AK, Year: 2019, Article Title: Enhancing Collaborative Filtering with Generative Augmentation, Source: In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Anchorage, Volume: None, Issue: None, First Page: 548, Last Page: 556\",\n    \"Ref ID: ref46, Given Names: Xu Andrew Charles, Surname: Jiajing Zhai Rosenberg, Year: 2022, Article Title: Rethinking Personalized Ranking at Pinterest: An End-to-End Approach, Source: In Proceedings of the 16th ACM Conference on Recommender Systems, Volume: None, Issue: None, First Page: 502, Last Page: 505\",\n    \"Ref ID: ref47, Given Names: Ying Deqing Hongwei, Surname: Yuxin Wang Zheng, Year: 2023, Article Title: CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users in Recommendation, Source: In Proceedings of the ACM Web Conference, Volume: None, Issue: None, First Page: 1396, Last Page: 1404\",\n    \"Ref ID: ref48, Given Names: Zamani Filip, Surname: Hamed Radlinski, Year: 2022, Article Title: Conversational information seeking, Source: arXiv preprint arXiv:2201.08808, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref49, Given Names: Zou Evangelos, Surname: Jie Kanoulas, Year: 2020, Article Title: Towards Question-Based Recommender Systems, Source: In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20), Volume: None, Issue: None, First Page: 881, Last Page: 890\"\n  ]\n}",
    "{\n  \"Article_ID\": \"3d8b35954eea453381a55aed4e0112bd\",\n  \"DocumentTitle\": \"SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes\",\n  \"Auteurs\": \"Paul Saves, R\\u00e9mi Lafage, Nathalie Bartoli, Youssef Diouane, Jasper Bussemaker, Thierry Lefebvre, John T. Hwang, Joseph Morlier, Joaquim R.R.A. Martins g, \",\n  \"Institutions\": \"German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, ICA, Universite\\u0301 de Toulouse, ISAE-SUPAERO, Universite\\u0301 de Toulouse, ONERA/DTIS, Universite\\u0301 de Toulouse, Polytechnique Montre\\u0301al, University of California San Diego, Department of Mechanical and Aerospace Engineering, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"-\",\n      \"paragraphs\": [\n        \"A R T I C L E\",\n        \"I N F O\\nDataset link: https://colab.research.google.com\\n/github/SMTorg/smt/blob/master/tutorial/No\\ntebookRunTestCases_Paper_SMT_v2.ipynb\"\n      ]\n    },\n    {\n      \"title\": \"1. Motivation and significance\",\n      \"paragraphs\": [\n        \"With the increasing complexity and accuracy of numerical models, it\\nhas become more challenging to run complex simulations and computer\\ncodes 1,2. As a consequence, surrogate models have been recognized\\nas a key tool for engineering tasks such as design space exploration,\\nuncertainty quantification, and optimization 3. In practice, surrogate\\nmodels are used to reduce the computational effort of these tasks by\\nreplacing expensive numerical simulations with closed-form\\napproximations 4, Ch. 10. To build such a model, we start by evaluating\\nthe original expensive simulation at a set of points through a Design\\nof Experiments (DoE). Then, the corresponding evaluations are used to\\nbuild the surrogate model according to the chosen approximation, such\\nas Kriging, quadratic interpolation, or least squares regression.\",\n        \"The Surrogate Modeling Toolbox (SMT) is an open-source\\nframework that provides functions to efficiently build surrogate models 5.\",\n        \"\\n        A B S T R A C T\\nThe Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate\\nmodeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major\\nnew release of SMT that introduces significant upgrades and new features to the toolbox. This release adds\\nthe capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables\\nare becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT\\nby extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives\\nfor Kriging. This release also includes new functions to handle noisy and use multi-fidelity data. To the best of\\nour knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical\\nand mixed inputs. This open-source software is distributed under the New BSD license.2\\nKriging models (also known as Gaussian processes) that take advantage\\nof derivative information are one of SMT\\u2019s key features 6. Numerical\\nexperiments have shown that SMT achieved lower prediction error\\nand computational cost than Scikit-learn 7 and UQLab \\n        \",\n        \"\\n        In systems engineering, architecture-level choices significantly\\ninfluence the final system performance, and therefore, it is desirable to\\nconsider such choices in the early design phases \\n        \",\n        \"Reference\\nLicense\\nLanguage\\nMixed var.\",\n        \"\\n        GD kernel\\nCR kernel\\nHH kernel\\nEHH kernel\\nHierarchical var.\\n\\n        \",\n        \"\\n        When architectural choices lead to different sets of design variables,\\nwe have hierarchical variables \\n        \",\n        \"\\n        Handling hierarchical and mixed variables requires specialized\\nsurrogate modeling techniques \\n        \",\n        \"\\n        There are two new major capabilities in SMT 2.0: the ability\\nto build surrogate models involving mixed variables and the support\\nfor hierarchical variables within Kriging models. To handle mixed\\nvariables in Kriging models, existing libraries such as BoTorch \\n        \",\n        \"\\n        SMT 2.0 introduces other enhancements, such as additional\\nsampling procedures, new surrogate models, new Kriging kernels (and their\\nderivatives), Kriging variance derivatives, and an adaptive criterion for\\nhigh-dimensional problems. SMT 2.0 adds applications of Bayesian\\noptimization (BO) with hierarchical and mixed variables or noisy\\ncoKriging that have been successfully applied to aircraft design \\n        \",\n        \"The remainder of the paper is organized as follows. First, we\\nintroduce the organization and the main implemented features of the\\nrelease in Section 2. Then, we describe the mixed-variable Kriging\\nmodel with an example in Section 3. Similarly, we describe and provide\\nan example for a hierarchical-variable Kriging model in Section 4.\",\n        \"The Bayesian optimization models and applications are described in\\nSection 5. Finally, we describe the other relevant contributions in\\nSection 6 and conclude in Section 7.\",\n        \"From a software point of view, SMT 2.0 maintains and improves\\nthe modularity and generality of the original SMT version 5. In this\\nsection, we describe the software as follows. Section 2.1 describes the\\nlegacy of SMT 0.2. Then, Section 2.2 describes the organization of the\\nrepository. Finally, Section 2.3 shows the new capabilities implemented\\nin the SMT 2.0 update.\",\n        \"SMT 5 is an open-source collaborative work originally developed\\nby ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of\\nMichigan. Now, both Polytechnique Montr\\u00e9al and the University of\\nCalifornia San Diego are also contributors. SMT 2.0 updates and\\nextends the original SMT repository capabilities among which the original\\npublication 5 focuses on different types of derivatives for surrogate\\nmodels detailed hereafter.\\n3 http://smt.readthedocs.io/en/latest\\n4 https://github.com/SMTorg/smt\\n5 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\",\n        \"\\n            original main motivations for SMT was derivative support. In fact, none\\nof the existing packages for surrogate modeling such as Scikit-learn in\\nPython 7, SUMO in Matlab \\n            \",\n        \"\\n            nized along three main sub-modules that implement a set of sampling\\ntechniques (sampling_methods), benchmarking functions (problems),\\nand surrogate modeling techniques (surrogate_models). The toolbox\\ndocumentation6 is created using reStructuredText and Sphinx, a\\ndocumentation generation package for Python, with custom extensions.\\nCode snippets in the documentation pages are taken directly from\\nactual tests in the source code and are automatically updated. The\\noutput from these code snippets and tables of options are generated\\ndynamically by custom Sphinx extensions. This leads to high-quality\\ndocumentation with minimal effort. Along with user documentation,\\ndeveloper documentation is also provided to explain how to contribute\\nto SMT. This includes a list of API methods for the SurrogateModel,\\nSamplingMethod, and Problem classes, that must be implemented\\nto create a new surrogate modeling method, sampling technique, or\\nbenchmarking problem. When a developer submits a pull request, it is\\nmerged only after passing the automated tests and receiving approval\\nfrom at least one reviewer. The repository on GitHub7 is linked to\\ncontinuous integration tests (GitHub Actions) for Windows, Linux and\\nMacOS, to a coverage test on coveralls.io and to a dependency version\\ncheck for Python with DependaBot. Various parts of the source code\\nhave been accelerated using Numba \\n            \",\n        \"The main features of the open-source repository SMT 2.0 are\\ndescribed in Fig. 1. More precisely, Sampling Methods, Problems\\nand Surrogate models are kept from SMT 0.2 and two new\\nsections Models applications and Interactive notebooks\\nhave been added to the architecture of the code. These sections are\\n6 https://smt.readthedocs.org\\n7 https://github.com/SMTorg/smt\\nhighlighted in blue and detailed on Fig. 1. The new major features\\nimplemented in SMT 2.0 are highlighted in lavender whereas the\\nlegacy features that were already in present in the original publication\\nfor SMT 0.2 5 are in black.\\n2.3. New features within SMT 2.0\",\n        \"The main objective of this new release is to enable Kriging surrogate\\nmodels for use with both hierarchical and mixed variables. Moreover,\\nfor each of these five sub-modules described in Section 2.2, several\\nimprovements have been made between the original version and the\\nSMT 2.0 release.\",\n        \"\\n            Hierarchical and mixed design space. A new design space definition\\nclass DesignSpace has been added that implements hierarchical\\nand mixed functionalities. Design variables can either be\\ncontinuous (FloatVariable), ordered (OrdinalVariable) or categorical\\n(CategoricalVariable). The integer type (IntegerVariable)\\nrepresents a special case of the ordered variable, specified by bounds\\n(inclusive) rather than a list of possible values. The hierarchical\\nstructure of the design space can be defined using declare_decreed_var:\\nthis function declares that a variable is a decreed variable that is\\nactivated when the associated meta variable takes one of a set of\\nspecified values, see Section 4 for background. The DesignSpace\\nclass also implements mechanisms for sampling valid design vectors\\n(i.e. design vectors that adhere to the hierarchical structure of the\\ndesign space) using any of the below-mentioned samplers, for\\ncorrecting and imputing design vectors, and for requesting which design\\nvariables are acting in a given design vector. Correction ensures that\\nvariables have valid values (e.g. integers for discrete variables) \\n            \",\n        \"\\n            Sampling. SMT implements three methods for sampling. The first one\\nis a na\\u00efve approach, called Random that draws uniformly points along\\nevery dimension. The second sampling method is called Full\\nFactorial and draws a point for every cross combination of variables,\\nto have an \\u2018\\u2018exhaustive\\u2019\\u2019 design of experiments. The last one is the\\nLatin Hypercube Sampling (LHS) \\n            \",\n        \"Problems. SMT implements two new engineering problems: a mixed\\nvariant of a cantilever beam described in Section 3 and a hierarchical\\nneural network described in Section 4.\",\n        \"\\n            Surrogate models. In order to keep up with state-of-art, several\\nreleases done from the original version developed new options for the\\nalready existing surrogates. In particular, compared to the original\\npublication 5, SMT 2.0 adds gradient-enhanced neural networks \\n            \",\n        \"In the following, Section 3 details the Kriging based surrogate\\nmodels for mixed variables, and Section 4 presents our new Kriging\\nsurrogate for hierarchical variables. Section 5 details the EGO\\napplication and the other new relevant features aforementioned are described\\nsuccinctly in Section 6.\",\n        \"8 https://github.com/SMTorg/smt/tree/master/tutorial\\n9 https://colab.research.google.com/github/SMTorg/smt/\\n10 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\",\n        \"As mentioned in Section 1, design variables can be either of\\ncontinuous or discrete type, and a problem with both types is a mixed-variable\\nproblem. Discrete variables can be ordinal or categorical. A discrete\\nvariable is ordinal if there is an order relation within the set of possible\\nvalues. An example of an ordinal design variable is the number of\\nengines in an aircraft. A possible set of values in this case could be\\n2, 4, 8. A discrete variable is categorical if no order relation is known\\nbetween the possible choices the variable can take. One example of a\\ncategorical variable is the color of a surface. A possible example of a\\nset of choices could be blue, red, green. The possible choices are called\\nthe levels of the variable.\",\n        \"\\n            Several methods have been proposed to address the recent increase\\ninterest in mixed Kriging based models \\n            \",\n        \"The continuous and ordinal variables are both treated similarly\\nin SMT 2.0 with a continuous kernel, where the ordinal values are\\nconverted to continuous through relaxation. For categorical variables,\\nfour models (GD, CR, EHH and HH) can be used in SMT 2.0 if\\nspecified by the API. This is why we developed a unified mathematical\\nformulation that allows a unique implementation for any model.\",\n        \"Denote  the number of categorical variables. For a given  \\u2208\\n{1, \\u2026 , }, the th categorical variable is denoted  and its number\\nof levels is denoted . The hyperparameter matrix peculiar to this\\nvariable  is\",\n        \"\\u23a1 1,1 . \\u23a4\\n = \\u23a2\\u23a2\\u23a2 \\u22ee1,2 \\u22f12,2 \\u22f1 \\u23a5\\u23a5\\u23a5 ,\",\n        \"\\n            \\u23a3\\u23a21, \\u2026 \\u22121, , \\u23a6\\u23a5\\nand the categorical parameters are defined as  = {1, \\u2026 , }. For\\ntwo given inputs in the DoE, for example, the th and th points, let\\n and  be the associated categorical variables taking respectively\\nthe  and the  level on the categorical variable . The categorical\\n\\ncorrelation kernel is defined by\\n(, , ) =\\n\\n\\u220f ((), ) ((), )((), )((), )\\n=1        \\n(1)\\nwhere  is either a positive definite kernel or identity and (.) is a\\nsymmetric positive definite (SPD) function such that the matrix ()\\nis SPD if  is SPD. For an exponential kernel, Table 3 gives the\\nparameterizations of  and  that correspond to GD, CR, HH, and\\nEHH kernels. The complexity of these different kernels depends on\\nthe number of hyperparameters that characterizes them. As defined\\nby Saves et al. \\n            \",\n        \"\\n            Another Kriging based model that can use mixed variables is Kriging\\nwith partial least squares (KPLS) \\n            \",\n        \"\\n            A classic engineering problem commonly used for model validation\\nis the beam bending problem \\n            \",\n        \"\\n            To compare the mixed Kriging models of SMT 2.0, we draw a 98\\npoint LHS as training set and the validation set is a grid of 12 \\u00d7 30 \\u00d7\\n30 = 10800 points. For the four implemented methods, displacement\\nerror (computed with a root-mean-square error criterion), likelihood,\\nnumber of hyperparameters and computational time for every model\\nare shown in Table 4. For the continuous variables, we use the square\\nexponential kernel. More details are found in \\n            \",\n        \"\\n          To introduce the newly developed Kriging model for hierarchical\\nvariables implemented in SMT 2.0, we present the general\\nmathematical framework for hierarchical and mixed variables established\\nby Audet et al. \\n          \",\n        \"\\n            A problem structure is classified as hierarchical when the sets of\\nactive variables depend on architectural choices. This occurs frequently\\nin industrial design problems. In hierarchical problems, we can classify\\nvariables as neutral, meta (also known as dimensional) or decreed\\n(also known as conditionally active) as detailed in Audet et al. \\n            \",\n        \"However, the wing aspect ratio being neutral, it is not affected by this\\nhierarchy.\",\n        \"\\n            Problems involving hierarchical variables are generally dependant\\non discrete architectures and as such involve mixed variables. Hence,\\nin addition to their role (neutral, meta or decreed), each variable also\\nhas a variable type amongst categorical, ordinal or continuous. For the\\nsake of simplicity and because both continuous and ordinal variables\\nare treated similarly \\n            \",\n        \"\\n            To explain the framework and the new Kriging model, we illustrate\\nthe inputs variables of the model using a classical machine\\nlearning problem related to the hyperparameters optimization of a\\nfullyconnected Multi-Layer Perceptron (MLP) \\n            \",\n        \"According to their types, the MLP input variables can be classified as\\nfollows:\\n4. The meta variable \\u2018\\u2018# of hidden layers\\u2019\\u2019 is an integer and, as\\nqnt\\nsuch, is represented by the component met .\\n5. The decreed variables \\u2018\\u2018# of neurons hidden layer \\u2019\\u2019 are integers\\nqnt\\nand, as such, are represented by the component dec.\\n6. The \\u2018\\u2018Learning rate\\u2019\\u2019, \\u2018\\u2018Momentum\\u2019\\u2019, \\u2018\\u2018Activation function\\u2019\\u2019 and\\n\\u2018\\u2018Batch size\\u2019\\u2019 are, respectively, continuous, for the first two\\n(every value between two bounds), categorical (qualitative between\\nthree choices) and integer (quantitative between 6 choices).\\nTherefore, the \\u2018\\u2018Activation function\\u2019\\u2019 and the \\u2018\\u2018Momentum\\u2019\\u2019 are\\nrepresented by the component cnaetu. The \\u2018\\u2018Learning rate\\u2019\\u2019 and the\\nqnt\\n\\u2018\\u2018Batch size\\u2019\\u2019 are represented by the component neu.\",\n        \"\\n            To model hierarchical variables, as proposed in \\n            \",\n        \"In this section, a new method to build a Kriging model with\\nhierarchical variables is introduced based on the framework aforementioned.\\nThe proposed methods are included in SMT 2.0.\",\n        \"\\n            Assuming that the decreed variables are quantitative, Hutter and\\nOsborne \\n            \",\n        \"In the following, we describe our new method to build a correlation\\nkernel for hierarchical variables. In particular, we introduce a new\\nalgebraic kernel called Alg-Kernel that behaves like the Arc-Kernel\\nwhilst correcting most of its drawbacks. In particular, our kernel does\\nnot add any hyperparameters, and the normalization is handled in a\\nnatural way.\",\n        \"\\n            For modeling purposes, we assume that the decreed space is\\nquantitative, i.e., \\ue244dec = \\ue244dqenct . Let  \\u2208 \\ue244 be an input point partitioned as\\n = (neu, met , inc(met )) and, similarly,  \\u2208 \\ue244 is another input such\\nthat  = (neu, met , inc(met )). The new kernel  that we propose for\\nhierarchical variables is given by\\n(, ) = neu(neu, neu) \\u00d7 met (met , met )\\n\\u00d7 met,dec(met , inc(met ), met , inc(met )),\\n(2)\\nwhere neu, met and met,dec are as follows:\\n\\u2022 neu represents the neutral kernel that encompasses both\\ncategorical and quantitative neutral variables, i.e., neu can be\\ndecomposed into two parts neu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, nqenut).\\nqnt\\nThe categorical kernel, denoted cat , could be any Symmetric\\nPositive Definite (SPD) \\n            \",\n        \"qnt (qnt , qnt ) = \\u220f exp(\\u2212(qnt , qnt )).\",\n        \"=1\\n\\u2022 met is the meta variables related kernel. It is also separated into\\ntwo parts: met (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett ) where the\\nquantitative kernel is ordered and not continuous because meta\\nvariables take value in a finite set.\\n\\u2022 met,dec is an SPD kernel that models the correlations between the\\nmeta levels (all the possible subspaces) and the decreed variables.\",\n        \"In what comes next, we detailed this kernel.\",\n        \"\\n            Meta-decreed kernels like the imputation kernel or the\\nArc-Kernel were first proposed in \\n            \",\n        \"Our proposed Alg-Kernel kernel is given by\",\n        \"= amlegt (met , met ) \\u00d7 daelgc(inc(met ), inc(met )).\",\n        \"Mathematically, we could consider that there is only one meta variable\\nwhose levels correspond to every possible included subspace. Let sub\\ndenotes the components indices of possible subspaces, the subspaces\\nparameterized by the meta component met are defined as \\ue244inc(met =\\n),  \\u2208 sub. It follows that the fully extended continuous decreed\\nspace writes as \\ue244dec = \\u22c3\\u2208sub \\ue244inc(met = ) and dec is the set of the\\nassociated indices. Let  denotes the set of components related to\\n,\\nthe space \\ue244inc(met , met ) containing the variables decreed-included in\\nboth \\ue244inc(met ) and \\ue244inc(met ).\",\n        \"Since the decreed variables are quantitative, one has\\ndaelgc(inc(met ), inc(met )) = qnt (inc(met ), inc(met ))\\n= \\u220f qnt (inc(met ), inc(met ))\\n(4)\\n(5)\\n\\u2208,\\nThe construction of the quantitative kernel qnt depends on a given\\ndistance denoted alg. The kernel amlegt is an induced meta kernel that\\ndepends on the same distance alg to preserve the SPD property of\\nalg\\nmet,dec. For every  \\u2208 dec, if  \\u2208 , the new algebraic distance is\\n,\\ngiven by\\nalg(inc(met ), inc(met )) = \\u239c\\u239c\\u239b \\u221a 2|inc(met ) \\u2212\\u221ainc(met )| \\u239f\\u239f\\u239e ,\\n\\u239c inc(met )2 + 1 inc(met )2 + 1 \\u239f\\n\\u239d \\u23a0\\nwhere  \\u2208 R+ is a continuous hyperparameter. Otherwise, if  \\u2208 dec\\nbut  \\u2209 , there should be a non-zero residual distance between the\\n,\\ntwo different subspaces \\ue244inc(met ) and \\ue244inc(met ) to ensure the kernel\\nSPD property. To have a residual not depending on the decreed values,\\nour model considers that there is a unit distance\\nalg(inc(met ), inc(met )) = 1.0 , \\u2200 \\u2208 dec \\u29f5 .\\n,\\nThe induced meta kernel amlegt (met , met ) to preserve the SPD property\\nof alg is defined as:\\n(6)\\n(7)\\n(3)\\namlegt (met , met ) = \\u220f qnt (1.0 ).\",\n        \"\\u2208met\",\n        \"\\n            Not only our kernel of Eq. (2) uses less hyperparameters than the\\nArcKernel (as we cut off its extra parameters) but it is also a more flexible\\nkernel as it allows different kernels for meta and decreed variables.\\nMoreover, another advantage of our kernel is that it is numerically\\nmore stable thanks to the new non-stationary \\n            \",\n        \"In what comes next, we will refer to the implementation of the\\nkernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and\\nSMT Alg-Kernel . We note also that the implementation of SMT\\nArc-Kernel differs slightly from the original Arc-Kernel as we\\nfixed some hyperparameters to 1 in order to avoid adding extra\\nhyperparameters and use the formulation of Eq. (2) and rescaling of the\\ndata.\",\n        \"In this section, we illustrate the hierarchical Arc-Kernel on the\\nMLP example. For that sake, we consider two design variables  and\\n such that  = (2.10\\u22124, 0.9, ReLU, 16, 2, 55, 51) and  = (5.10\\u22123, 0.8,\\nSigmoid, 64, 3, 50, 54, 53). Since the value of met (i.e., the number of\\nhidden layers) differs from one point to another (namely, 2 for  and 3\\nfor ), the associated variables inc(met ) have either 2 or 3 variables\\nfor the number of neurons in each layer (namely 55 and 51 for ,\\nand 50, 54 and 53 for the point ). In our case, 8\\nhyperparameters (11,2, 1, \\u2026 , 7) will have to be optimized where  is given by\\nEq. (2). These 7 hyperparameters can be described using our proposed\\nframework as follows:\\n\\u2022 For the neutral components, we have neu = (2.10\\u22124, 0.9, ReLU, 16)\\nand neu = (5.10\\u22123, 0.8, Sigmoid, 64). Therefore, for a categorical\\nmatrix kernel 1 and a square exponential quantitative kernel,\\nqnt qnt\\nneu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, neu)\\n= 11,2 exp \\u22121(2.10\\u22124 \\u2212 5.10\\u22123)2\",\n        \"exp \\u22122(0.9 \\u2212 0.8)2 exp \\u22123(16 \\u2212 64)2.\",\n        \"The values 11,2, 1, 2 and 3 need to be optimized. Here,\\n11,2 is the correlation between \\\"ReLU\\\" and \\\"Sigmoid\\\".\\n\\u2022 For the meta components, we have met = 2 and met = 3.\",\n        \"Therefore, for a square exponential quantitative kernel,\\nmet (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett )\",\n        \"= exp \\u22124(3 \\u2212 2)2.\",\n        \"The value 4 needs to be optimized.\\n\\u2022 For the meta-decreed kernel, we have met , inc(met ) = 2, (55, 51)\\nand met , inc(met ) = 3, (50, 54, 53) which gives\\nalg\\nwhere the meta induced component is met (met , met ) = exp \\u22127\\nbecause the decreed value 53 in  has nothing to be compared\\nwith in  as in Eq. (7). The values 5, 6 and 7 need to be\\noptimized which complete the description of the hyperparameters.\\nWe note that for the MLP problem, Alg-Kernel models use\\n10 hyperparameters whereas the Arc-Kernel would require\\n12 hyperparameters without the meta kernel (4) but with 3\\nextra decreed hyperparameters and the Wedge-Kernel would\\nrequire 15 hyperparameters. For deep learning applications, a\\nmore complex perceptron with up to 10 hidden layers would\\nrequire 17 hyperparameters with SMT 2.0 models against 26\\nfor Arc-Kernel and 36 for Wedge-Kernel . The next section\\nillustrates the interest of our method to build a surrogate model\\nfor this neural network engineering problem.\\n4.3. A neural network test-case using SMT 2.0\",\n        \"In this section, we apply our models to the hyperparameters\\noptimization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0\\nan example illustrates this MLP problem. For the sake of showing the\\nKriging surrogate abilities, we implemented a dummy function with no\\nsignificance to replace the real black-box that would require training\\na whole Neural Network (NN) with big data. This function requires a\\nnumber of variables that depends on the value of the meta variable,\\ni.e the number of hidden layers. To simplify, we have chosen only\\n1, 2 or 3 hidden layers and therefore, we have 3 decreed variables\\nbut deep neural networks could also be investigated as our model can\\ntackle a few dozen variables. A test case (test_hierarchical_variables_NN )\\nshows that our model SMT Alg-Kernel interpolates the data\\nproperly, checks that the data dimension is correct and also asserts that\\nthe inactive decreed variables have no influence over the prediction.\\nIn Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical\\nand mixed variables based on the implementation of SMT 2.0 for\\ntest_hierarchical_variables_NN.\",\n        \"\\n            To compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel\\nand SMT Arc-Kernel ) with the state-of-the-art imputation method\\npreviously used on industrial application (Imp-Kernel ) \\n            \",\n        \"\\n            Efficient global optimization (EGO) is a sequential Bayesian\\noptimization algorithm designed to find the optimum of a black-box\\nfunction that may be expensive to evaluate \\n            \",\n        \"\\n            Because SMT 2.0 implements Kriging models that handle mixed\\nand hierarchical variables, we can use EGO to solve problems\\ninvolving such design variables. Other Bayesian optimization algorithms\\noften adopt approaches based on solving subproblems with\\ncontinuous or non-hierarchical Kriging. This subproblem approach is less\\nefficient and scales poorly, but it can only solve simple problems.\\nSeveral Bayesian optimization software packages can handle mixed or\\nhierarchical variables with such a subproblem approach. The\\npackages include BoTorch \\n            \",\n        \"\\n            Fig. 6 compares the four EGO methods implemented in SMT 2.0:\\nSMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that\\nillustrates Bayesian optimization is a toy test case \\n            \",\n        \"\\n            In Fig. 7 we illustrate how to use EGO with mixed variables based\\non the implementation of SMT 2.0. The illustrated problem is a mixed\\nvariant of the Branin function \\n            \",\n        \"\\n            Note that a dedicated notebook is available to reproduce the results\\npresented in this paper and the mixed integer notebook also includes\\nan extra mechanical application with composite materials \\n            \",\n        \"\\n            The hierarchical test case considered in this paper to illustrate\\nBayesian optimization is a modified Goldstein function \\n            \",\n        \"\\n          The new release SMT 2.0 introduces several improvements\\nbesides Kriging for hierarchical and mixed variables. This section details\\nthe most important new contributions. Recall from Section 2.2 that\\nfive sub-modules are present in the code: Sampling, Problems,\\nSurrogate Models, Applications and Notebooks.\\n6.1. Contributions to Sampling\\nPseudo-random sampling. The Latin Hypercube Sampling (LHS) is a\\nstochastic sampling technique to generate quasi-random sampling\\ndistributions. It is among the most popular sampling method in computer\\nexperiments thanks to its simplicity and projection properties with\\nhigh-dimensional problems. The LHS method uses the pyDOE package\\n(Design Of Experiments for Python). Five criteria for the construction\\nof LHS are implemented in SMT. The first four criteria (center,\\nmaximin, centermaximin, correlation) are the same as in\\npyDOE.12 The last criterion ese, is implemented by the authors of\\nSMT \\n          \",\n        \"\\n            based on hyperparameters and on a correlation kernel. Four\\ncorrelation kernels are now implemented in SMT 2.0 \\n            \",\n        \"\\n            Noisy Kriging. In engineering and in big data contexts with real\\nexperiments, surrogate models for noisy data are of significant interest. In\\nparticular, there is a growing need for techniques like noisy Kriging\\nand noisy Multi-Fidelity Kriging (MFK) for data fusion \\n            \",\n        \"\\n            problems, the toolbox implements Kriging with partial least squares\\n(KPLS) \\n            \",\n        \"\\n            Marginal Gaussian process. SMT 2.0 implements Marginal Gaussian\\nProcess (MGP) surrogate models for high dimensional problems \\n            \",\n        \"\\n            Gradient-enhanced neural network. The new release SMT 2.0\\nimplements Gradient-Enhanced Neural Network (GENN) models \\n            \",\n        \"\\n            Parallel Bayesian optimization. Due to the recent progress made in\\nhardware configurations, it has been of high interest to perform parallel\\noptimizations. A parallel criterion called qEI \\n            \"\n      ]\n    },\n    {\n      \"title\": \"2. SMT 2.0 : an improved surrogate modeling toolbox\",\n      \"paragraphs\": [\n        \"From a software point of view, SMT 2.0 maintains and improves\\nthe modularity and generality of the original SMT version 5. In this\\nsection, we describe the software as follows. Section 2.1 describes the\\nlegacy of SMT 0.2. Then, Section 2.2 describes the organization of the\\nrepository. Finally, Section 2.3 shows the new capabilities implemented\\nin the SMT 2.0 update.\",\n        \"SMT 5 is an open-source collaborative work originally developed\\nby ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of\\nMichigan. Now, both Polytechnique Montr\\u00e9al and the University of\\nCalifornia San Diego are also contributors. SMT 2.0 updates and\\nextends the original SMT repository capabilities among which the original\\npublication 5 focuses on different types of derivatives for surrogate\\nmodels detailed hereafter.\\n3 http://smt.readthedocs.io/en/latest\\n4 https://github.com/SMTorg/smt\\n5 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\",\n        \"\\n            original main motivations for SMT was derivative support. In fact, none\\nof the existing packages for surrogate modeling such as Scikit-learn in\\nPython 7, SUMO in Matlab \\n            \",\n        \"\\n            nized along three main sub-modules that implement a set of sampling\\ntechniques (sampling_methods), benchmarking functions (problems),\\nand surrogate modeling techniques (surrogate_models). The toolbox\\ndocumentation6 is created using reStructuredText and Sphinx, a\\ndocumentation generation package for Python, with custom extensions.\\nCode snippets in the documentation pages are taken directly from\\nactual tests in the source code and are automatically updated. The\\noutput from these code snippets and tables of options are generated\\ndynamically by custom Sphinx extensions. This leads to high-quality\\ndocumentation with minimal effort. Along with user documentation,\\ndeveloper documentation is also provided to explain how to contribute\\nto SMT. This includes a list of API methods for the SurrogateModel,\\nSamplingMethod, and Problem classes, that must be implemented\\nto create a new surrogate modeling method, sampling technique, or\\nbenchmarking problem. When a developer submits a pull request, it is\\nmerged only after passing the automated tests and receiving approval\\nfrom at least one reviewer. The repository on GitHub7 is linked to\\ncontinuous integration tests (GitHub Actions) for Windows, Linux and\\nMacOS, to a coverage test on coveralls.io and to a dependency version\\ncheck for Python with DependaBot. Various parts of the source code\\nhave been accelerated using Numba \\n            \",\n        \"The main features of the open-source repository SMT 2.0 are\\ndescribed in Fig. 1. More precisely, Sampling Methods, Problems\\nand Surrogate models are kept from SMT 0.2 and two new\\nsections Models applications and Interactive notebooks\\nhave been added to the architecture of the code. These sections are\\n6 https://smt.readthedocs.org\\n7 https://github.com/SMTorg/smt\\nhighlighted in blue and detailed on Fig. 1. The new major features\\nimplemented in SMT 2.0 are highlighted in lavender whereas the\\nlegacy features that were already in present in the original publication\\nfor SMT 0.2 5 are in black.\\n2.3. New features within SMT 2.0\",\n        \"The main objective of this new release is to enable Kriging surrogate\\nmodels for use with both hierarchical and mixed variables. Moreover,\\nfor each of these five sub-modules described in Section 2.2, several\\nimprovements have been made between the original version and the\\nSMT 2.0 release.\",\n        \"\\n            Hierarchical and mixed design space. A new design space definition\\nclass DesignSpace has been added that implements hierarchical\\nand mixed functionalities. Design variables can either be\\ncontinuous (FloatVariable), ordered (OrdinalVariable) or categorical\\n(CategoricalVariable). The integer type (IntegerVariable)\\nrepresents a special case of the ordered variable, specified by bounds\\n(inclusive) rather than a list of possible values. The hierarchical\\nstructure of the design space can be defined using declare_decreed_var:\\nthis function declares that a variable is a decreed variable that is\\nactivated when the associated meta variable takes one of a set of\\nspecified values, see Section 4 for background. The DesignSpace\\nclass also implements mechanisms for sampling valid design vectors\\n(i.e. design vectors that adhere to the hierarchical structure of the\\ndesign space) using any of the below-mentioned samplers, for\\ncorrecting and imputing design vectors, and for requesting which design\\nvariables are acting in a given design vector. Correction ensures that\\nvariables have valid values (e.g. integers for discrete variables) \\n            \",\n        \"\\n            Sampling. SMT implements three methods for sampling. The first one\\nis a na\\u00efve approach, called Random that draws uniformly points along\\nevery dimension. The second sampling method is called Full\\nFactorial and draws a point for every cross combination of variables,\\nto have an \\u2018\\u2018exhaustive\\u2019\\u2019 design of experiments. The last one is the\\nLatin Hypercube Sampling (LHS) \\n            \",\n        \"Problems. SMT implements two new engineering problems: a mixed\\nvariant of a cantilever beam described in Section 3 and a hierarchical\\nneural network described in Section 4.\",\n        \"\\n            Surrogate models. In order to keep up with state-of-art, several\\nreleases done from the original version developed new options for the\\nalready existing surrogates. In particular, compared to the original\\npublication 5, SMT 2.0 adds gradient-enhanced neural networks \\n            \",\n        \"In the following, Section 3 details the Kriging based surrogate\\nmodels for mixed variables, and Section 4 presents our new Kriging\\nsurrogate for hierarchical variables. Section 5 details the EGO\\napplication and the other new relevant features aforementioned are described\\nsuccinctly in Section 6.\",\n        \"8 https://github.com/SMTorg/smt/tree/master/tutorial\\n9 https://colab.research.google.com/github/SMTorg/smt/\\n10 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\",\n        \"As mentioned in Section 1, design variables can be either of\\ncontinuous or discrete type, and a problem with both types is a mixed-variable\\nproblem. Discrete variables can be ordinal or categorical. A discrete\\nvariable is ordinal if there is an order relation within the set of possible\\nvalues. An example of an ordinal design variable is the number of\\nengines in an aircraft. A possible set of values in this case could be\\n2, 4, 8. A discrete variable is categorical if no order relation is known\\nbetween the possible choices the variable can take. One example of a\\ncategorical variable is the color of a surface. A possible example of a\\nset of choices could be blue, red, green. The possible choices are called\\nthe levels of the variable.\",\n        \"\\n            Several methods have been proposed to address the recent increase\\ninterest in mixed Kriging based models \\n            \",\n        \"The continuous and ordinal variables are both treated similarly\\nin SMT 2.0 with a continuous kernel, where the ordinal values are\\nconverted to continuous through relaxation. For categorical variables,\\nfour models (GD, CR, EHH and HH) can be used in SMT 2.0 if\\nspecified by the API. This is why we developed a unified mathematical\\nformulation that allows a unique implementation for any model.\",\n        \"Denote  the number of categorical variables. For a given  \\u2208\\n{1, \\u2026 , }, the th categorical variable is denoted  and its number\\nof levels is denoted . The hyperparameter matrix peculiar to this\\nvariable  is\",\n        \"\\u23a1 1,1 . \\u23a4\\n = \\u23a2\\u23a2\\u23a2 \\u22ee1,2 \\u22f12,2 \\u22f1 \\u23a5\\u23a5\\u23a5 ,\",\n        \"\\n            \\u23a3\\u23a21, \\u2026 \\u22121, , \\u23a6\\u23a5\\nand the categorical parameters are defined as  = {1, \\u2026 , }. For\\ntwo given inputs in the DoE, for example, the th and th points, let\\n and  be the associated categorical variables taking respectively\\nthe  and the  level on the categorical variable . The categorical\\n\\ncorrelation kernel is defined by\\n(, , ) =\\n\\n\\u220f ((), ) ((), )((), )((), )\\n=1        \\n(1)\\nwhere  is either a positive definite kernel or identity and (.) is a\\nsymmetric positive definite (SPD) function such that the matrix ()\\nis SPD if  is SPD. For an exponential kernel, Table 3 gives the\\nparameterizations of  and  that correspond to GD, CR, HH, and\\nEHH kernels. The complexity of these different kernels depends on\\nthe number of hyperparameters that characterizes them. As defined\\nby Saves et al. \\n            \",\n        \"\\n            Another Kriging based model that can use mixed variables is Kriging\\nwith partial least squares (KPLS) \\n            \",\n        \"\\n            A classic engineering problem commonly used for model validation\\nis the beam bending problem \\n            \",\n        \"\\n            To compare the mixed Kriging models of SMT 2.0, we draw a 98\\npoint LHS as training set and the validation set is a grid of 12 \\u00d7 30 \\u00d7\\n30 = 10800 points. For the four implemented methods, displacement\\nerror (computed with a root-mean-square error criterion), likelihood,\\nnumber of hyperparameters and computational time for every model\\nare shown in Table 4. For the continuous variables, we use the square\\nexponential kernel. More details are found in \\n            \"\n      ]\n    },\n    {\n      \"title\": \"2.1. Background on SMT former version: SMT 0.2\",\n      \"paragraphs\": [\n        \"SMT 5 is an open-source collaborative work originally developed\\nby ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of\\nMichigan. Now, both Polytechnique Montr\\u00e9al and the University of\\nCalifornia San Diego are also contributors. SMT 2.0 updates and\\nextends the original SMT repository capabilities among which the original\\npublication 5 focuses on different types of derivatives for surrogate\\nmodels detailed hereafter.\\n3 http://smt.readthedocs.io/en/latest\\n4 https://github.com/SMTorg/smt\\n5 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\"\n      ]\n    },\n    {\n      \"title\": \"A Python surrogate modeling framework with derivatives. One of the\",\n      \"paragraphs\": [\n        \"\\n            original main motivations for SMT was derivative support. In fact, none\\nof the existing packages for surrogate modeling such as Scikit-learn in\\nPython 7, SUMO in Matlab \\n            \"\n      ]\n    },\n    {\n      \"title\": \"Software architecture, documentation, and automatic testing. SMT is orga\",\n      \"paragraphs\": [\n        \"\\n            nized along three main sub-modules that implement a set of sampling\\ntechniques (sampling_methods), benchmarking functions (problems),\\nand surrogate modeling techniques (surrogate_models). The toolbox\\ndocumentation6 is created using reStructuredText and Sphinx, a\\ndocumentation generation package for Python, with custom extensions.\\nCode snippets in the documentation pages are taken directly from\\nactual tests in the source code and are automatically updated. The\\noutput from these code snippets and tables of options are generated\\ndynamically by custom Sphinx extensions. This leads to high-quality\\ndocumentation with minimal effort. Along with user documentation,\\ndeveloper documentation is also provided to explain how to contribute\\nto SMT. This includes a list of API methods for the SurrogateModel,\\nSamplingMethod, and Problem classes, that must be implemented\\nto create a new surrogate modeling method, sampling technique, or\\nbenchmarking problem. When a developer submits a pull request, it is\\nmerged only after passing the automated tests and receiving approval\\nfrom at least one reviewer. The repository on GitHub7 is linked to\\ncontinuous integration tests (GitHub Actions) for Windows, Linux and\\nMacOS, to a coverage test on coveralls.io and to a dependency version\\ncheck for Python with DependaBot. Various parts of the source code\\nhave been accelerated using Numba \\n            \",\n        \"The main features of the open-source repository SMT 2.0 are\\ndescribed in Fig. 1. More precisely, Sampling Methods, Problems\\nand Surrogate models are kept from SMT 0.2 and two new\\nsections Models applications and Interactive notebooks\\nhave been added to the architecture of the code. These sections are\\n6 https://smt.readthedocs.org\\n7 https://github.com/SMTorg/smt\\nhighlighted in blue and detailed on Fig. 1. The new major features\\nimplemented in SMT 2.0 are highlighted in lavender whereas the\\nlegacy features that were already in present in the original publication\\nfor SMT 0.2 5 are in black.\\n2.3. New features within SMT 2.0\",\n        \"The main objective of this new release is to enable Kriging surrogate\\nmodels for use with both hierarchical and mixed variables. Moreover,\\nfor each of these five sub-modules described in Section 2.2, several\\nimprovements have been made between the original version and the\\nSMT 2.0 release.\",\n        \"\\n            Hierarchical and mixed design space. A new design space definition\\nclass DesignSpace has been added that implements hierarchical\\nand mixed functionalities. Design variables can either be\\ncontinuous (FloatVariable), ordered (OrdinalVariable) or categorical\\n(CategoricalVariable). The integer type (IntegerVariable)\\nrepresents a special case of the ordered variable, specified by bounds\\n(inclusive) rather than a list of possible values. The hierarchical\\nstructure of the design space can be defined using declare_decreed_var:\\nthis function declares that a variable is a decreed variable that is\\nactivated when the associated meta variable takes one of a set of\\nspecified values, see Section 4 for background. The DesignSpace\\nclass also implements mechanisms for sampling valid design vectors\\n(i.e. design vectors that adhere to the hierarchical structure of the\\ndesign space) using any of the below-mentioned samplers, for\\ncorrecting and imputing design vectors, and for requesting which design\\nvariables are acting in a given design vector. Correction ensures that\\nvariables have valid values (e.g. integers for discrete variables) \\n            \",\n        \"\\n            Sampling. SMT implements three methods for sampling. The first one\\nis a na\\u00efve approach, called Random that draws uniformly points along\\nevery dimension. The second sampling method is called Full\\nFactorial and draws a point for every cross combination of variables,\\nto have an \\u2018\\u2018exhaustive\\u2019\\u2019 design of experiments. The last one is the\\nLatin Hypercube Sampling (LHS) \\n            \",\n        \"Problems. SMT implements two new engineering problems: a mixed\\nvariant of a cantilever beam described in Section 3 and a hierarchical\\nneural network described in Section 4.\",\n        \"\\n            Surrogate models. In order to keep up with state-of-art, several\\nreleases done from the original version developed new options for the\\nalready existing surrogates. In particular, compared to the original\\npublication 5, SMT 2.0 adds gradient-enhanced neural networks \\n            \",\n        \"In the following, Section 3 details the Kriging based surrogate\\nmodels for mixed variables, and Section 4 presents our new Kriging\\nsurrogate for hierarchical variables. Section 5 details the EGO\\napplication and the other new relevant features aforementioned are described\\nsuccinctly in Section 6.\",\n        \"8 https://github.com/SMTorg/smt/tree/master/tutorial\\n9 https://colab.research.google.com/github/SMTorg/smt/\\n10 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\",\n        \"As mentioned in Section 1, design variables can be either of\\ncontinuous or discrete type, and a problem with both types is a mixed-variable\\nproblem. Discrete variables can be ordinal or categorical. A discrete\\nvariable is ordinal if there is an order relation within the set of possible\\nvalues. An example of an ordinal design variable is the number of\\nengines in an aircraft. A possible set of values in this case could be\\n2, 4, 8. A discrete variable is categorical if no order relation is known\\nbetween the possible choices the variable can take. One example of a\\ncategorical variable is the color of a surface. A possible example of a\\nset of choices could be blue, red, green. The possible choices are called\\nthe levels of the variable.\",\n        \"\\n            Several methods have been proposed to address the recent increase\\ninterest in mixed Kriging based models \\n            \"\n      ]\n    },\n    {\n      \"title\": \"3.1. Mixed Gaussian processes\",\n      \"paragraphs\": [\n        \"The continuous and ordinal variables are both treated similarly\\nin SMT 2.0 with a continuous kernel, where the ordinal values are\\nconverted to continuous through relaxation. For categorical variables,\\nfour models (GD, CR, EHH and HH) can be used in SMT 2.0 if\\nspecified by the API. This is why we developed a unified mathematical\\nformulation that allows a unique implementation for any model.\",\n        \"Denote  the number of categorical variables. For a given  \\u2208\\n{1, \\u2026 , }, the th categorical variable is denoted  and its number\\nof levels is denoted . The hyperparameter matrix peculiar to this\\nvariable  is\",\n        \"\\u23a1 1,1 . \\u23a4\\n = \\u23a2\\u23a2\\u23a2 \\u22ee1,2 \\u22f12,2 \\u22f1 \\u23a5\\u23a5\\u23a5 ,\",\n        \"\\n            \\u23a3\\u23a21, \\u2026 \\u22121, , \\u23a6\\u23a5\\nand the categorical parameters are defined as  = {1, \\u2026 , }. For\\ntwo given inputs in the DoE, for example, the th and th points, let\\n and  be the associated categorical variables taking respectively\\nthe  and the  level on the categorical variable . The categorical\\n\\ncorrelation kernel is defined by\\n(, , ) =\\n\\n\\u220f ((), ) ((), )((), )((), )\\n=1        \\n(1)\\nwhere  is either a positive definite kernel or identity and (.) is a\\nsymmetric positive definite (SPD) function such that the matrix ()\\nis SPD if  is SPD. For an exponential kernel, Table 3 gives the\\nparameterizations of  and  that correspond to GD, CR, HH, and\\nEHH kernels. The complexity of these different kernels depends on\\nthe number of hyperparameters that characterizes them. As defined\\nby Saves et al. \\n            \",\n        \"\\n            Another Kriging based model that can use mixed variables is Kriging\\nwith partial least squares (KPLS) \\n            \"\n      ]\n    },\n    {\n      \"title\": \"3.2. An engineering design test-case\",\n      \"paragraphs\": [\n        \"\\n            A classic engineering problem commonly used for model validation\\nis the beam bending problem \\n            \",\n        \"\\n            To compare the mixed Kriging models of SMT 2.0, we draw a 98\\npoint LHS as training set and the validation set is a grid of 12 \\u00d7 30 \\u00d7\\n30 = 10800 points. For the four implemented methods, displacement\\nerror (computed with a root-mean-square error criterion), likelihood,\\nnumber of hyperparameters and computational time for every model\\nare shown in Table 4. For the continuous variables, we use the square\\nexponential kernel. More details are found in \\n            \"\n      ]\n    },\n    {\n      \"title\": \"4. Surrogate models with hierarchical variables in SMT 2.0\",\n      \"paragraphs\": [\n        \"\\n          To introduce the newly developed Kriging model for hierarchical\\nvariables implemented in SMT 2.0, we present the general\\nmathematical framework for hierarchical and mixed variables established\\nby Audet et al. \\n          \",\n        \"\\n            A problem structure is classified as hierarchical when the sets of\\nactive variables depend on architectural choices. This occurs frequently\\nin industrial design problems. In hierarchical problems, we can classify\\nvariables as neutral, meta (also known as dimensional) or decreed\\n(also known as conditionally active) as detailed in Audet et al. \\n            \",\n        \"However, the wing aspect ratio being neutral, it is not affected by this\\nhierarchy.\",\n        \"\\n            Problems involving hierarchical variables are generally dependant\\non discrete architectures and as such involve mixed variables. Hence,\\nin addition to their role (neutral, meta or decreed), each variable also\\nhas a variable type amongst categorical, ordinal or continuous. For the\\nsake of simplicity and because both continuous and ordinal variables\\nare treated similarly \\n            \",\n        \"\\n            To explain the framework and the new Kriging model, we illustrate\\nthe inputs variables of the model using a classical machine\\nlearning problem related to the hyperparameters optimization of a\\nfullyconnected Multi-Layer Perceptron (MLP) \\n            \",\n        \"According to their types, the MLP input variables can be classified as\\nfollows:\\n4. The meta variable \\u2018\\u2018# of hidden layers\\u2019\\u2019 is an integer and, as\\nqnt\\nsuch, is represented by the component met .\\n5. The decreed variables \\u2018\\u2018# of neurons hidden layer \\u2019\\u2019 are integers\\nqnt\\nand, as such, are represented by the component dec.\\n6. The \\u2018\\u2018Learning rate\\u2019\\u2019, \\u2018\\u2018Momentum\\u2019\\u2019, \\u2018\\u2018Activation function\\u2019\\u2019 and\\n\\u2018\\u2018Batch size\\u2019\\u2019 are, respectively, continuous, for the first two\\n(every value between two bounds), categorical (qualitative between\\nthree choices) and integer (quantitative between 6 choices).\\nTherefore, the \\u2018\\u2018Activation function\\u2019\\u2019 and the \\u2018\\u2018Momentum\\u2019\\u2019 are\\nrepresented by the component cnaetu. The \\u2018\\u2018Learning rate\\u2019\\u2019 and the\\nqnt\\n\\u2018\\u2018Batch size\\u2019\\u2019 are represented by the component neu.\",\n        \"\\n            To model hierarchical variables, as proposed in \\n            \",\n        \"In this section, a new method to build a Kriging model with\\nhierarchical variables is introduced based on the framework aforementioned.\\nThe proposed methods are included in SMT 2.0.\",\n        \"\\n            Assuming that the decreed variables are quantitative, Hutter and\\nOsborne \\n            \",\n        \"In the following, we describe our new method to build a correlation\\nkernel for hierarchical variables. In particular, we introduce a new\\nalgebraic kernel called Alg-Kernel that behaves like the Arc-Kernel\\nwhilst correcting most of its drawbacks. In particular, our kernel does\\nnot add any hyperparameters, and the normalization is handled in a\\nnatural way.\",\n        \"\\n            For modeling purposes, we assume that the decreed space is\\nquantitative, i.e., \\ue244dec = \\ue244dqenct . Let  \\u2208 \\ue244 be an input point partitioned as\\n = (neu, met , inc(met )) and, similarly,  \\u2208 \\ue244 is another input such\\nthat  = (neu, met , inc(met )). The new kernel  that we propose for\\nhierarchical variables is given by\\n(, ) = neu(neu, neu) \\u00d7 met (met , met )\\n\\u00d7 met,dec(met , inc(met ), met , inc(met )),\\n(2)\\nwhere neu, met and met,dec are as follows:\\n\\u2022 neu represents the neutral kernel that encompasses both\\ncategorical and quantitative neutral variables, i.e., neu can be\\ndecomposed into two parts neu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, nqenut).\\nqnt\\nThe categorical kernel, denoted cat , could be any Symmetric\\nPositive Definite (SPD) \\n            \",\n        \"qnt (qnt , qnt ) = \\u220f exp(\\u2212(qnt , qnt )).\",\n        \"=1\\n\\u2022 met is the meta variables related kernel. It is also separated into\\ntwo parts: met (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett ) where the\\nquantitative kernel is ordered and not continuous because meta\\nvariables take value in a finite set.\\n\\u2022 met,dec is an SPD kernel that models the correlations between the\\nmeta levels (all the possible subspaces) and the decreed variables.\",\n        \"In what comes next, we detailed this kernel.\",\n        \"\\n            Meta-decreed kernels like the imputation kernel or the\\nArc-Kernel were first proposed in \\n            \",\n        \"Our proposed Alg-Kernel kernel is given by\",\n        \"= amlegt (met , met ) \\u00d7 daelgc(inc(met ), inc(met )).\",\n        \"Mathematically, we could consider that there is only one meta variable\\nwhose levels correspond to every possible included subspace. Let sub\\ndenotes the components indices of possible subspaces, the subspaces\\nparameterized by the meta component met are defined as \\ue244inc(met =\\n),  \\u2208 sub. It follows that the fully extended continuous decreed\\nspace writes as \\ue244dec = \\u22c3\\u2208sub \\ue244inc(met = ) and dec is the set of the\\nassociated indices. Let  denotes the set of components related to\\n,\\nthe space \\ue244inc(met , met ) containing the variables decreed-included in\\nboth \\ue244inc(met ) and \\ue244inc(met ).\",\n        \"Since the decreed variables are quantitative, one has\\ndaelgc(inc(met ), inc(met )) = qnt (inc(met ), inc(met ))\\n= \\u220f qnt (inc(met ), inc(met ))\\n(4)\\n(5)\\n\\u2208,\\nThe construction of the quantitative kernel qnt depends on a given\\ndistance denoted alg. The kernel amlegt is an induced meta kernel that\\ndepends on the same distance alg to preserve the SPD property of\\nalg\\nmet,dec. For every  \\u2208 dec, if  \\u2208 , the new algebraic distance is\\n,\\ngiven by\\nalg(inc(met ), inc(met )) = \\u239c\\u239c\\u239b \\u221a 2|inc(met ) \\u2212\\u221ainc(met )| \\u239f\\u239f\\u239e ,\\n\\u239c inc(met )2 + 1 inc(met )2 + 1 \\u239f\\n\\u239d \\u23a0\\nwhere  \\u2208 R+ is a continuous hyperparameter. Otherwise, if  \\u2208 dec\\nbut  \\u2209 , there should be a non-zero residual distance between the\\n,\\ntwo different subspaces \\ue244inc(met ) and \\ue244inc(met ) to ensure the kernel\\nSPD property. To have a residual not depending on the decreed values,\\nour model considers that there is a unit distance\\nalg(inc(met ), inc(met )) = 1.0 , \\u2200 \\u2208 dec \\u29f5 .\\n,\\nThe induced meta kernel amlegt (met , met ) to preserve the SPD property\\nof alg is defined as:\\n(6)\\n(7)\\n(3)\\namlegt (met , met ) = \\u220f qnt (1.0 ).\",\n        \"\\u2208met\",\n        \"\\n            Not only our kernel of Eq. (2) uses less hyperparameters than the\\nArcKernel (as we cut off its extra parameters) but it is also a more flexible\\nkernel as it allows different kernels for meta and decreed variables.\\nMoreover, another advantage of our kernel is that it is numerically\\nmore stable thanks to the new non-stationary \\n            \",\n        \"In what comes next, we will refer to the implementation of the\\nkernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and\\nSMT Alg-Kernel . We note also that the implementation of SMT\\nArc-Kernel differs slightly from the original Arc-Kernel as we\\nfixed some hyperparameters to 1 in order to avoid adding extra\\nhyperparameters and use the formulation of Eq. (2) and rescaling of the\\ndata.\",\n        \"In this section, we illustrate the hierarchical Arc-Kernel on the\\nMLP example. For that sake, we consider two design variables  and\\n such that  = (2.10\\u22124, 0.9, ReLU, 16, 2, 55, 51) and  = (5.10\\u22123, 0.8,\\nSigmoid, 64, 3, 50, 54, 53). Since the value of met (i.e., the number of\\nhidden layers) differs from one point to another (namely, 2 for  and 3\\nfor ), the associated variables inc(met ) have either 2 or 3 variables\\nfor the number of neurons in each layer (namely 55 and 51 for ,\\nand 50, 54 and 53 for the point ). In our case, 8\\nhyperparameters (11,2, 1, \\u2026 , 7) will have to be optimized where  is given by\\nEq. (2). These 7 hyperparameters can be described using our proposed\\nframework as follows:\\n\\u2022 For the neutral components, we have neu = (2.10\\u22124, 0.9, ReLU, 16)\\nand neu = (5.10\\u22123, 0.8, Sigmoid, 64). Therefore, for a categorical\\nmatrix kernel 1 and a square exponential quantitative kernel,\\nqnt qnt\\nneu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, neu)\\n= 11,2 exp \\u22121(2.10\\u22124 \\u2212 5.10\\u22123)2\",\n        \"exp \\u22122(0.9 \\u2212 0.8)2 exp \\u22123(16 \\u2212 64)2.\",\n        \"The values 11,2, 1, 2 and 3 need to be optimized. Here,\\n11,2 is the correlation between \\\"ReLU\\\" and \\\"Sigmoid\\\".\\n\\u2022 For the meta components, we have met = 2 and met = 3.\",\n        \"Therefore, for a square exponential quantitative kernel,\\nmet (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett )\",\n        \"= exp \\u22124(3 \\u2212 2)2.\",\n        \"The value 4 needs to be optimized.\\n\\u2022 For the meta-decreed kernel, we have met , inc(met ) = 2, (55, 51)\\nand met , inc(met ) = 3, (50, 54, 53) which gives\\nalg\\nwhere the meta induced component is met (met , met ) = exp \\u22127\\nbecause the decreed value 53 in  has nothing to be compared\\nwith in  as in Eq. (7). The values 5, 6 and 7 need to be\\noptimized which complete the description of the hyperparameters.\\nWe note that for the MLP problem, Alg-Kernel models use\\n10 hyperparameters whereas the Arc-Kernel would require\\n12 hyperparameters without the meta kernel (4) but with 3\\nextra decreed hyperparameters and the Wedge-Kernel would\\nrequire 15 hyperparameters. For deep learning applications, a\\nmore complex perceptron with up to 10 hidden layers would\\nrequire 17 hyperparameters with SMT 2.0 models against 26\\nfor Arc-Kernel and 36 for Wedge-Kernel . The next section\\nillustrates the interest of our method to build a surrogate model\\nfor this neural network engineering problem.\\n4.3. A neural network test-case using SMT 2.0\",\n        \"In this section, we apply our models to the hyperparameters\\noptimization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0\\nan example illustrates this MLP problem. For the sake of showing the\\nKriging surrogate abilities, we implemented a dummy function with no\\nsignificance to replace the real black-box that would require training\\na whole Neural Network (NN) with big data. This function requires a\\nnumber of variables that depends on the value of the meta variable,\\ni.e the number of hidden layers. To simplify, we have chosen only\\n1, 2 or 3 hidden layers and therefore, we have 3 decreed variables\\nbut deep neural networks could also be investigated as our model can\\ntackle a few dozen variables. A test case (test_hierarchical_variables_NN )\\nshows that our model SMT Alg-Kernel interpolates the data\\nproperly, checks that the data dimension is correct and also asserts that\\nthe inactive decreed variables have no influence over the prediction.\\nIn Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical\\nand mixed variables based on the implementation of SMT 2.0 for\\ntest_hierarchical_variables_NN.\",\n        \"\\n            To compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel\\nand SMT Arc-Kernel ) with the state-of-the-art imputation method\\npreviously used on industrial application (Imp-Kernel ) \\n            \",\n        \"\\n            Efficient global optimization (EGO) is a sequential Bayesian\\noptimization algorithm designed to find the optimum of a black-box\\nfunction that may be expensive to evaluate \\n            \",\n        \"\\n            Because SMT 2.0 implements Kriging models that handle mixed\\nand hierarchical variables, we can use EGO to solve problems\\ninvolving such design variables. Other Bayesian optimization algorithms\\noften adopt approaches based on solving subproblems with\\ncontinuous or non-hierarchical Kriging. This subproblem approach is less\\nefficient and scales poorly, but it can only solve simple problems.\\nSeveral Bayesian optimization software packages can handle mixed or\\nhierarchical variables with such a subproblem approach. The\\npackages include BoTorch \\n            \",\n        \"\\n            Fig. 6 compares the four EGO methods implemented in SMT 2.0:\\nSMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that\\nillustrates Bayesian optimization is a toy test case \\n            \",\n        \"\\n            In Fig. 7 we illustrate how to use EGO with mixed variables based\\non the implementation of SMT 2.0. The illustrated problem is a mixed\\nvariant of the Branin function \\n            \",\n        \"\\n            Note that a dedicated notebook is available to reproduce the results\\npresented in this paper and the mixed integer notebook also includes\\nan extra mechanical application with composite materials \\n            \",\n        \"\\n            The hierarchical test case considered in this paper to illustrate\\nBayesian optimization is a modified Goldstein function \\n            \"\n      ]\n    },\n    {\n      \"title\": \"4.1. The hierarchical variables framework\",\n      \"paragraphs\": [\n        \"\\n            A problem structure is classified as hierarchical when the sets of\\nactive variables depend on architectural choices. This occurs frequently\\nin industrial design problems. In hierarchical problems, we can classify\\nvariables as neutral, meta (also known as dimensional) or decreed\\n(also known as conditionally active) as detailed in Audet et al. \\n            \",\n        \"However, the wing aspect ratio being neutral, it is not affected by this\\nhierarchy.\",\n        \"\\n            Problems involving hierarchical variables are generally dependant\\non discrete architectures and as such involve mixed variables. Hence,\\nin addition to their role (neutral, meta or decreed), each variable also\\nhas a variable type amongst categorical, ordinal or continuous. For the\\nsake of simplicity and because both continuous and ordinal variables\\nare treated similarly \\n            \",\n        \"\\n            To explain the framework and the new Kriging model, we illustrate\\nthe inputs variables of the model using a classical machine\\nlearning problem related to the hyperparameters optimization of a\\nfullyconnected Multi-Layer Perceptron (MLP) \\n            \",\n        \"According to their types, the MLP input variables can be classified as\\nfollows:\\n4. The meta variable \\u2018\\u2018# of hidden layers\\u2019\\u2019 is an integer and, as\\nqnt\\nsuch, is represented by the component met .\\n5. The decreed variables \\u2018\\u2018# of neurons hidden layer \\u2019\\u2019 are integers\\nqnt\\nand, as such, are represented by the component dec.\\n6. The \\u2018\\u2018Learning rate\\u2019\\u2019, \\u2018\\u2018Momentum\\u2019\\u2019, \\u2018\\u2018Activation function\\u2019\\u2019 and\\n\\u2018\\u2018Batch size\\u2019\\u2019 are, respectively, continuous, for the first two\\n(every value between two bounds), categorical (qualitative between\\nthree choices) and integer (quantitative between 6 choices).\\nTherefore, the \\u2018\\u2018Activation function\\u2019\\u2019 and the \\u2018\\u2018Momentum\\u2019\\u2019 are\\nrepresented by the component cnaetu. The \\u2018\\u2018Learning rate\\u2019\\u2019 and the\\nqnt\\n\\u2018\\u2018Batch size\\u2019\\u2019 are represented by the component neu.\",\n        \"\\n            To model hierarchical variables, as proposed in \\n            \",\n        \"In this section, a new method to build a Kriging model with\\nhierarchical variables is introduced based on the framework aforementioned.\\nThe proposed methods are included in SMT 2.0.\"\n      ]\n    },\n    {\n      \"title\": \"4.2.1. Motivation and state-of-the-art\",\n      \"paragraphs\": [\n        \"\\n            Assuming that the decreed variables are quantitative, Hutter and\\nOsborne \\n            \",\n        \"In the following, we describe our new method to build a correlation\\nkernel for hierarchical variables. In particular, we introduce a new\\nalgebraic kernel called Alg-Kernel that behaves like the Arc-Kernel\\nwhilst correcting most of its drawbacks. In particular, our kernel does\\nnot add any hyperparameters, and the normalization is handled in a\\nnatural way.\"\n      ]\n    },\n    {\n      \"title\": \"4.2.2. A new hierarchical correlation kernel\",\n      \"paragraphs\": [\n        \"\\n            For modeling purposes, we assume that the decreed space is\\nquantitative, i.e., \\ue244dec = \\ue244dqenct . Let  \\u2208 \\ue244 be an input point partitioned as\\n = (neu, met , inc(met )) and, similarly,  \\u2208 \\ue244 is another input such\\nthat  = (neu, met , inc(met )). The new kernel  that we propose for\\nhierarchical variables is given by\\n(, ) = neu(neu, neu) \\u00d7 met (met , met )\\n\\u00d7 met,dec(met , inc(met ), met , inc(met )),\\n(2)\\nwhere neu, met and met,dec are as follows:\\n\\u2022 neu represents the neutral kernel that encompasses both\\ncategorical and quantitative neutral variables, i.e., neu can be\\ndecomposed into two parts neu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, nqenut).\\nqnt\\nThe categorical kernel, denoted cat , could be any Symmetric\\nPositive Definite (SPD) \\n            \",\n        \"qnt (qnt , qnt ) = \\u220f exp(\\u2212(qnt , qnt )).\",\n        \"=1\\n\\u2022 met is the meta variables related kernel. It is also separated into\\ntwo parts: met (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett ) where the\\nquantitative kernel is ordered and not continuous because meta\\nvariables take value in a finite set.\\n\\u2022 met,dec is an SPD kernel that models the correlations between the\\nmeta levels (all the possible subspaces) and the decreed variables.\",\n        \"In what comes next, we detailed this kernel.\"\n      ]\n    },\n    {\n      \"title\": \"4.2.3. Towards an algebraic meta-decreed kernel\",\n      \"paragraphs\": [\n        \"\\n            Meta-decreed kernels like the imputation kernel or the\\nArc-Kernel were first proposed in \\n            \",\n        \"Our proposed Alg-Kernel kernel is given by\",\n        \"= amlegt (met , met ) \\u00d7 daelgc(inc(met ), inc(met )).\",\n        \"Mathematically, we could consider that there is only one meta variable\\nwhose levels correspond to every possible included subspace. Let sub\\ndenotes the components indices of possible subspaces, the subspaces\\nparameterized by the meta component met are defined as \\ue244inc(met =\\n),  \\u2208 sub. It follows that the fully extended continuous decreed\\nspace writes as \\ue244dec = \\u22c3\\u2208sub \\ue244inc(met = ) and dec is the set of the\\nassociated indices. Let  denotes the set of components related to\\n,\\nthe space \\ue244inc(met , met ) containing the variables decreed-included in\\nboth \\ue244inc(met ) and \\ue244inc(met ).\",\n        \"Since the decreed variables are quantitative, one has\\ndaelgc(inc(met ), inc(met )) = qnt (inc(met ), inc(met ))\\n= \\u220f qnt (inc(met ), inc(met ))\\n(4)\\n(5)\\n\\u2208,\\nThe construction of the quantitative kernel qnt depends on a given\\ndistance denoted alg. The kernel amlegt is an induced meta kernel that\\ndepends on the same distance alg to preserve the SPD property of\\nalg\\nmet,dec. For every  \\u2208 dec, if  \\u2208 , the new algebraic distance is\\n,\\ngiven by\\nalg(inc(met ), inc(met )) = \\u239c\\u239c\\u239b \\u221a 2|inc(met ) \\u2212\\u221ainc(met )| \\u239f\\u239f\\u239e ,\\n\\u239c inc(met )2 + 1 inc(met )2 + 1 \\u239f\\n\\u239d \\u23a0\\nwhere  \\u2208 R+ is a continuous hyperparameter. Otherwise, if  \\u2208 dec\\nbut  \\u2209 , there should be a non-zero residual distance between the\\n,\\ntwo different subspaces \\ue244inc(met ) and \\ue244inc(met ) to ensure the kernel\\nSPD property. To have a residual not depending on the decreed values,\\nour model considers that there is a unit distance\\nalg(inc(met ), inc(met )) = 1.0 , \\u2200 \\u2208 dec \\u29f5 .\\n,\\nThe induced meta kernel amlegt (met , met ) to preserve the SPD property\\nof alg is defined as:\\n(6)\\n(7)\\n(3)\\namlegt (met , met ) = \\u220f qnt (1.0 ).\",\n        \"\\u2208met\",\n        \"\\n            Not only our kernel of Eq. (2) uses less hyperparameters than the\\nArcKernel (as we cut off its extra parameters) but it is also a more flexible\\nkernel as it allows different kernels for meta and decreed variables.\\nMoreover, another advantage of our kernel is that it is numerically\\nmore stable thanks to the new non-stationary \\n            \",\n        \"In what comes next, we will refer to the implementation of the\\nkernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and\\nSMT Alg-Kernel . We note also that the implementation of SMT\\nArc-Kernel differs slightly from the original Arc-Kernel as we\\nfixed some hyperparameters to 1 in order to avoid adding extra\\nhyperparameters and use the formulation of Eq. (2) and rescaling of the\\ndata.\"\n      ]\n    },\n    {\n      \"title\": \"4.2.4. Illustration on the MLP problem\",\n      \"paragraphs\": [\n        \"In this section, we illustrate the hierarchical Arc-Kernel on the\\nMLP example. For that sake, we consider two design variables  and\\n such that  = (2.10\\u22124, 0.9, ReLU, 16, 2, 55, 51) and  = (5.10\\u22123, 0.8,\\nSigmoid, 64, 3, 50, 54, 53). Since the value of met (i.e., the number of\\nhidden layers) differs from one point to another (namely, 2 for  and 3\\nfor ), the associated variables inc(met ) have either 2 or 3 variables\\nfor the number of neurons in each layer (namely 55 and 51 for ,\\nand 50, 54 and 53 for the point ). In our case, 8\\nhyperparameters (11,2, 1, \\u2026 , 7) will have to be optimized where  is given by\\nEq. (2). These 7 hyperparameters can be described using our proposed\\nframework as follows:\\n\\u2022 For the neutral components, we have neu = (2.10\\u22124, 0.9, ReLU, 16)\\nand neu = (5.10\\u22123, 0.8, Sigmoid, 64). Therefore, for a categorical\\nmatrix kernel 1 and a square exponential quantitative kernel,\\nqnt qnt\\nneu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, neu)\\n= 11,2 exp \\u22121(2.10\\u22124 \\u2212 5.10\\u22123)2\",\n        \"exp \\u22122(0.9 \\u2212 0.8)2 exp \\u22123(16 \\u2212 64)2.\",\n        \"The values 11,2, 1, 2 and 3 need to be optimized. Here,\\n11,2 is the correlation between \\\"ReLU\\\" and \\\"Sigmoid\\\".\\n\\u2022 For the meta components, we have met = 2 and met = 3.\",\n        \"Therefore, for a square exponential quantitative kernel,\\nmet (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett )\",\n        \"= exp \\u22124(3 \\u2212 2)2.\",\n        \"The value 4 needs to be optimized.\\n\\u2022 For the meta-decreed kernel, we have met , inc(met ) = 2, (55, 51)\\nand met , inc(met ) = 3, (50, 54, 53) which gives\\nalg\\nwhere the meta induced component is met (met , met ) = exp \\u22127\\nbecause the decreed value 53 in  has nothing to be compared\\nwith in  as in Eq. (7). The values 5, 6 and 7 need to be\\noptimized which complete the description of the hyperparameters.\\nWe note that for the MLP problem, Alg-Kernel models use\\n10 hyperparameters whereas the Arc-Kernel would require\\n12 hyperparameters without the meta kernel (4) but with 3\\nextra decreed hyperparameters and the Wedge-Kernel would\\nrequire 15 hyperparameters. For deep learning applications, a\\nmore complex perceptron with up to 10 hidden layers would\\nrequire 17 hyperparameters with SMT 2.0 models against 26\\nfor Arc-Kernel and 36 for Wedge-Kernel . The next section\\nillustrates the interest of our method to build a surrogate model\\nfor this neural network engineering problem.\\n4.3. A neural network test-case using SMT 2.0\",\n        \"In this section, we apply our models to the hyperparameters\\noptimization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0\\nan example illustrates this MLP problem. For the sake of showing the\\nKriging surrogate abilities, we implemented a dummy function with no\\nsignificance to replace the real black-box that would require training\\na whole Neural Network (NN) with big data. This function requires a\\nnumber of variables that depends on the value of the meta variable,\\ni.e the number of hidden layers. To simplify, we have chosen only\\n1, 2 or 3 hidden layers and therefore, we have 3 decreed variables\\nbut deep neural networks could also be investigated as our model can\\ntackle a few dozen variables. A test case (test_hierarchical_variables_NN )\\nshows that our model SMT Alg-Kernel interpolates the data\\nproperly, checks that the data dimension is correct and also asserts that\\nthe inactive decreed variables have no influence over the prediction.\\nIn Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical\\nand mixed variables based on the implementation of SMT 2.0 for\\ntest_hierarchical_variables_NN.\",\n        \"\\n            To compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel\\nand SMT Arc-Kernel ) with the state-of-the-art imputation method\\npreviously used on industrial application (Imp-Kernel ) \\n            \",\n        \"\\n            Efficient global optimization (EGO) is a sequential Bayesian\\noptimization algorithm designed to find the optimum of a black-box\\nfunction that may be expensive to evaluate \\n            \",\n        \"\\n            Because SMT 2.0 implements Kriging models that handle mixed\\nand hierarchical variables, we can use EGO to solve problems\\ninvolving such design variables. Other Bayesian optimization algorithms\\noften adopt approaches based on solving subproblems with\\ncontinuous or non-hierarchical Kriging. This subproblem approach is less\\nefficient and scales poorly, but it can only solve simple problems.\\nSeveral Bayesian optimization software packages can handle mixed or\\nhierarchical variables with such a subproblem approach. The\\npackages include BoTorch \\n            \"\n      ]\n    },\n    {\n      \"title\": \"5.1. A mixed optimization problem\",\n      \"paragraphs\": [\n        \"\\n            Fig. 6 compares the four EGO methods implemented in SMT 2.0:\\nSMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that\\nillustrates Bayesian optimization is a toy test case \\n            \",\n        \"\\n            In Fig. 7 we illustrate how to use EGO with mixed variables based\\non the implementation of SMT 2.0. The illustrated problem is a mixed\\nvariant of the Branin function \\n            \",\n        \"\\n            Note that a dedicated notebook is available to reproduce the results\\npresented in this paper and the mixed integer notebook also includes\\nan extra mechanical application with composite materials \\n            \",\n        \"\\n            The hierarchical test case considered in this paper to illustrate\\nBayesian optimization is a modified Goldstein function \\n            \"\n      ]\n    },\n    {\n      \"title\": \"6. Other relevant contributions in SMT 2.0\",\n      \"paragraphs\": [\n        \"\\n          The new release SMT 2.0 introduces several improvements\\nbesides Kriging for hierarchical and mixed variables. This section details\\nthe most important new contributions. Recall from Section 2.2 that\\nfive sub-modules are present in the code: Sampling, Problems,\\nSurrogate Models, Applications and Notebooks.\\n6.1. Contributions to Sampling\\nPseudo-random sampling. The Latin Hypercube Sampling (LHS) is a\\nstochastic sampling technique to generate quasi-random sampling\\ndistributions. It is among the most popular sampling method in computer\\nexperiments thanks to its simplicity and projection properties with\\nhigh-dimensional problems. The LHS method uses the pyDOE package\\n(Design Of Experiments for Python). Five criteria for the construction\\nof LHS are implemented in SMT. The first four criteria (center,\\nmaximin, centermaximin, correlation) are the same as in\\npyDOE.12 The last criterion ese, is implemented by the authors of\\nSMT \\n          \",\n        \"\\n            based on hyperparameters and on a correlation kernel. Four\\ncorrelation kernels are now implemented in SMT 2.0 \\n            \",\n        \"\\n            Noisy Kriging. In engineering and in big data contexts with real\\nexperiments, surrogate models for noisy data are of significant interest. In\\nparticular, there is a growing need for techniques like noisy Kriging\\nand noisy Multi-Fidelity Kriging (MFK) for data fusion \\n            \",\n        \"\\n            problems, the toolbox implements Kriging with partial least squares\\n(KPLS) \\n            \",\n        \"\\n            Marginal Gaussian process. SMT 2.0 implements Marginal Gaussian\\nProcess (MGP) surrogate models for high dimensional problems \\n            \",\n        \"\\n            Gradient-enhanced neural network. The new release SMT 2.0\\nimplements Gradient-Enhanced Neural Network (GENN) models \\n            \",\n        \"\\n            Parallel Bayesian optimization. Due to the recent progress made in\\nhardware configurations, it has been of high interest to perform parallel\\noptimizations. A parallel criterion called qEI \\n            \"\n      ]\n    },\n    {\n      \"title\": \"New kernels and their derivatives for Kriging. Kriging surrogates are\",\n      \"paragraphs\": [\n        \"\\n            based on hyperparameters and on a correlation kernel. Four\\ncorrelation kernels are now implemented in SMT 2.0 \\n            \",\n        \"\\n            Noisy Kriging. In engineering and in big data contexts with real\\nexperiments, surrogate models for noisy data are of significant interest. In\\nparticular, there is a growing need for techniques like noisy Kriging\\nand noisy Multi-Fidelity Kriging (MFK) for data fusion \\n            \"\n      ]\n    },\n    {\n      \"title\": \"Kriging with partial least squares. Beside MGP, for high-dimensional\",\n      \"paragraphs\": [\n        \"\\n            problems, the toolbox implements Kriging with partial least squares\\n(KPLS) \\n            \",\n        \"\\n            Marginal Gaussian process. SMT 2.0 implements Marginal Gaussian\\nProcess (MGP) surrogate models for high dimensional problems \\n            \",\n        \"\\n            Gradient-enhanced neural network. The new release SMT 2.0\\nimplements Gradient-Enhanced Neural Network (GENN) models \\n            \",\n        \"\\n            Parallel Bayesian optimization. Due to the recent progress made in\\nhardware configurations, it has been of high interest to perform parallel\\noptimizations. A parallel criterion called qEI \\n            \"\n      ]\n    },\n    {\n      \"title\": \"7. Conclusion\",\n      \"paragraphs\": [\n        \"SMT 2.0 introduces significant upgrades to the Surrogate Modeling\\nToolbox. This new release adds support for hierarchical and mixed\\nvariables and improves the surrogate models with a particular focus\\non Kriging (Gaussian process) models. SMT 2.0 is distributed through\\nan open-source license and is freely available online.14 We provide\\ndocumentation that caters to both users and potential developers.15\\nSMT 2.0 enables users and developers collaborating on the same\\nproject to have a common surrogate modeling tool that facilitates the\\nexchange of methods and reproducibility of results.\",\n        \"SMT has been widely used in aerospace and mechanical modeling\\napplications. Moreover, the toolbox is general and can be useful for\\nanyone who needs to use or develop surrogate modeling techniques,\\nregardless of the targeted applications. SMT is currently the only\\nopensource toolbox that can build hierarchical and mixed surrogate models.\"\n      ]\n    },\n    {\n      \"title\": \"Declaration of competing interest\",\n      \"paragraphs\": [\n        \"The authors declare that they have no known competing\\nfinancial interests or personal relationships that could have appeared to\\ninfluence the work reported in this paper.\"\n      ]\n    },\n    {\n      \"title\": \"Data availability\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Acknowledgments\",\n      \"paragraphs\": [\n        \"Data will be made available on request. Results can be reproduced\\nfreely online at https://colab.research.google.com/github/SMTorg/smt/\\nblob/master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\",\n        \"We want to thank all those who contribute to this release. Namely,\\nM. A. Bouhlel, I. Cardoso, R. Carreira Rufato, R. Charayron, R. Conde\\nArenzana, S. Dubreuil, A. F. L\\u00f3pez-Lopera, M. Meliani, M. Menz, N.\\nMo\\u00ebllo, A. Thouvenot, R. Priem, E. Roux and F. Vergnes. This work is\\npart of the activities of ONERA - ISAE - ENAC joint research group. We\\nalso acknowledge the partners institutions: ONERA, NASA Glenn,\\nISAESUPAERO, Institut Cl\\u00e9ment Ader (ICA), the University of Michigan,\\nPolytechnique Montr\\u00e9al and the University of California San Diego.\",\n        \"The research presented in this paper has been performed in the\\nframework of the AGILE 4.0 project (Towards cyber-physical\\ncollaborative aircraft development), funded by the European Union Horizon\\n2020 research and innovation framework programme under grant\\nagreement n\\u25e6 815122 and in the COLOSSUS project (Collaborative\\nSystem of Systems Exploration of Aviation Products, Services and\\n13 https://smt.readthedocs.io/en/latest/_src_docs/examples/airfoil_\\nparameters/learning_airfoil_parameters.html\\n14 https://github.com/SMTorg/SMT\\n15 https://smt.readthedocs.io/en/latest/\",\n        \"Business Models) funded by the European Union Horizon Europe\\nresearch and innovation framework programme under grant agreement\\nn\\u25e6 101097120.\",\n        \"We also are grateful to E. Hall\\u00e9-Hannan from Polytechnique\\nMontr\\u00e9al for the hierarchical variables framework.\"\n      ]\n    },\n    {\n      \"title\": \"Appendix A. Toy test function\",\n      \"paragraphs\": [\n        \"This Appendix gives the detail of the toy function of Section 5.1.16\\nFirst, we recall the optimization problem:\\nmin  (cat , qnt )\\nw.r.t. cat = 1 \\u2208 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\",\n        \"qnt = 1 \\u2208 0, 1\\nThe toy function  is defined as\\n (, 1) =11=0 cos(3.6( \\u2212 2)) +  \\u2212 1\\n+11=1 2 cos(1.1 exp()) \\u2212  + 2\",\n        \"2\\n+11=2 cos(2) +\\n1\",\n        \"2\\n+11=3 (cos(3.4( \\u2212 1)) \\u2212  \\u2212 1 )\",\n        \"2\\n+11=4 \\u2212 \\n2\\n2\\n+11=5 2 cos(0.25 exp(\\u22124))2 \\u2212  + 1\",\n        \"2\\n+11=6  cos(3.4) \\u2212  + 1\",\n        \"2\\n+11=7 \\u2212 (cos(3.5) +  ) + 2\",\n        \"2\\n+11=8 \\u2212 \\n2\\n5\"\n      ]\n    },\n    {\n      \"title\": \"Appendix B. Hierarchical Goldstein test function\",\n      \"paragraphs\": [\n        \"This Appendix gives the detail of the hierarchical Goldstein problem\\nof Section 5.2.17 First, we recall the optimization problem:\\nmin  (cnaetu, nqenut, cat , qdenct)\\nThe hierarchical and mixed function  is defined as a hierarchical\\nfunction that depends on 0, 1, 2 and cont as describes in the\\nfollowing.\\n (1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2) =\",\n        \"11=00(1, 2, 1, 2, 3, 4, 5, 2)\\n+ 11=11(1, 2, 3, 2, 3, 4, 5, 2)\\n+ 11=22(1, 2, 4, 1, 3, 4, 5, 2)\\n+ 11=3cont(1, 2, 3, 4, 3, 4, 5, 2).\\n16 https://github.com/jbussemaker/SBArchOpt\\n17 https://github.com/jbussemaker/SBArchOpt\\nThen, the functions 0, 1 and 2 are defined as mixed variants of\\ncont as such\\n0(1, 2, 1, 2, 3, 4, 5, 2) =\\n1\\n1\\n1\\n2=0\",\n        \"(11=0cont(1, 2, 20, 20, 3, 4, 5, 2)\\n+ 11=1cont(1, 2, 50, 20, 3, 4, 5, 2)\\n+ 11=2cont(1, 2, 80, 20, 3, 4, 5, 2) )\\n2=1\",\n        \"(11=0cont(1, 2, 20, 50, 3, 4, 5, 2)\\n+ 11=1cont(1, 2, 50, 50, 3, 4, 5, 2)\\n+ 11=2cont(1, 2, 80, 50, 3, 4, 5, 2) )\\n2=2\",\n        \"(11=0cont(1, 2, 20, 80, 3, 4, 5, 2)\\n+ 11=1cont(1, 2, 50, 80, 3, 4, 5, 2)\\n+ 11=2cont(1, 2, 80, 80, 3, 4, 5, 2) )\\n1(1, 2, 3, 2, 3, 4, 5, 2) =\\n12=0cont(1, 2, 3, 20, 3, 4, 5, 2)\\n+ 12=1cont(1, 2, 3, 50, 3, 4, 5, 2)\\n+ 12=2cont(1, 2, 3, 80, 3, 4, 5, 2)\\n2(1, 2, 4, 1, 3, 4, 5, 2) =\\n11=0cont(1, 2, 20, 4, 3, 4, 5, 2)\\n+ 11=1cont(1, 50, 2, 4, 3, 4, 5, 2)\\n+ 11=2cont(1, 2, 80, 4, 3, 4, 5, 2)\\nTo finish with, the function cont is given by\\n(B.3)\\ncont(1, 2, 3, 4, 3, 4, 5, 2) = 53.3108 + 0.1849011\\n\\u2212 5.0291413.10\\u22126 + 7.7252213 .10\\u22128 \\u2212 0.08707752 \\u2212 0.1069593\\n+ 7.9877234 .10\\u22126 + 0.002424824 + 1.3285143.10\\u22126 \\u2212 0.0014639312\",\n        \"More at https://colab.research.google.com/github/SMTorg/smt/blob/\\nmaster/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\",\n        \"Supplementary material related to this article can be found online\\nat https://doi.org/10.1016/j.advengsoft.2023.103571.\\n1 Mader CA, Martins JRRA, Alonso JJ, van der Weide E. ADjoint: An approach\\nfor the rapid development of discrete adjoint solvers. AIAA J 2008;46:863\\u201373.\\n2 Kennedy M, O\\u2019Hagan A. Bayesian calibration of computer models. J R Stat Soc\",\n        \"Ser B Stat Methodol 2001;63:425\\u201364.\\n3 Hwang JT, Martins JRRA. A fast-prediction surrogate model for large datasets.\",\n        \"Aerosp Sci Technol 2018;75:74\\u201387.\\n4 Martins JRRA, Ning A. Engineering design optimization. Cambridge University\",\n        \"Press; 2021.\\n5 Bouhlel MA, Hwang JT, Bartoli N, Lafage R, Morlier J, Martins JRA.\",\n        \"A Python surrogate modeling framework with derivatives. Adv Eng Softw\\n2019;135:102662.\\n6 Bouhlel MA, Martins J. Gradient-enhanced kriging for high-dimensional\\nproblems. Eng Comput 2019;35:157\\u201373.\\n7 Pedregosa F, Varoquaux G, Gramfort A, Thirion VMB, Grisel O, et al. Scikit-learn:\\nMachine learning in Python. J Mach Learn Res 2011;12:2825\\u201330.\"\n      ]\n    }\n  ],\n  \"references\": [\n    \"Ref ID: ref1, Given Names: C S B, Surname: Lataniotis Marelli Sudret, Year: 2022, Article Title: Uqlab 2.0 and uqcloud: open-source vs. cloud-based uncertainty quantification, Source: In: SIAM conference on uncertainty quantification, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref2, Given Names: A P N., Surname: Faraci Beaurepaire Gayton, Year: 2022, Article Title: Review on Python toolboxes for Kriging surrogate modelling, Source: In: ESREL, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref3, Given Names: M J M A, Surname: Kr\\u00fcgener Zapata Usandivaras Bauerheim Urbano, Year: 2022, Article Title: Coaxial-injector surrogate modeling based on Reynolds-averaged Navier-Stokes simulations using deep learning, Source: J Propuls Power, Volume: 38, Issue: None, First Page: 783, Last Page: 98\",\n    \"Ref ID: ref4, Given Names: D D S. Deep, Surname: Ming Williamson Guillas, Year: 2022, Article Title: Gaussian process emulation using stochastic imputation, Source: Technometrics, Volume: None, Issue: None, First Page: 1, Last Page: 12\",\n    \"Ref ID: ref5, Given Names: J V, Surname: Eli\\u00e1\\u0161 Vo\\u0159echovsky Sad\\u00edlekv, Year: 2020, Article Title: Periodic version of the minimax distance criterion for Monte Carlo integration, Source: Adv Eng Softw, Volume: 149, Issue: None, First Page: 102900, Last Page: None\",\n    \"Ref ID: ref6, Given Names: V M L S J., Surname: Drouet Balesdent Brevault Dubreuil Morio, Year: 2023, Article Title: Multi-fidelity algorithm for the sensitivity analysis of multidisciplinary problems, Source: J Mech Des, Volume: 145, Issue: None, First Page: 1, Last Page: 22\",\n    \"Ref ID: ref7, Given Names: P D T I I., Surname: Karban P\\u00e1nek Orosz Petr\\u00e1\\u0161ov\\u00e1 Dole\\u017eel, Year: 2021, Article Title: FEM based robust design optimization with Agros and A\\u00af rtap, Source: Comput Math Appl, Volume: 81, Issue: None, First Page: 618, Last Page: 33\",\n    \"Ref ID: ref8, Given Names: J R, Surname: Kudela Matousek, Year: 2022, Article Title: Recent advances and applications of surrogate models for finite element method computations: a review, Source: Soft Comput, Volume: 26, Issue: None, First Page: 13709, Last Page: 33\",\n    \"Ref ID: ref9, Given Names: Y F B S BT K, Surname: Chen Dababneh Zhang Kassaee Smith Liu, Year: 2020, Article Title: Surrogate modeling for capacity planning of charging station equipped with photovoltaic panel and hydropneumatic energy storage, Source: J Energy Res Technol, Volume: 142, Issue: None, First Page: 050907, Last Page: None\",\n    \"Ref ID: ref10, Given Names: J P D G, Surname: Jasa Bortolotti Zalkind Barter, Year: 2022, Article Title: Effectively using multifidelity optimization for wind turbine design, Source: Wind Energy Sci, Volume: 7, Issue: None, First Page: 991, Last Page: 1006\",\n    \"Ref ID: ref11, Given Names: W G D J J, Surname: Wang Tao Ke Luo Cui, Year: 2023, Article Title: Transpiration cooling of high pressure turbine vane with optimized porosity distribution, Source: Appl Therm Eng, Volume: 223, Issue: None, First Page: 119831, Last Page: None\",\n    \"Ref ID: ref12, Given Names: T HF EA R D., Surname: Savage Almeida-Trasvina del R\\u00edo-Chanona Smith Zhang, Year: 2020, Article Title: An adaptive data-driven modelling and optimization framework for complex chemical process design, Source: Comput Aided Chem Eng, Volume: 48, Issue: None, First Page: 73, Last Page: 8\",\n    \"Ref ID: ref13, Given Names: A AF T., Surname: Chan Pires Polacsek, Year: 2022, Article Title: Trying to elicit and assign goals to the right actors, Source: In: Conceptual modeling: 41st international conference, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref14, Given Names: F MA, Surname: Hutter Osborne, Year: 2013, Article Title: A kernel for hierarchical parameter spaces, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref15, Given Names: JH PD B, Surname: Bussemaker Ciampa Nagel, Year: 2020, Article Title: System architecture design space exploration: An approach to modeling and optimization, Source: In: AIAA aviation, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref16, Given Names: MEA EJ J JRRA DF L, Surname: Fouda Adler Bussemaker Martins Kurtulus Boggero, Year: 2022, Article Title: Automated hybrid propulsion model construction for conceptual aircraft design and optimization, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref17, Given Names: JH N T PD B, Surname: Bussemaker Bartoli Lefebvre Ciampa Nagel, Year: 2021, Article Title: Effectiveness of surrogate-based optimization algorithms for system architecture optimization, Source: In: AIAA aviation, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref18, Given Names: M B D S B A, Surname: Balandat Karrer Jiang Daulton Letham Wilson, Year: 2020, Article Title: BoTorch: A framework for efficient Monte-Carlo Bayesian optimization, Source: Adv Neural Inf Process Syst, Volume: 33, Issue: None, First Page: 21524, Last Page: 38\",\n    \"Ref ID: ref19, Given Names: B W K M J M, Surname: Adams Bohnhoff Dalbey Ebeida Eddy Eldred NM (United States: Sandia National Lab.(SNL-NM), Year: 2020, Article Title: Dakota, a multilevel parallel object-oriented framework for design optimization, parameter estimation, uncertainty quantification, and sensitivity analysis: Version 6.13 user's manual, Source: Technical report, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref20, Given Names: O D Y, Surname: Roustant Ginsbourger Deville, Year: 2012, Article Title: Two R packages for the analysis of computer experiments by Kriging-based metamodeling and optimization, Source: J Stat Softw, Volume: 51, Issue: None, First Page: 1, Last Page: 55\",\n    \"Ref ID: ref21, Given Names: Y S W D., Surname: Zhang Tao Chen Apley, Year: 2020, Article Title: A latent variable approach to Gaussian process modeling with qualitative and quantitative factors, Source: Technometrics, Volume: 62, Issue: None, First Page: 291, Last Page: 302\",\n    \"Ref ID: ref22, Given Names: TH SM, Surname: Chang Wild, Year: 2023, Article Title: ParMOO: A Python library for parallel multiobjective simulation optimization, Source: J Open Source Softw, Volume: 8, Issue: None, First Page: 4468, Last Page: None\",\n    \"Ref ID: ref23, Given Names: EC D, Surname: Garrido-Merch\\u00e1n Hern\\u00e1ndez-Lobato, Year: 2020, Article Title: Dealing with categorical and integer-valued variables in Bayesian optimization with Gaussian processes, Source: Neurocomputing, Volume: 380, Issue: None, First Page: 20, Last Page: 35\",\n    \"Ref ID: ref24, Given Names: M., Surname: Halstrup, Year: 2016, Article Title: Black-box optimization of mixed discrete-continuous optimization problems (, Source: Ph.D. thesis), Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref25, Given Names: O E Y A G J, Surname: Roustant Padonou Deville Cl\\u00e9ment Perrin Giorla, Year: 2020, Article Title: Group kernels for gaussian process metamodels with categorical inputs, Source: SIAM J Uncertain Quant, Volume: 8, Issue: None, First Page: 775, Last Page: 806\",\n    \"Ref ID: ref26, Given Names: Q PZG S., Surname: Zhou Qian Zhou, Year: 2011, Article Title: A simple approach to emulation for computer models with qualitative and quantitative factors, Source: Technometrics, Volume: 53, Issue: None, First Page: 266, Last Page: 73\",\n    \"Ref ID: ref27, Given Names: P Y N T J., Surname: Saves Diouane Bartoli Lefebvre Morlier, Year: 2023, Article Title: A mixed-categorical correlation kernel for Gaussian process, Source: Neurocomputing, Volume: 550, Issue: None, First Page: 126472, Last Page: None\",\n    \"Ref ID: ref28, Given Names: J L M E-G, Surname: Pelamatti Brevault Balesdent Talbi, Year: 2019, Article Title: Efficient global optimization of constrained mixed variable problems, Source: J Global Optim, Volume: 73, Issue: None, First Page: 583, Last Page: 613\",\n    \"Ref ID: ref29, Given Names: D J N-JS, Surname: Horn Stork ler, Year: 2019, Article Title: Surrogates for hierarchical search spaces: The Wedge-Kernel and an automated analysis, Source: In: Proceedings of the genetic and evolutionary computation conference, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref30, Given Names: Y VR SN, Surname: Hung Joseph Melkote, Year: 2009, Article Title: Design and analysis of computer experiments with branching and nested factors, Source: Technometrics, Volume: 51, Issue: None, First Page: 354, Last Page: 65\",\n    \"Ref ID: ref31, Given Names: C E, Surname: Audet Hall\\u00e9-Hannan, Year: 2023, Article Title: A general mathematical framework for constrained mixed-variable blackbox optimization problems with meta and categorical variables, Source: Oper Res Forum, Volume: 4, Issue: None, First Page: 1, Last Page: 37\",\n    \"Ref ID: ref32, Given Names: P E N Y T C S J, Surname: Saves Nguyen Van Bartoli Diouane Lefebvre David Defoort Morlier, Year: 2022, Article Title: Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref33, Given Names: R A S N T., Surname: Conde Arenzana L\\u00f3pez-Lopera Mouton Bartoli Lefebvre Multifidelity, Year: 2021, Article Title: Gaussian process model for CFD and wind tunnel data fusion, Source: In: ECCOMAS aerobest, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref34, Given Names: RC Y J R J., Surname: Rufato Diouane Henry Ahlfeld Morlier, Year: 2022, Article Title: A mixed-categorical data-driven approach for prediction and optimization of hybrid discontinuous composites performance, Source: In: AIAA aviation, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref35, Given Names: D K I T P., Surname: Gorissen Crombecq Couckuyt Dhaene Demeester, Year: 2010, Article Title: A surrogate modeling and adaptive sampling toolbox for computer based design, Source: J Mach Learn Res, Volume: 11, Issue: None, First Page: 2051, Last Page: 5\",\n    \"Ref ID: ref36, Given Names: CK CE, Surname: Williams Rasmussen, Year: 2006, Article Title: Gaussian processes for machine learning, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref37, Given Names: MA N R A J, Surname: Bouhlel Bartoli Regis Otsmane Morlier, Year: 2018, Article Title: Efficient Global Optimization for high-dimensional constrained problems by using the Kriging models combined with the Partial Least Squares method, Source: Eng Optim, Volume: 50, Issue: None, First Page: 2038, Last Page: 53\",\n    \"Ref ID: ref38, Given Names: MA S J, Surname: Bouhlel He Martins Scalable, Year: 2020, Article Title: -enhanced artificial neural networks for airfoil shape design in the subsonic and transonic regimes, Source: Struct Multidiscip Optim, Volume: 61, Issue: None, First Page: 1363, Last Page: 76\",\n    \"Ref ID: ref39, Given Names: LS A S., Surname: Kwan Pitrou Seibert, Year: 2015, Article Title: Numba: A LLVM-based python JIT compiler, Source: In: Proceedings of the second workshop on the LLVM compiler infrastructure in HPC, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref40, Given Names: M D., Surname: Zaefferer Horn, Year: 2018, Article Title: A first analysis of kernels for Kriging-based optimization in hierarchical search spaces, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref41, Given Names: R W A, Surname: Jin Chen Sudjianto, Year: 2005, Article Title: An efficient algorithm for constructing optimal design of computer experiments, Source: J Statist Plann Inference, Volume: 2, Issue: None, First Page: 545, Last Page: 54\",\n    \"Ref ID: ref42, Given Names: R M P, Surname: Garnett Osborne Hennig, Year: 2013, Article Title: Active learning of linear embeddings for Gaussian processes, Source: In: Uncertainty in artificial intelligence - Proceedings of the 30th conference, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref43, Given Names: D., Surname: Jones, Year: 2001, Article Title: A taxonomy of global optimization methods based on response surfaces, Source: J Global Optim, Volume: 21, Issue: None, First Page: 345, Last Page: 83\",\n    \"Ref ID: ref44, Given Names: R., Surname: Lafage, Year: 2022, Article Title: egobox, a Rust toolbox for efficient global optimization, Source: J Open Source Softw, Volume: 7, Issue: None, First Page: 4737, Last Page: None\",\n    \"Ref ID: ref45, Given Names: DR M WJ, Surname: Jones Schonlau Welch, Year: 1998, Article Title: Efficient global optimization of expensive black-box functions, Source: J Global Optim, Volume: 13, Issue: None, First Page: 455, Last Page: 92\",\n    \"Ref ID: ref46, Given Names: X CD K RK, Surname: Deng Lin Liu Rowe, Year: 2017, Article Title: Additive Gaussian process for computer models with qualitative and quantitative factors, Source: Technometrics, Volume: 59, Issue: None, First Page: 283, Last Page: 92\",\n    \"Ref ID: ref47, Given Names: J R O G C, Surname: Cuesta-Ramirez Le Riche Roustant Perrin Durantin, Year: 2021, Article Title: Gliere A. A comparison of mixed-variables Bayesian optimization approaches, Source: Adv Model Simul Eng Sci, Volume: 9, Issue: None, First Page: 1, Last Page: 29\",\n    \"Ref ID: ref48, Given Names: R P., Surname: Rebonato Jaeckel, Year: 2001, Article Title: The most general methodology to create a valid correlation matrix for risk management and option pricing purposes, Source: J Risk, Volume: 2, Issue: None, First Page: 17, Last Page: 27\",\n    \"Ref ID: ref49, Given Names: F D, Surname: Rapisarda Brigo, Year: 2007, Article Title: Parameterizing correlations: a geometric interpretation, Source: IMA J Manag Math, Volume: 18, Issue: None, First Page: 55, Last Page: 73\",\n    \"Ref ID: ref50, Given Names: MA N R A J, Surname: Bouhlel Bartoli Regis Otsmane Morlier, Year: 2016, Article Title: An improved approach for estimating the hyperparameters of the Kriging model for highdimensional problems through the Partial Least Squares method, Source: Math Probl Eng, Volume: None, Issue: None, First Page: 6723410, Last Page: None\",\n    \"Ref ID: ref51, Given Names: GH A KH GG, Surname: Cheng Younis Hajikolaei Wang, Year: 2015, Article Title: Trust region based mode pursuing sampling method for global optimization of high dimensional design problems, Source: J Mech Des, Volume: 137, Issue: None, First Page: 021407, Last Page: None\",\n    \"Ref ID: ref52, Given Names: R L S, Surname: Karlsson Bliek Verwer, Year: 2021, Article Title: Continuous surrogate-based optimization algorithms are well-suited for expensive discrete problems, Source: In: Artificial intelligence and machine learning, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref53, Given Names: J L M E-G, Surname: Pelamatti Brevault Balesdent Talbi, Year: 2021, Article Title: Bayesian optimization of variable-size design space problems, Source: Opt Eng, Volume: 22, Issue: None, First Page: 387, Last Page: 447\",\n    \"Ref ID: ref54, Given Names: A L M E-G, Surname: Hebbal Brevault Balesdent Talbi, Year: 2021, Article Title: Melab N. Bayesian optimization using deep Gaussian processes with applications to aerospace system design, Source: Opt Eng, Volume: 22, Issue: None, First Page: 321, Last Page: 61\",\n    \"Ref ID: ref55, Given Names: N., Surname: Wildberger, Year: 2007, Article Title: A rational approach to trigonometry, Source: Math Horiz, Volume: 15, Issue: None, First Page: 16, Last Page: 20\",\n    \"Ref ID: ref56, Given Names: H Y E D Y W., Surname: Cho Kim Lee Choi Lee Rhee, Year: 2020, Article Title: Basic enhancement strategies when using bayesian optimization for hyperparameter tuning of deep neural networks, Source: IEEE Access, Volume: 8, Issue: None, First Page: 52588, Last Page: 608\",\n    \"Ref ID: ref57, Given Names: MM D, Surname: Zuniga Sinoquet, Year: 2020, Article Title: Global optimization for mixed categorical-continuous variables based on Gaussian process models with a randomized categorical space exploration step, Source: INFOR Inf Syst Oper Res, Volume: 58, Issue: None, First Page: 310, Last Page: 41\",\n    \"Ref ID: ref58, Given Names: M K M D C, Surname: Lindauer Eggensperger Feurer Deng Benjamins, Year: 2022, Article Title: SMAC3: A versatile Bayesian optimization package for hyperparameter optimization, Source: J Mach Learn Res, Volume: 23, Issue: None, First Page: 1, Last Page: 9\",\n    \"Ref ID: ref59, Given Names: V J H H U S, Surname: Picheny Berkeley Moss Stojic Granta Ober, Year: 2023, Article Title: Trieste: Efficiently exploring the depths of black-box functions with TensorFlow, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref60, Given Names: AI W Z R H J, Surname: Cowen-Rivers Ly Wang Tutunov Jianye Wang, Year: 2020, Article Title: Heteroscedastic evolutionary Bayesian optimisation, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref61, Given Names: H Y Y W C, Surname: Jiang Shen Li Zhang Zhang, Year: 2023, Article Title: Cui B, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref62, Given Names: K KR W B C J, Surname: Kandasamy Vysyaraju Neiswanger Paria Collins Schneider, Year: 2020, Article Title: Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly, Source: J Mach Learn Res, Volume: 21, Issue: None, First Page: 3098, Last Page: 124\",\n    \"Ref ID: ref63, Given Names: S WA BK KT JS, Surname: Roy Crossley Stanford Moore Gray, Year: 2019, Article Title: A mixed integer efficient global optimization algorithm with multiple infill strategy - Applied to a wing topology optimization problem, Source: In: AIAA scitech, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref64, Given Names: J CA R, Surname: M\\u00fcller Shoemaker Pich\\u00e9, Year: 2013, Article Title: A surrogate model algorithm for computationally expensive nonlinear mixed-integer black-box global optimization problems, Source: Comput Oper Res, Volume: 40, Issue: None, First Page: 1383, Last Page: 400\",\n    \"Ref ID: ref65, Given Names: T D S M, Surname: Tran Sinoquet Da Veiga Mongeau, Year: 2021, Article Title: Derivative-free mixed binary necklace optimization for cyclic-symmetry optimal design problems, Source: Opt Eng, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref66, Given Names: M N T MA JRRA J, Surname: Meliani Bartoli Lefebvre Bouhlel Martins Morlier, Year: 2019, Article Title: Multifidelity efficient global optimization: Methodology and application to airfoil shape design, Source: In: AIAA aviation, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref67, Given Names: H., Surname: Lee, Year: 2011, Article Title: Gaussian processes, Source: None, Volume: None, Issue: None, First Page: 575, Last Page: 7\",\n    \"Ref ID: ref68, Given Names: AF D J F., Surname: L\\u00f3pez-Lopera Idier Rohmer Bachoc Multioutput, Year: 2022, Article Title: Gaussian processes with functional data: A study on coastal flood hazard assessment, Source: Reliab Eng Syst Saf, Volume: 218, Issue: None, First Page: 108139, Last Page: None\",\n    \"Ref ID: ref69, Given Names: G S M N C, Surname: Berthelin Dubreuil Sala\\u00fcn Bartoli Gogu, Year: 2022, Article Title: Disciplinary proper orthogonal decomposition and interpolation for the resolution of parameterized multidisciplinary analysis, Source: Internat J Numer Methods Engrg, Volume: 123, Issue: None, First Page: 3594, Last Page: 626\",\n    \"Ref ID: ref70, Given Names: I S N C M R, Surname: Cardoso Dubreuil Bartoli Gogu Sala\\u00fcn Lafage, Year: 2023, Article Title: Disciplinary surrogates for gradient-based optimization of multidisciplinary systems, Source: In: ECCOMAS Aerobest, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref71, Given Names: J S T T H., Surname: Platt Penny Smith Chen Abarbanel, Year: 2022, Article Title: A systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics, Source: Neural Netw, Volume: 153, Issue: None, First Page: 530, Last Page: 52\",\n    \"Ref ID: ref72, Given Names: R T N J., Surname: Charayron Lefebvre Bartoli Morlier, Year: 2023, Article Title: Multi-fidelity Bayesian optimization strategy applied to overall drone design, Source: In: AIAA scitech, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref73, Given Names: R T N J., Surname: Charayron Lefebvre Bartoli Morlier Towards, Year: 2023, Article Title: a multi-fidelity and multi-objective Bayesian optimization efficient algorithm, Source: Aerosp Sci Technol, Volume: 142, Issue: None, First Page: 108673, Last Page: None\",\n    \"Ref ID: ref74, Given Names: H., Surname: Wold, Year: 1975, Article Title: Soft modelling by latent variables: The non-linear iterative partial least squares (NIPALS) approach, Source: J Appl Probab, Volume: 12, Issue: None, First Page: 117, Last Page: 42\",\n    \"Ref ID: ref75, Given Names: R Y N S P, Surname: Priem Diouane Bartoli Dubreuil Saves, Year: 2023, Article Title: High-dimensional efficient global optimization using both random and supervised embeddings, Source: In: AIAA aviation, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref76, Given Names: W I D., Surname: Betz Papaioannou Straub, Year: 2014, Article Title: Numerical methods for the discretization of random fields by means of the Karhunen-Lo\\u00e8ve expansion, Source: Comput Methods Appl Mech Engrg, Volume: 271, Issue: None, First Page: 109, Last Page: 29\",\n    \"Ref ID: ref77, Given Names: M S J C N M., Surname: Menz Dubreuil Morio Gogu Bartoli Chiron, Year: 2021, Article Title: Variance based sensitivity analysis for Monte Carlo and importance sampling reliability assessment with Gaussian processes, Source: Struct Saf, Volume: 93, Issue: None, First Page: 102116, Last Page: None\",\n    \"Ref ID: ref78, Given Names: D R, Surname: Ginsbourger Le Riche, Year: 2010, Article Title: Kriging is well-suited to parallelize optimization, Source: None, Volume: None, Issue: None, First Page: 131, Last Page: 62\",\n    \"Ref ID: ref79, Given Names: E Y S P-O., Surname: Roux Tillier Kraria Bouchard, Year: 2020, Article Title: An efficient parallel global optimization strategy based on Kriging properties suitable for material parameters identification, Source: Arch Mech Eng, Volume: None, Issue: None, First Page: 67, Last Page: None\"\n  ]\n}",
    "{\n  \"Article_ID\": \"0c17a811baf94d9b9fa0f1e183b649b1\",\n  \"DocumentTitle\": \"Fuzzy logic based MPPT control for a PV system using SEPIC converter\",\n  \"Auteurs\": \"Moulay Abdellah Alhachemi, Mansour Becahr, Mohammed Habbab, Hicham Gouabi, Abdeldjebar Hazzab, \",\n  \"Institutions\": \"E\\u0301cole de Technologie Supe\\u0301rieure, E\\u0301cole de Technologie Supe\\u0301rieure, Laboratoire de CAOSEE, Universite\\u0301 Tahri Mohamed, Laboratory of CAOSEE, Universite\\u0301 Tahri Mohamed, Laboratory of CAOSEE, Universite\\u0301 Tahri Mohamed, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"I. INTRODUCTION\",\n      \"paragraphs\": [\n        \"\\n        The non-renewable energy sources are rapidly running out,\\nwhile the electricity demand is increasing daily. To solve this\\nproblem, efficient and efficient electric power generation from\\nrenewable energy sources is required \\n        \",\n        \"\\n        For the control of the PV systems, there are different sorts of\\nDC-DC converters such as Buck converters, Boost converters,\\nand Buck-Boost converters. Due to its output pick-up\\nadaptability, a single-ended primary-inductor converter\\n(SEPIC) acts as a buck-boost DC/DC converter, where it\\nchanges its output voltage agreeing to its duty cycle. Unlike the\\ncustomary buck-boost converter, the SEPIC converter includes\\na non-inverted output and it uses an arrangement capacitor to\\nseparate input from output \\n        \",\n        \"This paper presents a fuzzy-based P&O strategy for an MPPT\\nstandalone PV system. The proposed MPPT can abuse the\\npreferences of the P&O strategy and eliminate its drawbacks.\\nOutput has been separated into five fuzzy subsets. As the\\nproposed strategy continuously exchanges maximum power\\nfrom PV arrays, it optimizes the number of PV modules.\"\n      ]\n    },\n    {\n      \"title\": \"II. MODELIGN OF PV SYSTEM\",\n      \"paragraphs\": [\n        \"\\n        Photovoltaic is the technique and study connected to devices\\nthat directly convert sunlight into electricity utilizing\\nphotovoltaic semiconductors. Direct conversion of solar energy\\ninto DC electrical energy can be achieved by photovoltaic cells\\n\\n        \",\n        \"\\n        The shunt resistance (Rsh) is typically orders of magnitude\\nlarger than the series resistance (Rs) \\n        \"\n      ]\n    },\n    {\n      \"title\": \"III. SEPIC CONVERTER\",\n      \"paragraphs\": [\n        \"\\n        Power electronics researchers are working hard to create\\nDCDC converters with simpler designs and greater efficiency \\n        \",\n        \"\\n        When the switch is turned on, the input inductor is charged\\nfrom the source, and the second inductor is charged from the\\nfirst capacitor. No energy is supplied to the load capacitor\\nduring this time. The inductor current and capacitor voltage\\npolarities are marked in this Figure. When the power switch is\\nturned off, the energy stored in the inductor is transferred. The\\nenergy stored is transferred through the diode and supplies the\\nenergy to the load \\n        \",\n        \"A SEPIC converter is to process the electricity from the PV\\nsystem. This converter either increases or decreases the PV\\nsystem voltage at the load. The proposed SEPIC converter\\noperates in buck mode.\",\n        \"\\n        In the fuzzy logic maximum power point tracking (MPPT)\\nalgorithm, the voltage and current at each instant k are\\nmeasured to calculate the active power. The active power is\\nthen compared with the power at the previous instant (k-1) to\\nobtain the change in power (\\u0394P(k)). Similarly, the voltage at\\ninstant k is compared with the voltage at instant k-1 to obtain\\nthe voltage error (\\u0394V(k)) \\n        \",\n        \"Table. 1. Fuzzy logic rules for the push-pull converter. NB,\\nnegative big; NS, negative small; ZE, zero; PS, positive small;\",\n        \"PB, positive big.\\n\\u2206Vpv*o/p \\u2206Vpv*i/p\\n\\u2206Ppv*i/p\",\n        \"NB\\nNS\\nZE\\nPS\\nPB\",\n        \"NB\\nPS\\nPS\\nZE\\nNS\\nNS\",\n        \"NS\\nNB\\nPS\\nZE\\nNS\\nNB\",\n        \"ZE\\nNB\\nNS\\nZE\\nPS\\nPB\",\n        \"PS\\nNB\\nNS\\nZE\\nPS\\nPB\",\n        \"PB\\nNS\\nNS\\nZE\\nPS\\nPS\",\n        \"The following are the fuzzy rules in Table 1, which are used\\nfor the desired MPP of push-pull converter PWM. The\\nmembership for input variables (DPpv, DVpv) are shown in\\nFigure 5, and the membership for output variable (DVpv*) is\\nshown in Figure 6. All the functions are defined on a normalized\\ninterval -1 1.\",\n        \"Fig. 5. Membership for input variables.\"\n      ]\n    },\n    {\n      \"title\": \"V. SIMULATION RESULTS The characteristics of the photovoltaic array that we use in this paper are given in Table 2. Table. 2. Electrical data of photovoltaic array.\",\n      \"paragraphs\": [\n        \"Overall, using MPPT and fuzzy logic to a SEPIC converter for\\na PV system result in considerable performance gains.\\nIncreased power output, higher efficiency, decreased ripple,\\nenhanced transient response, and resilience are examples of\\nthese enhancements.\",\n        \"\\u2165. CONCLUSIONS\",\n        \"This paper presents the design of an off-grid photovoltaic\\nsystem with a fuzzy logic MPPT-controlled push-pull boost\\nconverter. The proposed system was simulated in\\nMATLAB/Simulink and tested under various weather\\nconditions. The results showed that the fuzzy logic algorithm\\noutperformed the conventional algorithms in terms of MPPT\\naccuracy and minimization of fluctuations, regardless of rapid\\nchanges in irradiance.\"\n      ]\n    }\n  ],\n  \"references\": [\n    \"Ref ID: ref1, Given Names: G., Surname: Mahendran, Year: 2013, Article Title: Ant Colony Optimized Tuned DC-DC converter, Source: International Journal of Computer Applications, Volume: 0975, Issue: None, First Page: 8887, Last Page: 50\",\n    \"Ref ID: ref2, Given Names: J., Surname: Dunia, Year: 2013, Article Title: \\u201cPerformance Comparison between \\u0106UK and SEPIC Converters for Maximum Power Point Tracking Using Incremental Conductance Technique in Solar Power Applications, Source: International Journal of Electrical, Volume: 7, Issue: None, First Page: 2510, Last Page: 2517\",\n    \"Ref ID: ref3, Given Names: M. M., Surname: Cirrincione Pucci, Year: 2011, Article Title: Growing Neural Gas (GNG)- Based Maximum Power Point Tracking for High-Performance Wind Generator With an Induction Machine, Source: \\u201d IEEE Transactions on Industry Applications 47.2, Volume: None, Issue: None, First Page: 861, Last Page: 872\",\n    \"Ref ID: ref4, Given Names: M., Surname: Abdulkadir S., Year: 2013, Article Title: Modeling and Simulation of a Solar Photovoltaic System, Its Dynamics and Transient Characteristics in LABVIEW.\\\", Source: International Journal of Power Electronics and Drive System (IJPEDS) 3, Volume: None, Issue: None, First Page: 185, Last Page: 192\",\n    \"Ref ID: ref5, Given Names: H. C. T. I. Z. S., Surname: Bouzeria Fetha Bahi Abadlia Layate Lekhchine, Year: 2015, Article Title: Fuzzy Logic Space Vector Direct Torque Control of PMSM for Photovoltaic Water Pumping System, Source: \\u201d Energy Procedia, Volume: 74, Issue: None, First Page: 760, Last Page: 771\",\n    \"Ref ID: ref6, Given Names: YM., Surname: Chen, Year: 2007, Article Title: \\u201cMulti-Input Inverter for Grid-Connected Hybrid PV/Wind Power System, Source: \\u201d IEEE Transactions on Power Electronics 22.3, Volume: None, Issue: None, First Page: 742, Last Page: 750\",\n    \"Ref ID: ref7, Given Names: S. J. A Maximum, Surname: Ganesh Janani Power, Year: 2014, Article Title: Point Tracker for PV Panels Using SEPIC Converter, Source: International Journal of Electrical, Volume: 8, Issue: None, First Page: 637, Last Page: 642\",\n    \"Ref ID: ref8, Given Names: R. S. Z, Surname: Vijayabalan Ravivarman Source, Year: 2012, Article Title: Inverter for Photovoltaic System with Fuzzy Logic Controller, Source: International Journal of Power Electronics and Drive System (IJPEDS) 2, Volume: None, Issue: None, First Page: 371, Last Page: 379\",\n    \"Ref ID: ref9, Given Names: A., Surname: Ramkumar, Year: 2015, Article Title: \\\"Analysis of Single Phase AC-DC SEPIC Converter using Closed Loop Techniques, Source: International Journal of Advanced Research in Electrical, Electronics and Instrumentation Engineering, Volume: 4, Issue: None, First Page: 193, Last Page: 201\",\n    \"Ref ID: ref10, Given Names: G. P., Surname: Tadi Ramamurthyraju, Year: 2013, Article Title: Analysis of SEPIC for PVApplications using PI Controller and Current Mode Control.\\\", Source: International Journal for Scientific Research & Development 1.9, Volume: None, Issue: None, First Page: 175, Last Page: 180\",\n    \"Ref ID: ref11, Given Names: J. and H., Surname: Li Wang, Year: 2009, Article Title: A novel stand-alone PV generation system based on variable step size INC MPPT and SVPWM control.\\\", Source: In Proceedings of the 2009 IEEE 6th International Power Electronics and Motion Control Conference, Volume: None, Issue: None, First Page: 17, Last Page: 20\",\n    \"Ref ID: ref12, Given Names: C., Surname: Wang, Year: 2015, Article Title: A Study of Membership Functions on Mamdani-Type Fuzzy Inference System for Industrial Decision-Making.\\u201d Master's thesis, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref13, Given Names: R.K. N.R., Surname: - Pal, Year: December 2011, Article Title: Design and performance of solar tracking system with fuzzy logic controller used different membership functions, Source: In Proceedings of the 2011 7th International Conference on Electrical and Electronics Engineering (ELECO), Volume: 7, Issue: 4, First Page: 1, Last Page: 16\",\n    \"Ref ID: ref14, Given Names: None, Surname: RK. Mudi, Year: 1999, Article Title: A robust self-tuning scheme for PI-, Source: \\u201d IEEE Transactions on Fuzzy Systems 7.1, Volume: None, Issue: None, First Page: 2, Last Page: 16\",\n    \"Ref ID: ref15, Given Names: A. H., Surname: Shehata Metered, Year: 2015, Article Title: Vibration control of active vehicle suspension system using fuzzy logic controller, Source: None, Volume: 389, Issue: None, First Page: 399, Last Page: None\"\n  ]\n}",
    "{\n  \"Article_ID\": \"c2a4d491742e4cfc9528ac25c65b7498\",\n  \"DocumentTitle\": \"Framing the News: From Human Perception to Large Language Model Inferences\",\n  \"Auteurs\": \"David Alonso del Barrio, Daniel Gatica-Perez, \",\n  \"Institutions\": \"Idiap Research Institute and EPFL, Idiap Research Institute, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"CCS CONCEPTS\",\n      \"paragraphs\": [\n        \"\\u2022 Computing methodologies \\u2192 Information extraction; \\u2022\\nHuman-centered computing \\u2192 Text input.\\nCovid-19 no-vax, news framing, GPT-3, prompt-engineering,\\ntransformers, large language models\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\",\n        \"ICMR \\u201923, June 12\\u201315, 2023, Thessaloniki, Greece\\n\\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-0178-8/23/06. . . $15.00\\nhttps://doi.org/10.1145/3591106.3592278\"\n      ]\n    },\n    {\n      \"title\": \"INTRODUCTION\",\n      \"paragraphs\": [\n        \"\\n        In recent years, there has been a proliferation in the use of concepts\\nsuch as data journalism, computational journalism, and\\ncomputerassisted reporting \\n        \",\n        \"Frame analysis is a concept from journalism, which consists of\\nstudying the way in which news stories are presented on an issue,\\nand what aspects are emphasized: Is a merely informative vision\\ngiven in an article? Or is it intended to leave a moral lesson? Is\\na news article being presented from an economic point of view?\\nOr from a more human, emotional angle? The examples above\\ncorrespond to diferent frames with which an article can be written.\",\n        \"\\n        The concept of news framing has been studied in computing as\\na step beyond topic modeling and sentiment analysis, and for this\\npurpose, in recent years, pre-trained language models have been\\nused for fine-tuning the classification process of these frames \\n        \",\n        \"Our work aims to address this research gap by posing the\\nfollowing research questions:\",\n        \"RQ1: What are the main frames in the news headlines about\\nthe anti-vaccine movement, as reported in newspapers across 5\\nEuropean countries?\",\n        \"RQ2: Can prompt engineering be used for classification of\\nheadlines according to frames?\",\n        \"By addressing the above research questions, our work makes the\\nfollowing contributions:\",\n        \"Contribution 1. We implemented a process to do human\\nannotation of the main frame of 1786 headlines of articles about the\\nCovid-19 no-vax movement, as reported in 19 newspapers from 5\\nEuropean countries (France, Italy, Spain, Switzerland and United\\nKingdom.) At the headline level, we found that the predominant\\nframe was human interest, where this frame corresponds to a\\npersonification of an event, either through a statement by a person,\\nor the explanation of a specific event that happened to a person.\\nFurthermore, we found a large number of headlines annotated as\\ncontaining no frame, as they simply present information without\\nentering into evaluations. We also found that for all the countries\\ninvolved, the distribution of frame types was very similar, i.e.,\\nhuman interest and no frame are the two predominant frames. Finally,\\nthe generated annotations allowed to subsequently study the\\nperformance of a large language model.\",\n        \"Contribution 2. We studied the performance of GPT-3.5 on\\nthe task of frame classification of headlines. In addition to using\\nthe fine-tuning approach from previous literature, we propose an\\nalternative approach for frame classification that requires no labeled\\ndata for training, namely prompt-engineering using GPT-3.5. The\\nresults show that fine-tuning with GPT-3.5 produces 72% accuracy\\n(slightly higher than other smaller models), and that the\\npromptengineering approach results in lower performance (49% accuracy.)\\nOur analysis also shows that the subjectivity of the human labeling\\ntask has an efect on the obtained accufracy.\",\n        \"The paper is organized as follows. In Section 2, we discuss related\\nwork. In Section 3, we describe the news dataset. In Section 4, we\\ndescribe the methodology for both human labeling and machine\\nclassification of news frames. We present and discuss results for\\nRQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide\\nconclusions in Section 7.\\n2\"\n      ]\n    },\n    {\n      \"title\": \"RELATED WORK\",\n      \"paragraphs\": [\n        \"\\n        Framing has been a concept widely studied in journalism, with a\\ndefinition that is rooted in the study of this domain \\n        \",\n        \"\\n        For frame recognition, there are two main approaches: the\\ninductive approach \\n        \",\n        \"\\n        We now compare the two approaches on a common topic, such\\nas Covid-19. Ebrahim et al. \\n        \",\n        \"We decided to follow the deductive approach because a\\npredeifned list of frames allows to compare among topics, countries,\\nprevious literature, and also because they represent a fixed list of\\nlabels for machine classification models. Furthermore, the\\ninductive approach tends to be more specific to a topic, and from the\\ncomputing viewpoint, past work has tried to justify topic modeling\\nas a technique to extract frames from articles.\",\n        \"\\n        Yl\\u00e4-Antitila et al. \\n        \",\n        \"\\n        From Entman\\u2019s definition of frame \\n        \",\n        \"\\n        Isoaho et al.\\n        \",\n        \"\\n        We also consider that the larger the number of possible frame\\ntypes, the more likely it is to end up doing topic modeling instead of\\nframe analysis. Using a deductive approach, Dallas et al. \\n        \",\n        \"\\n        A final decision in our work was the type of text to analyze,\\nwhether headlines or whole article. For this decision, the chosen\\nclassification method was also going to be important. For example,\\nKhanehzar et al. \\n        \",\n        \"\\n        Continuing with the question of the methods used for\\nclassiifcation, much work has been developed in prompt engineering,\\nespecially since the release of GPT-3. Liu et al.\\n        \",\n        \"\\n        As mentioned before, the emergence of giant models like GPT-3,\\nBLOOM, and ChatGPT are a very active research topic. To the best\\nof our knowledge, on one hand our work extends the computational\\nanalysis of news related to the covid-19 no-vax movement, which\\nillustrates the influence of the press on the ways societies think\\nabout relevant issues \\n        \"\n      ]\n    },\n    {\n      \"title\": \"3 DATA: EUROPEAN COVID-19 NEWS\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"DATASET\",\n      \"paragraphs\": [\n        \"\\n        We used part of the European Covid-19 News dataset collected in\\nour recent work \\n        \",\n        \"In the first phase, annotators had to read the codebook and get\\nfamiliar with the task. In the second phase, they were asked to\\nidentify the main frame in the same subset of 50 headlines. At the\\nend of the second phase, the intercoder reliability (ICR) was 0.58\\nbetween the 2 annotators. We analyzed those cases where there\\nwere discrepancies, and observed that in some cases, there was not a\\nunique main frame, because both annotators had valid arguments to\\nselect one of the frames. In other cases, the discrepancies were due\\nto slight misunderstanding of the definitions. In the third phase, the\\nannotators coded again 50 headlines, and the ICR increased to was\\n0.66. We realized that the possibility of having two frames remained.\\nThey discussed the cases in which they had disagreed, and if the\\nother person\\u2019s arguments were considered valid, it could be said that\\nthere were two frames. After this three-phase training procedure,\\nannotators were ready to annotate the dataset independently. We\\ndivided the dataset into two equal parts, and each person annotated\\n893 headlines.\\n4.2\"\n      ]\n    },\n    {\n      \"title\": \"Fine-tuning GPT-3.5 and BERT-based models\",\n      \"paragraphs\": [\n        \"With the annotated dataset, we investigated two NLP approaches:\\nthe first one involves fine-tuning a pre-trained model; the second\\none is prompt engineering. Pre-trained language models have been\\ntrained with large text strings based on two unsupervised tasks,\\nnext sentence prediction and masked language model. Figure 1\\nsummarizes these techniques.\",\n        \"\\n        In the first approach, a model with a fixed architecture is\\npretrained as a language model (LM), predicting the likelihood of the\\nobserved textual data. This can be done due to the availability of\\nlarge, raw text data needed to train LMs. This learning process can\\nproduce general purpose features of the modeled language. The\\nlearning process produces robust, general-purpose features of the\\nlanguage being modeled. The above pre-trained LM is then adapted\\nto diferent downstream tasks, by introducing additional parameters\\nand adjusting them using task-specific objective functions. In this\\napproach, the focus was primarily on goal engineering, designing\\nthe training targets used in both the pre-training and the fine-tuning\\nstages \\n        \",\n        \"We present an example to illustrate the idea. Imagine that the\\ntask is sentiment analysis, and we have a dataset with sentences\\nand their associated sentiment, and a pre-trained model, which is a\\nsaved neural network trained with a much larger dataset. For that\\npre-trained model to address the target task, we unfreeze a few of\\nthe top layers of the saved model base and jointly train both the\\nnewly-added classifier layers and the last layers of the base model.\\nThis allows to \\\"fine-tune\\\" the higher-order feature representations\\nin the base model to make them more relevant for the sentiment\\nanalysis task. In this way, instead of having to obtain a very large\\ndataset with target labels to train a model, we can reuse the\\npretrained model and use a much smaller train dataset. We use a part\\nof our dataset as examples for the model to learn the task, while\\nthe other part of the dataset is used to evaluate model performance.\",\n        \"Previous works related to frame classification in the computing\\nliterature have used fine-tuning, BERT-based models. In our work,\\nwe have done the same as a baseline, but we aimed to go one step\\nfurther and also produce results using fine-tuning of GPT-3.5.\\n4.3\"\n      ]\n    },\n    {\n      \"title\": \"Prompt-engineering with GPT-3.5\",\n      \"paragraphs\": [\n        \"\\n        Model fine-tuning has been widely used, but with the emergence\\nof generative models such as GPT-3, another way to approach\\nclassification tasks has appeared. The idea is to use the pre-trained\\nmodel directly and convert the task to be performed into a format\\nas close as possible to the tasks for which it has been pre-trained.\\nThat is, if the model has been pre-trained from next word prediction\\nas in the case of GPT-3, classification can be done by defining a\\nprompt, where the input to the model is an incomplete sentence,\\nand the model must complete it with a word or several words, just\\nas it has been trained. This avoids having to use part of the already\\nlabeled dataset to teach the task to be performed to the model, and\\na previous labeling is not needed \\n        \",\n        \"\\n        In this approach, instead of adapting pre-trained LMs to\\ndownstream tasks via objective engineering, downstream tasks are\\nreformulated to look more like those solved during the original LM\\ntraining with the help of a textual prompt. For example, when\\nrecognizing the emotion of a social media post, \\u201cI missed the bus today.\\u201d,\\nwe may continue with a prompt \\u201cI felt so _\\u201d, and ask the LM to\\nifll the blank with an emotion-bearing word. Or if we choose the\\nprompt \\u201cEnglish: I missed the bus today. French: _\\u201d), an LM may\\nbe able to fill in the blank with a French translation. In this way,\\nby selecting the appropriate prompts, we can influence the model\\nbehavior so that the pre-trained LM itself can be used to predict the\\ndesired output, even without any additional task-specific training\\n\\n        \",\n        \"\\n        We use this emerging NLP approach to classify frames at headline\\nlevel. We are not aware of previous uses of this strategy to classify\\nframes as we propose here. The idea is the following. Prompt\\nengineering consists of giving a prompt to the model, and understands\\nthat prompt as an incomplete sentence. To do prompt\\nengineering with our dataset, we needed to define an appropriate prompt\\nthat would produce the headline frames as output. We defined\\nseveral experiments with the Playground of GPT-3, in order to find\\nthe best prompt for our task. In our initial experiments, we\\nfollowed existing approaches in prompt engineering to do sentiment\\nanalysis, where the individual answer was an adjective, and this\\nadjective was matched with a sentiment. In a similar fashion, we\\ndecided to build a thesaurus of adjectives that define each of the\\nframes. For instance, the human interest frame could be\\n\\u2019interesting\\u2019, \\u2019emotional\\u2019, \\u2019personal\\u2019, \\u2019human\\u2019. The conflict frame could be:\\n\\u2019conflictive\\u2019, \\u2019bellicose\\u2019, \\u2019troublesome\\u2019, \\u2019rowdy\\u2019, \\u2019quarrelsome\\u2019,\\n\\u2019troublemaker\\u2019, \\u2019agitator\\u2019, etc. After the list of adjectives was defined,\\nwe needed to define the prompt in order to get, as an answer, one\\nof the adjectives in our thesaurus to match them with the frame.\\nWe used the GPT-3 playground using the headline as input and\\nasking for the frame as output, but the strategy did not work. In\\nour final experiment, instead of giving the headline as input, we\\ngave the definitions of each type of frame plus the headline, and we\\nasked the model to choose between the diferent types of frames\\nas output. In this way, the output of the model was directly one of\\nthe frames, and we avoided the step of matching adjectives with\\nframes. An example is shown in Figure 2.\\nFor the GPT-3 configuration 1, there are 3 main concepts:\\n\\u2022 TEMPERATURE \\n        \",\n        \"After testing with the GPT-3 playground and varying diferent\\nhyper-parameters to assess performance, we set the temperature to\\n0, since the higher the temperature the more random the response.\\nFurthermore, the Top-p parameter was set to 1, as it would likely\\nget a set of the most likely words for the model to choose from. The\\nmaximum number of tokens was set to 2; in this way, the model\\nis asked to choose between one of the responses. As a model, we\\nused the one with the best performance at the time of experimental\\ndesign, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\\n5\"\n      ]\n    },\n    {\n      \"title\": \"RESULTS: HUMAN LABELING OF FRAMES\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"IN NO-VAX NEWS HEADLINES (RQ1)\",\n      \"paragraphs\": [\n        \"In this section, we present and discuss the results of the analysis\\nrelated to our first RQ.\",\n        \"Figure 3 shows the distribution of frames per country at headline\\nlevel, with human interest and no-frame being the predominant\\n1https://beta.openai.com/docs/introduction\\nones. Attribution of responsibility is the third one except in\\nSwitzerland, where the corresponding frame is conflict. Finally, morality\\nand economic are the least represented in the dataset for every\\ncountry.\",\n        \"The monthly distribution of frames aggregated for all countries\\nis shown in Fig. 4. We can see two big peaks, the first one in January\\n2021 and the second one in August 2021. In all countries, the\\nvaccination process started at the end of December 2020, so it makes\\nsense that the no-vax movement started to be more predominant in\\nthe news in January 2021. Human interest is the most predominant\\nframe. Manual inspection shows that this is because the headlines\\nare about personal cases of people who are pro- or anti- vaccine.\\nAttribution of responsibility is also present. Manual inspection\\nindicates that local politicians and health authorities had to make\\ndecisions about who could be vaccinated at the beginning of the\\nprocess. The second peak at the end of summer 2021 coincided\\nwith the health pass (also called Covid passport in some countries),\\nand we can observe a peak in the curve corresponding to the\\nconlfict frame, reflecting the demonstrations against the measure of\\nmandatory health passes taken by country governments.\",\n        \"\\n        In Figure 5, we compare the sentiment per frame and per country,\\nto understand if there were any major diferences. The sentiment\\nanalysis labels were obtained using BERT-sent from the Hugging\\nFace package \\n        \",\n        \"Switzerland, and the United Kingdom,\\n\\u2022 No frame: 20-30% of negative content.\",\n        \"Regarding the results of the annotation process, the fact that the\\ndistribution of the 6 frame types is relatively similar between\\ncountries suggests that the anti-vaccine movement issue was treated\\nin a similar way in these countries. The fact that human interest\\nis the most dominant frame indicates that this issue was treated\\nfrom a more human and emotional approach, with headlines about\\npersonal experiences, celebrities giving their opinion about\\nvaccination, and politicians defending vaccine policies. Moreover, the\\nreason for many headlines being classified as no-frame is partly\\ndue to how data was selected. We chose articles that contained\\nwords related to no-vax, either in the headline or in the article. This\\nresulted in many headlines not containing anything specific related\\nto no-vax, while the no-vax content was actually included in the\\nmain text of the corresponding articles.\",\n        \"It is worth mentioning that prior to obtaining the results, we had\\nexpected that attribution of responsibility would be among the most\\nprominent frames, since governments took many measures such as\\nmandatory health pass requirements to access certain sites; we had\\nalso expected that the conflict frame would be prominent, since\\nthere were many demonstrations in Europe. In reality, however,\\nthese frames categories were not reflected as frequently at the\\nheadline level.\",\n        \"Regarding the analysis at the temporal level, it is clear that certain\\nevents were captured by the press, such as the start of vaccination\\nor the mandatory vaccination passport.\",\n        \"\\n        Finally, the sentiment analysis of the diferent frames shows that\\nthe predominant tone in all of them is neutral or negative, with very\\nsimilar trends between countries. This association between\\nsentiment analysis and frames has been discussed in previous literature\\n\\n        \"\n      ]\n    },\n    {\n      \"title\": \"RESULTS: GPT-3.5 FOR FRAME\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"CLASSIFICATION OF HEADLINES (RQ2)\",\n      \"paragraphs\": [\n        \"Here, we present and discuss the results related to our second RQ.\\n6.1\"\n      ]\n    },\n    {\n      \"title\": \"Fine-tuning GPT-3.5\",\n      \"paragraphs\": [\n        \"Table 4 shows the results of the 6-class classification task using\\n5-cross validation. Three models were used: GPT-3.5 and two\\nBERTbased models. We observe that, on average, GPT-3.5 performs better\\nthan the BERT-based models. This is somehow expected as\\nGPT3.5 is a much larger model. Overall, in the case of fine-tuning, the\\nbest performance for the six-class frame classification task is 72%\\naccuracy, which is promising, with an improvement over previous\\nmodels based on BERT. Yet, it should be noted that the performance\\ndiferences are modest (2% improvement between GPT-3.5 and\\nRoBERTa).\",\n        \"On the other hand, BERT is open-source, while GPT-3 has an\\neconomic cost as the use of the model is not free, which monetarily\\nlimits the number of experiments that can be performed with it,\\nas well as the diferent configurations one can explore to improve\\nperformance. This is important because much of the improvement\\nin performance requires empirical explorations of model parameters\\nMore specifically, the cost of an experiment for each of the folds has\\na cost of 4 dollars (at the time of writing this paper.) This represents\\na limitation in practice.\",\n        \"\\n        Furthermore, GPT-3 has a significant carbon footprint. Similarly,\\nfor prompt engineering (discussed in the next subsection), choosing\\nthe right prompt (i.e., the words that best define the task so that the\\nmodel is able to perform adequately) is also based on trial and error.\\nThis also has an impact on carbon footprint. In connection with\\nthis topic, Strubell et al.\\n        \"\n      ]\n    },\n    {\n      \"title\": \"Prompt-engineering with GPT-3.5\",\n      \"paragraphs\": [\n        \"For each headline, we got the frame that the model considered the\\nmost likely, and we compared these GPT-3.5 inferences with the\\nframes labeled by the annotators. The agreement between model\\nand annotator was of 49%. Analyzing the results, and specifically\\nlooking at the cases where the annotator and GPT-3.5 disagreed,\\nwe discovered that according to the frame definitions, the model\\nin some cases proposed a frame that indeed made sense. This\\nobservation, together with our previous experience in the annotation\\nprocess, where headlines could have more than one valid frame,\\nled us to design a second post-hoc experiment. We took all the\\nheadlines where each of the two annotators had disagreed with\\nGPT-3.5, and we asked the annotators to state whether they would\\nagree (or not) with each GPT-inferred label for a given headline.\\nIt is important to emphasize that the annotators did not know the\\norigin of that label, i.e., they did not know if it was the label they\\nhad originally assigned, or if it was a random one. In this way, we\\ncould quantify how GPT-3.5 worked according to valid arguments\\nprovided by the annotators. In this post-hoc experiment, the model\\nagreed in 76% of cases with the annotators.\",\n        \"\\n        Looking at the results of the classification models, the 49%\\naccuracy of the prompt-engineering approach can be considered low,\\nyet we consider that it is a valid avenue for further investigation,\\nas in the second post-hoc analysis, we found that the model agrees\\nwith human annotators in 76% of the cases. Clearly, framing\\ninvolves aspects of subjectivity \\n        \",\n        \"\\n        News reading is never fully objective, and the annotators\\nengaged in the frame classification task, influenced by their personal\\nstate of mind, experience, and culture, may perceive information\\ndiferently. Monarch afirms that \\\"for simple tasks, like binary labels\\non objective tasks, the statistics are fairly straightforward to decide\\nwhich is the \\u2018correct\\u2019 label when diferent annotators disagree. But\\nfor subjective tasks, or even objective tasks with continuous data,\\nthere are no simple heuristics for deciding what the correct label\\nshould be\\\" \\n        \",\n        \"\\n        Subjectivity is involved in both the generation and perception\\nof information: the assumption that there is only one frame is\\ncomplicated by the point of view of the reader. In the case of news, the\\ninformation sender (the journalist) has an intention, but the receiver\\n(the reader) plays a role and is influenced by it. In psychology, this\\nis known as the lens model of interpersonal communication, where\\nthe sender has certain objectives, but the receiver can interpret\\nor re-interpret what the sender wants to say, with more or less\\naccuracy \\n        \",\n        \"Following this discussion on subjectivity, the question arose as to\\nwhat would happen if, instead of headlines, we used the complete\\narticle as a source of analysis. We wondered if longer text could\\nmake the frame labeling task clearer than when using headlines.\\nYet another possible hypothesis is that having to read longer texts\\ncould lead to the same subject being presented from diferent angles.\\nPlease recall that in the existing literature discussed in Section 2,\\nboth headlines and full articles have been used from frame analysis\\n(see Table 1.) This remains as an issue for future work.\\n7\"\n      ]\n    },\n    {\n      \"title\": \"CONCLUSIONS\",\n      \"paragraphs\": [\n        \"In this paper, we first presented an analysis of human-generated\\nnews frames on the covid-19 no-vax movement in Europe, and\\nthen studied diferent approaches using large language models for\\nautomatic inference of frames. We conclude by answering the two\\nresearch questions we posed:\",\n        \"RQ1: What are the main frames in the news headlines about the\\ncovid-19 anti-vaccine movement in 5 European countries? After\\nannotating the headlines, we found that of the 1786 headlines,\\nthe predominant frame is human interest (45.3% of cases), which\\npresents a news item with an emotional angle, putting a face to a\\nproblem or situation. We also found that a substantial proportion\\nof headlines were annotated as not presenting any frame (40.2% of\\ncases). Finally, the other frame types are found more infrequently.\",\n        \"RQ2: Can prompt engineering be used for classification of\\nheadlines according to frames? We first used fine-tuning of a number of\\nlanguage models, and found that GPT-3.5 produced classicfiation\\naccuracy of 72% on a six-frame classification task. This represented a\\nmodest 2% improvement over BERT-based models, at a significantly\\nlarger environmental cost. We then presented a new way of\\nclassifying frames using prompts. At the headline level, inferences made\\nwith GPT-3.5 reached 49% of agreement with human-generated\\nframe labels. In many cases, the GPT-3.5 model inferred frame\\ntypes that were considered as valid choices by human annotators,\\nand in an post-doc experiment, the human-machine agreement\\nreached 76%. These results have opened several new directions for\\nfuture work.\"\n      ]\n    },\n    {\n      \"title\": \"ACKNOWLEDGMENTS\",\n      \"paragraphs\": [\n        \"This work was supported by the AI4Media project, funded by the\\nEuropean Commission (Grant 951911) under the H2020 Programme\\nICT-48-2020. We also thank the newspapers for sharing their online\\narticles. Finally, we thank our colleagues Haeeun Kim and Emma\\nBouton-Bessac for their support with annotations, and Victor Bros\\nand Oleksii Polegkyi for discussions.\"\n      ]\n    }\n  ],\n  \"references\": [\n    \"Ref ID: ref1, Given Names: Justito, Surname: Adiprasetio, Year: 2020, Article Title: Pandemic crisis in online media: Quantitative framing analysis on Detik, Source: com's coverage of Covid-19. Jurnal Ilmu Sosial Dan Ilmu Politik, Volume: 24, Issue: 2, First Page: 153, Last Page: 170\",\n    \"Ref ID: ref2, Given Names: Neel C Jess Alexis, Surname: Alex Riedel Carlier, Year: 2021, Article Title: RAFT: A real-world few-shot text classification benchmark, Source: arXiv preprint arXiv:2109.14076, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref3, Given Names: Vaccination, Surname: News? A Five-Country Analysis., Year: 2022, Article Title: How Did Europe's Press Cover Covid-, Source: [3] David Alonso del Barrio and Daniel Gatica-Perez, Volume: None, Issue: None, First Page: 35, Last Page: 43\",\n    \"Ref ID: ref4, Given Names: M Bender Timnit Shmargaret, Surname: Emily Gebru Angelina McMillan-Major Shmitchell, Year: 2021, Article Title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? (, Source: None, Volume: None, Issue: None, First Page: 610, Last Page: 623\",\n    \"Ref ID: ref5, Given Names: Santosh, Surname: Kumar, Year: 2020, Article Title: Artificial intelligence in journalism: A boon or bane? In Optimization in machine learning and applications, Source: None, Volume: None, Issue: None, First Page: 155, Last Page: 167\",\n    \"Ref ID: ref6, Given Names: Erik Rana, Surname: Bleich Abdelhamid, Year: 2015, Article Title: Media portrayals of minorities: Muslims in British newspaper headlines,, Source: Journal of Ethnic and Migration Studies, Volume: 41, Issue: 6, First Page: 2012, Last Page: 962\",\n    \"Ref ID: ref7, Given Names: Michael, Surname: Bommarito, Year: 2022, Article Title: GPT Takes the Bar Exam, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref8, Given Names: Meredith, Surname: Broussard Ching-Hua Chuan, Year: 2019, Article Title: Artificial intelligence and journalism, Source: Journalism & Mass Communication Quarterly, Volume: 96, Issue: 3, First Page: 673, Last Page: 695\",\n    \"Ref ID: ref9, Given Names: Tom Amanda, Surname: Brown Jared D Kaplan Askell, Year: 2020, Article Title: Language models are few-shot learners, Source: Advances in neural information processing systems, Volume: 33, Issue: None, First Page: 1877, Last Page: 1901\",\n    \"Ref ID: ref10, Given Names: Burscher, Surname: Bj\\u00f6rn Claes H De Vreese, Year: 2014, Article Title: Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis, Source: Communication Methods and Measures 8, Volume: None, Issue: 3, First Page: 190, Last Page: 206\",\n    \"Ref ID: ref11, Given Names: Burscher, Surname: Bjorn Claes H de Vreese, Year: 2016, Article Title: Frames beyond words: Applying cluster and sentiment analysis to news coverage of the nuclear power issue, Source: Social Science Computer Review, Volume: 34, Issue: 5, First Page: 530, Last Page: 545\",\n    \"Ref ID: ref12, Given Names: Card Noah, Surname: Dallas Smith, Year: 2015, Article Title: The Media Frames Corpus: Annotations of Frames Across Issues, Source: None, Volume: 2, Issue: 01, First Page: 438, Last Page: 444\",\n    \"Ref ID: ref13, Given Names: Carlos, Surname: Daniel El\\u00edas, Year: 2020, Article Title: Vaccine hesitancy in the age of coronavirus and fake news: analysis of journalistic sources in the Spanish quality press, Source: International Journal of Environmental Research and Public Health, Volume: 17, Issue: 21, First Page: 8136, Last Page: None\",\n    \"Ref ID: ref14, Given Names: Carmen, Surname: Daniel Pe\\u00f1afiel-Saiz, Year: 2019, Article Title: Media and mistrust of vaccines: a content analysis of press headlines, Source: None, Volume: 74, Issue: None, First Page: 786, Last Page: 802\",\n    \"Ref ID: ref15, Given Names: Mark, Surname: Coddington, Year: 2015, Article Title: Clarifying journalism's quantitative turn: A typology for evaluating data journalism, computational journalism, and computer-assisted reporting, Source: Digital journalism 3, Volume: None, Issue: 3, First Page: 331, Last Page: 348\",\n    \"Ref ID: ref16, Given Names: D, Surname: Stephen Cooper, Year: 2010, Article Title: The oppositional framing of bloggers, Source: In Doing News Framing Analysis. Routledge, Volume: 151, Issue: None, First Page: 172, Last Page: None\",\n    \"Ref ID: ref17, Given Names: Robert, Surname: Dale, Year: 2021, Article Title: GPT-3: What's it good for?, Source: Natural Language Engineering, Volume: 27, Issue: 1, First Page: 113, Last Page: 118\",\n    \"Ref ID: ref18, Given Names: Astrid Dave, Surname: Dirikx Gelders, Year: 2010, Article Title: To frame is to explain: A deductive frame-analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties, Source: Public Understanding of Science 19, Volume: None, Issue: 6, First Page: 732, Last Page: 742\",\n    \"Ref ID: ref19, Given Names: Astrid Dave, Surname: Dirikx Gelders, Year: 2010, Article Title: To frame is to explain: A deductive frameanalysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties, Source: Public understanding of science 19, Volume: None, Issue: 6, First Page: 732, Last Page: 742\",\n    \"Ref ID: ref20, Given Names: Dou Graham, Surname: Zi-Yi Neubig, Year: 2020, Article Title: Gsum: A general framework for guided neural abstractive summarization, Source: None, Volume: 08014, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref21, Given Names: Sumayya, Surname: Ebrahim, Year: 2022, Article Title: The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa, Source: Health SA Gesondheid (Online) 27, Volume: None, Issue: None, First Page: 1, Last Page: 8\",\n    \"Ref ID: ref22, Given Names: Hend, Surname: Abdelgaber Ahmed El-Behary Egypt., Year: 2021, Article Title: A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref23, Given Names: M, Surname: Robert Entman, Year: 1993, Article Title: Framing: Towards clarification of a fractured paradigm, Source: McQuail's reader in mass communication theory 390, Volume: None, Issue: None, First Page: 397, Last Page: None\",\n    \"Ref ID: ref24, Given Names: Gao Danqi, Surname: Tianyu Chen, Year: 2020, Article Title: Making pre-trained language models better few-shot learners, Source: None, Volume: 15723, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref25, Given Names: Piyush Koji, Surname: Ghasiya Okamura, Year: 2021, Article Title: Investigating COVID-19 news across four nations: a topic modeling and sentiment analysis approach, Source: Ieee Access, Volume: 9, Issue: None, First Page: 36645, Last Page: 36656\",\n    \"Ref ID: ref26, Given Names: Robert, Surname: Giford, Year: 1994, Article Title: A Lens-Mapping Framework for Understanding the Encoding and Decoding of Interpersonal Dispositions in Nonverbal Behavior, Source: Journal of Personality and Social Psychology, Volume: 66, Issue: None, First Page: 398, Last Page: 412\",\n    \"Ref ID: ref27, Given Names: Grail Eric, Surname: Quentin Gaussier, Year: 2021, Article Title: Globalizing BERT-based transformer architectures for long document summarization, Source: In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:, Volume: None, Issue: None, First Page: 1792, Last Page: 1810\",\n    \"Ref ID: ref28, Given Names: Gupta Rahul, Surname: Anushka Katarya, Year: 2022, Article Title: Automated news summarization using transformers, Source: In Sustainable Advanced Computing, Volume: None, Issue: None, First Page: 249, Last Page: 259\",\n    \"Ref ID: ref29, Given Names: Alfred, Surname: Hermida, Year: 2017, Article Title: Finding the data unicorn: A hierarchy of hybridity in data and computational journalism, Source: Digital Journalism, Volume: 5, Issue: 2, First Page: 159, Last Page: 176\",\n    \"Ref ID: ref30, Given Names: Isoaho Eetu, Surname: Karoliina M\\u00e4kel\\u00e4, Year: 2021, Article Title: Topic modeling and text analysis for qualitative policy research, Source: Policy Studies Journal, Volume: 49, Issue: 1, First Page: 300, Last Page: 324\",\n    \"Ref ID: ref31, Given Names: Jacobi and Kasper, Surname: Carina Welbers, Year: 2016, Article Title: Quantitative analysis of large amounts of journalistic texts using topic modelling, Source: Digital journalism 4, Volume: None, Issue: 1, First Page: 89, Last Page: 106\",\n    \"Ref ID: ref32, Given Names: Jiang Jun Graham, Surname: Zhengbao Araki Neubig, Year: 2020, Article Title: How can we know what language models know? Transactions of the Association for Computational Linguistics 8 (, Source: None, Volume: None, Issue: None, First Page: 423, Last Page: 438\",\n    \"Ref ID: ref33, Given Names: Khanehzar Andrew Gosia, Surname: Shima Turpin Miko\\u0142ajczak, Year: 2019, Article Title: Modeling Political Framing Across Policy Issues and Contexts, Source: In ALTA., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref34, Given Names: Jeesun Wayne, Surname: Kim Wanta, Year: 2018, Article Title: News framing of the US immigration debate during election years: Focus on generic frames, Source: The Communication Review, Volume: 21, Issue: 2, First Page: 89, Last Page: 115\",\n    \"Ref ID: ref35, Given Names: Liang Tony Ananya, Surname: Percy Lee Kumar, Year: 2022, Article Title: Holistic evaluation of language models, Source: arXiv preprint arXiv:2211.09110, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref36, Given Names: Liu Graham, Surname: Pengfei Neubig, Year: 2021, Article Title: Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref37, Given Names: Liu, Surname: Siyi, Year: 2019, Article Title: Detecting frames in news headlines and its application to analyzing news framing trends surrounding US gun violence, Source: In Proceedings of the 23rd conference on computational natural language learning (CoNLL)., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref38, Given Names: J\\u00f6rg Matthias, Surname: Matthes Kohring, Year: 2008, Article Title: The Content Analysis of Media Frames: Toward Improving Reliability and Validity, Source: Journal of Communication, Volume: 58, Issue: None, First Page: 2466, Last Page: None\",\n    \"Ref ID: ref39, Given Names: Meyer Bernd David E, Surname: Selina Ludwig Losada, Year: 2022, Article Title: Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI, Source: In Proceedings of the 4th Conference on Conversational User Interfaces. 1-6., Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref40, Given Names: E Middleton Yiannis, Surname: Stuart Kompatsiaris, Year: 2018, Article Title: Social computing for verifying social media content in breaking news, Source: IEEE Internet Computing, Volume: 22, Issue: 2, First Page: 83, Last Page: 89\",\n    \"Ref ID: ref41, Given Names: Marko Igor, Surname: Milosavljevi\\u0107 Vobi\\u010d, Year: 2021, Article Title: Our task is to demystify fears': Analysing newsroom management of automation in journalism, Source: Journalism, Volume: 22, Issue: 9, First Page: 2203, Last Page: 2221\",\n    \"Ref ID: ref42, Given Names: R., Surname: Monarch, Year: 2021, Article Title: Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref43, Given Names: Tom Pepper D, Surname: Nicholls Culpepper, Year: 2021, Article Title: Computational identification of media frames: Strengths, weaknesses, and opportunities, Source: Political Communication, Volume: 38, Issue: None, First Page: 1, Last Page: 2\",\n    \"Ref ID: ref44, Given Names: Zhongdang, Surname: Pan and Gerald M Kosicki, Year: 1993, Article Title: Framing analysis: An approach to news discourse, Source: Political communication 10, Volume: None, Issue: 1, First Page: 55, Last Page: 75\",\n    \"Ref ID: ref45, Given Names: Raul Bryan, Surname: Puri Catanzaro, Year: 2019, Article Title: Zero-shot text classification with generative language models, Source: None, Volume: 10165, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref46, Given Names: Guanghui Jason, Surname: Qin Eisner, Year: 2021, Article Title: Learning how to ask: Querying lms with mixtures of soft prompts, Source: arXiv preprint arXiv:2104.06599, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref47, Given Names: Rabindra, Surname: Lamsal, Year: 2021, Article Title: Sentiment Analysis of English Tweets with BERTsent, Source: None, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref48, Given Names: Radford Dario Ilya, Surname: Alec Amodei Sutskever, Year: 2019, Article Title: Language models are unsupervised multitask learners, Source: OpenAI blog 1, Volume: None, Issue: 8, First Page: 9, Last Page: None\",\n    \"Ref ID: ref49, Given Names: Rai Ahad, Surname: Nishant Ali, Year: 2022, Article Title: Fake News Classification using transformer based enhanced LSTM and BERT, Source: International Journal of Cognitive Computing in Engineering, Volume: 3, Issue: None, First Page: 98, Last Page: 105\",\n    \"Ref ID: ref50, Given Names: V, Surname: Frida Rodelo, Year: 2021, Article Title: Framing of the Covid-19 pandemic and its organizational predictors, Source: Cuadernos. info 50, Volume: None, Issue: None, First Page: 91, Last Page: 112\",\n    \"Ref ID: ref51, Given Names: Teven Matthias, Surname: Le Scao Gall\\u00e9, Year: 2022, Article Title: Bloom: A 176b-parameter open-access multilingual language model, Source: arXiv preprint arXiv:2211.05100, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref52, Given Names: Holli Patti, Surname: Semetko Valkenburg, Year: 2000, Article Title: Framing European Politics: A Content Analysis of Press and Television News, Source: Journal of Communication, Volume: 50, Issue: None, First Page: 93, Last Page: 109\",\n    \"Ref ID: ref53, Given Names: Shin, Surname: Richard, Year: 2021, Article Title: Constrained language models yield few-shot semantic parsers, Source: arXiv preprint arXiv:2104.08768, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref54, Given Names: Efstathios Andreas, Surname: Sidiropoulos Veglis, Year: 2017, Article Title: Computer Supported Collaborative Work trends on Media Organizations: Mixing Qualitative and Quantitative Approaches, Source: Studies in Media and Communication, Volume: 5, Issue: 04, First Page: 2279, Last Page: None\",\n    \"Ref ID: ref55, Given Names: Strubell, Surname: Emma Andrew McCallum, Year: 2019, Article Title: Energy and policy considerations for deep learning in NLP, Source: None, Volume: 02243, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref56, Given Names: Tamkin Deep, Surname: Alex Ganguli, Year: 2021, Article Title: Understanding the capabilities, limitations, and societal impact of large language models, Source: arXiv preprint arXiv:2102.02503, Volume: None, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref57, Given Names: H, Surname: Trieu Trinh and Quoc V Le, Year: 2018, Article Title: A simple method for commonsense reasoning, Source: None, Volume: 02847, Issue: None, First Page: None, Last Page: None\",\n    \"Ref ID: ref58, Given Names: Tsimpoukelli Felix, Surname: Maria Oriol Vinyals Hill, Year: 2021, Article Title: Serkan Cabi, Source: Advances in Neural Information Processing Systems, Volume: 34, Issue: None, First Page: 200, Last Page: 212\",\n    \"Ref ID: ref59, Given Names: A Vannoy Prashant, Surname: Sandra Palvia, Year: 2010, Article Title: The social influence model of technology adoption, Source: Commun. ACM 53, Volume: None, Issue: 6, First Page: 149, Last Page: 153\",\n    \"Ref ID: ref60, Given Names: Tuukka Veikko Anna, Surname: Yl\\u00e4-Anttila Eranti Kukkonen, Year: 2022, Article Title: Topic modeling for frame analysis: A study of media debates on climate change in India and USA, Source: Global Media and Communication, Volume: 18, Issue: 1, First Page: 91, Last Page: 112\"\n  ]\n}",
    "{\n  \"Article_ID\": \"9926f8227a71417aab38186cea31a845\",\n  \"DocumentTitle\": \"Cours 8 - Tests MODULE INTRODUCTION AU G\\u00c9NIE LOGICIEL\",\n  \"Auteurs\": \"MOSTEFAI MOHAMMED AMINE - BATATA SOFIANE, \",\n  \"Institutions\": \"\",\n  \"Abstract\": \"no\",\n  \"Sections\": [\n    {\n      \"title\": \"-\",\n      \"paragraphs\": [\n        \"6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Objectifs du Cours\",\n        \"Pr\\u00e9senter\\nl\\u2019activit\\u00e9 de\",\n        \"tests\\nIGL\\u2013Cours8,Tests\",\n        \"Donner les\",\n        \"diff\\u00e9rents\\ntypes de tests\\nIntroduire les\",\n        \"tests\\nunitaires\",\n        \"Introduire les\\ntests en bo\\u00eete\\nnoire\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"IGL \\u2013 Cours 8, Tests\\nPlan du Cours\",\n        \"Section 1 :\\nIntroduction\",\n        \"Section 2 :\\nLe Processus\\nde Tests\",\n        \"Section 3 :\",\n        \"Tests en\\nbo\\u00eete noire\",\n        \"Section 4 :\",\n        \"Tests\\nunitaires\\nI\\nS\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Introduction\",\n        \"SECTION 1\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nn\\na\\ni\\nf\\no\\nS\\nn\\ni\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Cycle de Vie\",\n        \"Expression des\\nbesoins\",\n        \"Analyse\",\n        \"Conception\\nImpl\\u00e9mentation\",\n        \"Tests\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\\n\\u2022 Les tests sont le proc\\u00e9d\\u00e9 de validation et de v\\u00e9rification d\\u2019un logiciel\\n\\u2022 Les tests assurent que le produit est conforme \\u00e0 ses sp\\u00e9cifications\\n\\u2022 Les tests assurent que le produit fait ce qu\\u2019on attend de lui\\n\\u2022 Les tests identifient les bugs, les erreurs ou les d\\u00e9faillances les plus\\nimportantes dans le but de les r\\u00e9parer\\n\\u2022 Une d\\u00e9faillance importante est celle qui affecte l\\u2019utilisabilit\\u00e9 du\\nproduit\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Qualit\\u00e9 de logiciels\",\n        \"Qualit\\u00e9\\nfonctionnelle\",\n        \"Qualit\\u00e9\\nstructurelle\",\n        \"Qualit\\u00e9 du\\nlogiciel\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\\n\\u2022 La qualit\\u00e9 fonctionnelle assure que le produit est conforme \\u00e0 ses\\nsp\\u00e9cifications fonctionnelle.\\n\\u2022 La qualit\\u00e9 structurelle concerne la conformit\\u00e9 aux sp\\u00e9cifications non\\nfonctionnelles (par exemple robustesse, rapidit\\u00e9, performance) et\\naussi \\u00e0 l\\u2019analyse de la structure interne du produit (code source,\\narchitecture,\\u2026)\\n\\u2022 L\\u2019objectif des tests n\\u2019est pas d\\u2019am\\u00e9liorer la qualit\\u00e9 mais plut\\u00f4t de la\\nmesurer\\nE\\n\\u00a9\\nt\",\n        \"V\\u00e9rification\",\n        \"Validation\\nRecensement\",\n        \"des\\nd\\u00e9faillances\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\\n\\u2022 La v\\u00e9rification consiste \\u00e0 v\\u00e9rifier une sp\\u00e9cification par rapport aux\\nattentes techniques. Par exemple, apr\\u00e8s la validation d\\u2019une facture,\\nretrouver le champ \\u00ab valide \\u00bb \\u00e0 \\u00ab true \\u00bb dans la base de donn\\u00e9es\\n\\u2022 La validation consiste \\u00e0 v\\u00e9rifier une sp\\u00e9cification par rapport aux\\nattentes m\\u00e9tier. Par exemple, apr\\u00e8s la validation d\\u2019une facture, la\\nquantit\\u00e9 du produit vendu doit d\\u00e9cro\\u00eetre\\n\\u2022 Une d\\u00e9faillance est une variation entre les r\\u00e9sultats actuels et les\\nr\\u00e9sultats attendus. L\\u2019origine de la d\\u00e9faillance peut \\u00eatre dans\\nl\\u2019expression de besoins, la conception, l\\u2019analyse ou l\\u2019impl\\u00e9mentation.\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Objectifs des tests - r\\u00e9ponses\",\n        \"Est-ce que \\u00e7a\\nmarche comme\\npr\\u00e9vu ?\",\n        \"Est-ce que c\\u2019est\",\n        \"conforme aux\\nsp\\u00e9cifications ?\",\n        \"Est-ce que \\u00e7a\\ncorrespond aux\\nattentes du client ?\",\n        \"Est-ce que le client\\nappr\\u00e9cie ?\\nEst-ce que c\\u2019est\\ncompatible avec\\nles autres syst\\u00e8mes\\n?\",\n        \"Est-ce que \\u00e7a\\ns\\u2019adapte \\u00e0 un\\nnombre important\\nd\\u2019utilisateurs ?\",\n        \"Quels sont les\\npoints \\u00e0 am\\u00e9liorer\\n?\",\n        \"Est-ce que c\\u2019est\",\n        \"pr\\u00eat pour le\\nd\\u00e9ploiement ?\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Objectifs des tests - Suite\\nEconomiser le temps et\\nl\\u2019argent en identifiant\\nrapidement les\\nd\\u00e9faillances\",\n        \"Rendre le\\nd\\u00e9veloppement plus\\nefficace\",\n        \"Augmenter la\\nsatisfaction du client\",\n        \"Correspondre le\\nr\\u00e9sultats aux attentes\",\n        \"Identifier les\\nmodifications \\u00e0 inclure\\ndans les prochaines\\nversions\",\n        \"Identifier les\\ncomposants et les\\nmodules r\\u00e9utilisables\",\n        \"Identifier les lacunes\\ndes d\\u00e9veloppeurs\\nE\\nC\\n|\\ne\",\n        \"Principes de tests\\nCommencer par les cas g\\u00e9n\\u00e9raux avant les cas particuliers\\nCommencer par les cas prioritaires\\nforc\\u00e9ment le code\",\n        \"Port\\u00e9e des tests\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nitrgh Conformit\\u00e9 aux\\nyopC sp\\u00e9cifications\\n|en fonctionnelles\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\nie\\u2013n Les restriction et\\ndAm recommandations\\nem des administrateurs\\nm\\naoh du syst\\u00e8me\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Conformit\\u00e9 aux\\nsp\\u00e9cifications\\ntechniques\",\n        \"Culture et\\nstandards de\\nl\\u2019entreprise\",\n        \"Exigences\\njuridiques\\nLa configuration\\nmat\\u00e9rielle\",\n        \"Le code source\",\n        \"Propri\\u00e9t\\u00e9s\\nculturelles et\\nlinguistiques\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\",\n        \"Introduction\",\n        \"SECTION 1 \\u2013 D\\u00c9BAT (10 MNS)\\nE\\n\\u00a9\\nt\\nn\\na\\ni\\nf\\no\\nS\\nB\\n\\u2013\\ne\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Processus de Tests\",\n        \"SECTION 2\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"L\\u2019\\u00e9quipe de testeurs\\n\\u2022 Les tests ne sont pas le travail d\\u2019un seul homme mais celui d\\u2019une\\n\\u00e9quipe\\n\\u2022 La taille de cette \\u00e9quipe d\\u00e9pend de la taille et de la complexit\\u00e9 du\\nprojet\\n\\u2022 Les d\\u00e9veloppeurs doivent avoir un r\\u00f4le dans les tests mais d\\u2019une fa\\u00e7on\\nr\\u00e9duite\\n\\u2022 Le testeur doit \\u00eatre minutieux, critique (pas au sens jugement), curieux\\ndot\\u00e9 d\\u2019une bonne communication\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"L\\u2019\\u00e9quipe de testeurs \\u2013 Questions \\u00e0 Poser\",\n        \"Comment\\npouvez-vous dire\\nque \\u00e7a marche ?\",\n        \"Que veut dire\\npour vous \\u00ab \\u00e7a\\nmarche \\u00bb ?\",\n        \"Pourquoi \\u00e7a\\nmarchait et que \\u00e7a\",\n        \"ne marche plus ?\",\n        \"Qu\\u2019est-ce qui a\\ncaus\\u00e9 le mauvais\\nfonctionnement ?\\nIGL|Cours8,Tests|Section 2,Processus deTest\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\\n\\u2022 Le coordinateur de tests cr\\u00e9e les plans de tests et les sp\\u00e9cifications de\\ntests sur la base des sp\\u00e9cifications techniques et fonctionnelles\\n\\u2022 Les testeurs ex\\u00e9cutent les tests et documentent les r\\u00e9sultats\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Cat\\u00e9gories de tests\",\n        \"Tests en bo\\u00eete noire\\n\\u2022 s\\u2019ex\\u00e9cutent en ignorant les\\nm\\u00e9canismes internes du\\nproduit\\n\\u2022 le testeur n\\u2019acc\\u00e8de pas au\\ncode source\",\n        \"Tests en bo\\u00eete blanche\\n\\u2022 Prennent les m\\u00e9canismes\",\n        \"internes en consid\\u00e9ration\\n\\u2022 le testeur acc\\u00e8de au code\\nsource\\nE\\nB\\n\\u2013\\ne\\n\\u2022 Il vaut mieux que les testeurs soient des personnes diff\\u00e9rentes des\\nprogrammeurs\\n\\u2022 Les testeurs s\\u2019int\\u00e9ressent \\u00e0 une fonction en terme d\\u2019entr\\u00e9e et de sorties\\nEntr\\u00e9e\",\n        \"Test\",\n        \"Sortie\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Le plan de tests\\n\\u2022 La planification des tests doit se faire dans les premi\\u00e8res phases du\\nprojet\\n\\u2022 Le plan de tests est un document obligatoire d\\u00e9terminant le\\nd\\u00e9roulement des tests durant le projet\\n\\u2022 Le tableau suivant indique le contenu d\\u2019un plan de test selon le\\nstandard \\u00ab American National Standards Institute and Institute for\\nElectrical and Electronic Engineers Standard 829/1983 for Software\\nTest Documentation\\u201d\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Le plan de tests - Composition\\n\\u00c9l\\u00e9ment\\nResponsabilit\\u00e9s\\nTests\\nCommunication\\nActeurs et affectation\\nPort\\u00e9e de tests, plannings,\\ndur\\u00e9es et priorit\\u00e9s\",\n        \"Plan de communication\\nAnalyse de risques\",\n        \"\\u00c9l\\u00e9ments critiques \\u00e0 tester\\nTra\\u00e7age des\\nd\\u00e9faillance\",\n        \"Documentation des\\nd\\u00e9faillances\\nD\\u00e9crit qui fait quoi dans les tests. Assure le\\nsuivi et les affectations\\nD\\u00e9finit le processus et d\\u00e9taille les actions \\u00e0\\nentreprendre\\nTout le monde doit savoir ce qu\\u2019il doit\\nsavoir avant les tests et ce qu\\u2019il doit faire\\nsavoir apr\\u00e8s les tests\\nIdentification des domaines qui sont\\ncritiques dans le projet\\nDocumenter les d\\u00e9faillances et les d\\u00e9tails\\nles concernant\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Priorisation des d\\u00e9faillances\\n\\u2022 Le plan de tests d\\u00e9termine la priorisation des d\\u00e9faillances\\n\\u2022 La priorisation facilite la communication entre d\\u00e9veloppeurs et\\ntesteurs ainsi que l\\u2019affectation et la planification des t\\u00e2ches\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\\n6- Cosm\\u00e9tique\\nImpossible de continuer les tests \\u00e0 cause de la s\\u00e9v\\u00e9rit\\u00e9 des d\\u00e9faillances\\nLes tests peuvent continuer mais l\\u2019application ne peut passer en mode production\\nL\\u2019application peut passer en mode production mais des exigences fonctionnelles\\nimportantes ne sont pas satisfaites\\nL\\u2019application peut passer en mode production mais des exigences fonctionnelles\\nsans tr\\u00e8s grand impact ne sont pas satisfaites\\nD\\u00e9faillances mineures relatives \\u00e0 l\\u2019interface (couleurs, police, \\u2026) mais n\\u2019ayant pas\\nune relation avec les exigences du client\",\n        \"Cas de test (Test Case)\\n\\u2022 Un cas de test est un ensemble d\\u2019entr\\u00e9es de tests, de conditions\\nd\\u2019ex\\u00e9cution et de r\\u00e9sultats attendus pour un objectif particulier tel\\nque la conformit\\u00e9 du programme avec une sp\\u00e9cification donn\\u00e9es\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nC\\nn\\na\\ni\\nf\\no\\nS\\nM\",\n        \"Priorit\\u00e9\\nPr\\u00e9conditions\\nR\\u00e9sultats attendus\",\n        \"R\\u00e9sultats actuels\",\n        \"Remarques\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\\n\\u2022 L\\u2019ID est un num\\u00e9ro unique qui permet la tra\\u00e7abilit\\u00e9 des cas de test, les lier \\u00e0 des\\nsp\\u00e9cifications ou \\u00e0 d\\u2019autres cas de test\\n\\u2022 Les pr\\u00e9conditions d\\u00e9terminent les conditions n\\u00e9cessaires \\u00e0 un cas de test. Elles\\nindiquent aussi les cas de test qui doivent \\u00eatre ex\\u00e9cut\\u00e9s pr\\u00e9c\\u00e9demment\\n\\u2022 La description est un texte d\\u00e9crivant le tests et ses attentes\\n\\u2022 Le sc\\u00e9nario d\\u00e9termine les \\u00e9tapes d\\u00e9taill\\u00e9es \\u00e0 suivre par le testeur\\n\\u2022 Les r\\u00e9sultats attendus sont attendus de l\\u2019ex\\u00e9cution du sc\\u00e9nario\\n\\u2022 Les r\\u00e9sultats actuels sont les vrais r\\u00e9sultats obtenus, s\\u2019ils sont diff\\u00e9rents des\\nr\\u00e9sultats attendus, une d\\u00e9faillance est signal\\u00e9e\\n\\u2022 Les remarques sont signal\\u00e9es par le testeur pour ajouter des information concernant\\nune ex\\u00e9cution donn\\u00e9e\\n\\u2022 Le sc\\u00e9nario est compos\\u00e9 de plusieurs actions num\\u00e9rot\\u00e9es\\n\\u2022 Chaque action peut avoir un r\\u00e9sultat attendu\\n\\u2022 L\\u2019ex\\u00e9cution de test affecte un r\\u00e9sultat actuel \\u00e0 chaque \\u00e9tape attendant\\nun r\\u00e9sultat\\n\\u2022 Si chaque r\\u00e9sultat actuel est conforme au r\\u00e9sultat attendu alors le test\\nr\\u00e9ussit sinon le test \\u00e9choue\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Exemple 1\",\n        \"ID\\nR\\u00e9sultats attendus\\nR\\u00e9sultats Actuels\\n1\\n123Ouverture d\\u2019un document\\nExistence d\\u2019un document\\n1- Cliquez sur le bouton \\u00ab Ouvrir \\u00bb\\n2- S\\u00e9lectionnez le fichier d\\u00e9sir\\u00e9\\n3- Appuyez sur OK\\n1- Une bo\\u00eete d\\u2019ouverture de fichiers\\ns\\u2019affiche\\n23- Le Document est ouvert et pr\\u00eat \\u00e0 \\u00eatre\\nmanipul\\u00e9\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Exemple 2\",\n        \"ID\\nR\\u00e9sultats attendus\\nR\\u00e9sultats actuels\\n2\\n12Impression d\\u2019un document\\n1- Cliquez sur le bouton imprimer\\n2- S\\u00e9lection la config puis cliquer sur\\n\\u00ab Imprimer \\u00bb\\n1- La bo\\u00eete de configuration\\nd\\u2019imprimante s\\u2019affiche\\n2- Le document sort sous format papier\\n\\u2022 Une campagne de tests s\\u00e9lectionne un certain nombre de cas de tests\\ns\\u00e9mantiquement relatifs et les ex\\u00e9cute\\n\\u2022 Chaque campagne de tests a un objectif trac\\u00e9 : validation du produit,\\nperformance, \\u2026 etc.\",\n        \"Exercice\\n\\u2022 Ecrire un cas de test d\\u2019inscription \\u00e0 un mail en tenant en compte des\\ncas particuliers suivants : adresse mail non valide, mail existant, mot de\\npasse non s\\u00e9curis\\u00e9\\nI\\nS\\nE\\nC\\nn\\na\\ni\\nf\\no\\nS\\nB\\nm\\na\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\",\n        \"Tests en Bo\\u00eete Noire\",\n        \"SECTION 3, D\\u00c9BAT 05 MNS\\nE\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\nn\\na\\ni\\nf\\no\\nS\\nB\\nm\\na\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\",\n        \"SECTION 4\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\n\\u00a9\\nt\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\n|\\ne\\nn\\na\\ni\\nf\\no\\nS\\na\\nt\\na\\nt\\na\\nB\\n\\u2013\\ne\\nn\\ni\\nm\\nA\\nd\\ne\\nm\\nm\\na\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\\n\\u2022 Les tests unitaires sont des tests en bo\\u00eete blanche\\n\\u2022 Les tests unitaires sont compos\\u00e9 d\\u2019un ensemble de classes appel\\u00e9es\\n\\u00ab classes de test \\u00bb\\n\\u2022 Ces classes valident que des portions de code r\\u00e9pondent \\u00e0 un certain\\nbesoin\\n\\u2022 Les tests unitaires sont importants car ils permettent de d\\u00e9tecter le\\nmaximum de d\\u00e9faillance avant les tests en bo\\u00eete noire et qu\\u2019ils\\npeuvent s\\u2019ex\\u00e9cuter d\\u2019une mani\\u00e8re automatique\",\n        \"Objectifs\\n\\u2022 Les tests unitaires ont deux objectifs : \\u00ab couverture de code \\u00bb et\\n\\u00ab couverture de donn\\u00e9es \\u00bb\\n\\u2022 La couverture du code stipule de tester chaque ligne de code \\u00e9crite\\n(appel de fonction, boucles, d\\u00e9cisions, d\\u00e9cisions multiples,\\u2026)\\n6\\n1\\n0\\n2\\n5\\n1\\n0\\n2\\nI\\nS\\nE\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\na\\nt\\na\\nt\\na\\nB\\nn\\ni\\nA\\nd\\ne\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\",\n        \"Caract\\u00e9ristiques\\n\\u2022 Les tests unitaires doivent \\u00eatre conforme \\u00e0 l\\u2019acronyme FIRST:\\n\\u2022 FAST : un test unitaire doit s\\u2019ex\\u00e9cuter rapidement\\n\\u2022 INDEPENDANT: chaque test doit \\u00eatre ind\\u00e9pendant des autres\\n\\u2022 REPEATABLE: chaque test peut \\u00eatre r\\u00e9p\\u00e9t\\u00e9 autant de fois que voulu\\n\\u2022 TIMELY: On \\u00e9crit les tests quand on en a besoin\\npublic class Calculator\\n{\\n{\\npublic int add(int x, int y)\\nreturn x + y;\\npublic int multi(int x, int y)\",\n        \"return x*y;\\nh\\ng\\ni\\nr\\ny\\np\\no\\nC\\na\\nt\\na\\nt\\na\\nB\\nn\\ni\\nd\\ne\\nh\\no\\nM\\ni\\na\\nf\\ne\\nt\\ns\\no\\nM\\n// tester la classe\\npublic class CalculatorTest\\n{\\n// tester l\\u2019addition\\npublic int addTest()\\n{\\nC\\n|\",\n        \"SECTION 4, D\\u00c9BAT 05 MNS\\n\\u2022 UML Component Diagrams, Veronica Carrega, 2004\\n\\u2022 Analyse, Conception Objet\\n\\u2022 Diagrammes de d\\u00e9ploiement, SIMMO/ENSM.SE, 2002\\nIGL \\u2013 Cours 6, Architecture de Logiciels\"\n      ]\n    }\n  ],\n  \"references\": []\n}"
  ]
}