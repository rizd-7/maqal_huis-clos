{
  "data": [
    "{\n  \"DocumentTitle\": \"Large Language Model Augmented Narrative Driven Recommendations\",\n  \"Auteurs\": \"Sheshera Mysore, Andrew McCallum, Hamed Zamani, \",\n  \"Institutions\": \"University of Massachusetts Amherst, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"CCS CONCEPTS\",\n      \"paragraphs\": [\n        \"\\u2022 Information systems \\u2192 Recommender systems; Users and\\ninteractive retrieval; \\u2022 Computing methodologies \\u2192 Natural language\\ngeneration.\"\n      ]\n    },\n    {\n      \"title\": \"INTRODUCTION\",\n      \"paragraphs\": [\n        \"\\n        Recommender systems personalized to users are an important\\ncomponent of several industry-scale platforms \\n        \",\n        \"\\n        However, with the emergence of conversational interfaces for\\ninformation access tasks, support for complex NDR tasks is likely\\nto become necessary. In this context, recent work has noted an\\nincrease in complex and subjective natural language requests\\ncompared to more conventional search interfaces \\n        \",\n        \"Specifically, given a user\\u2019s interactions,  , with items and\\ntheir accompanying text documents (e.g., reviews, descriptions)\\n = { }=1, selected from a user-item interaction dataset I, we\\nprompt InstructGPT, a 175B parameter LLM, to author a synthetic\\nnarrative query  based on  (Figure 2). Since we expect the\\nquery  to be noisy and not fully representative of all the user\\nreviews,  is filtered to retain only a fraction of the reviews based\\non a language-model assigned likelihood of  given a user\\ndocument,  . Then, a pre-trained LM based retrieval model (110M\\nparameters) is fine-tuned for retrieval on the synthetic queries and\\nifltered reviews.\",\n        \"\\n        Our approach, which we refer to as Mint2, follows from the\\nobservation that while narrative queries and suggestions are often\\nmade in online discussion forums, and could serve as training data,\\nthe number of these posts and the diversity of domains for which\\nthey are available is significantly smaller than the size and diversity\\nof passively gathered user-item interaction datasets. E.g. while\\nBogers and Koolen \\n        \",\n        \"\\n        We empirically evaluate Mint in a publicly available test\\ncollection for point of interest recommendation: pointrec \\n        \"\n      ]\n    },\n    {\n      \"title\": \"RELATED WORK\",\n      \"paragraphs\": [\n        \"\\n        Data Augmentation for Information Access. A line of recent\\nwork has explored using language models to generate synthetic\\nqueries for data augmentation to train models for information\\nretrieval tasks \\n        \",\n        \"\\n          Besides creating queries for ad-hoc retrieval tasks, concurrent\\nwork of Leszczynski et al. \\n          \",\n        \"\\n          Finally, while our work explores data augmentation from\\nuseritem interactions for a retrieval-oriented NDR task, prior work has\\nalso explored data augmentation of the user-item graph for training\\ncollaborative filtering models. This work has often explored\\naugmentation to improve recommendation performance for minority\\n\\n          \",\n        \"\\n          Complex Queries in Information Access. With the advent\\nof performant models for text understanding, focus on complex\\nand interactive information access tasks has seen a resurgence\\n\\n          \",\n        \"\\n          Besides this, a range of work has explored more complex,\\nlongform, and interactive query formulations for information access;\\nthese resemble queries in NDR. Arguello et al. \\n          \"\n      ]\n    },\n    {\n      \"title\": \"3https://github.com/iesl/narrative-driven-rec-mint/\",\n      \"paragraphs\": [\n        \"\\n          Besides creating queries for ad-hoc retrieval tasks, concurrent\\nwork of Leszczynski et al. \\n          \",\n        \"\\n          Finally, while our work explores data augmentation from\\nuseritem interactions for a retrieval-oriented NDR task, prior work has\\nalso explored data augmentation of the user-item graph for training\\ncollaborative filtering models. This work has often explored\\naugmentation to improve recommendation performance for minority\\n\\n          \",\n        \"\\n          Complex Queries in Information Access. With the advent\\nof performant models for text understanding, focus on complex\\nand interactive information access tasks has seen a resurgence\\n\\n          \",\n        \"\\n          Besides this, a range of work has explored more complex,\\nlongform, and interactive query formulations for information access;\\nthese resemble queries in NDR. Arguello et al. \\n          \"\n      ]\n    },\n    {\n      \"title\": \"Proposed Method\",\n      \"paragraphs\": [\n        \"\\n        Our proposed method, Mint, for NDR, re-purposes a dataset of\\n\\nabundantly available user-item interactions, I = {(, { }=1)} into\\ntraining data for retrieval models by using LLMs as query\\ngeneration models to author narrative queries  : D = {(, { }=1)}.\\nThen, retrieval models are trained on the synthetic dataset D\\n(Figure 3).\\n3.2.1 Narrative Queries from LLMs. To author a narrative query \\nfor a user in I, we make use of the 175B parameter InstructGPT4\\nmodel as our query generation model QGen. We include the text\\nof interacted items { }=1 in the prompt for QGen, and instruct it\\nto author a narrative query (Figure 2). To improve the coherence\\nof generated queries and obtain correctly formatted outputs, we\\nmanually author narrative queries for 3 topically diverse users\\nbased on their interacted items and include it in the prompt for\\nQGen. The same three few shot examples are used for the whole\\ndataset I, and the three users were chosen from I. Generating\\nnarrative queries based on user interactions may also be considered\\na form of multi-document summarization for generating a natural\\nlanguage user profile \\n        \",\n        \"q and d . Embeddings are obtained by averaging token embeddings\\nfrom the final layer of MPNet, and the same model is used for both\\nqueries and items. Cross-encoder models input both the query and\\nitem and output a score to be used for ranking  = Cr (  ;  ),\\nwhere Cr is parameterized as w dropout W MPNet(\\u00b7) . We\\ntrain our bi-encoder model with a margin ranking loss: L =\\n\\u00cd \\u00cd=1 max2(q, d ) \\u2212 2(q, d\\u2032 ) + , 0 with randomly\\nsampled negatives \\u2032 and  = 1. Our cross-encoders are trained with\\na cross-entropy loss: L = \\u00cd \\u00cd=1 log( \\u00cd\\u2032\\u2032 ). For training, 4\\nnegative example items \\u2032 are randomly sampled from ranks\\n100300 from our trained bi-encoder. At test time, we retrieve the top\\n200 items with our trained bi-encoder and re-rank them with the\\ncross-encoder - we evaluate both these components in experiments\\nand refer to them as BiEnc-Mint and CrEnc-Mint.\\n4\"\n      ]\n    },\n    {\n      \"title\": \"4https://platform.openai.com/docs/models/gpt-3, text-davinci-003\",\n      \"paragraphs\": [\n        \"q and d . Embeddings are obtained by averaging token embeddings\\nfrom the final layer of MPNet, and the same model is used for both\\nqueries and items. Cross-encoder models input both the query and\\nitem and output a score to be used for ranking  = Cr (  ;  ),\\nwhere Cr is parameterized as w dropout W MPNet(\\u00b7) . We\\ntrain our bi-encoder model with a margin ranking loss: L =\\n\\u00cd \\u00cd=1 max2(q, d ) \\u2212 2(q, d\\u2032 ) + , 0 with randomly\\nsampled negatives \\u2032 and  = 1. Our cross-encoders are trained with\\na cross-entropy loss: L = \\u00cd \\u00cd=1 log( \\u00cd\\u2032\\u2032 ). For training, 4\\nnegative example items \\u2032 are randomly sampled from ranks\\n100300 from our trained bi-encoder. At test time, we retrieve the top\\n200 items with our trained bi-encoder and re-rank them with the\\ncross-encoder - we evaluate both these components in experiments\\nand refer to them as BiEnc-Mint and CrEnc-Mint.\\n4\"\n      ]\n    },\n    {\n      \"title\": \"EXPERIMENTS AND RESULTS\",\n      \"paragraphs\": [\n        \"Next, we evaluate Mint on a publicly available test collection for\\nNDR and present a series of ablations.\\n4.1\",\n        \"\\n        Experimental Setup\\n4.1.1 Datasets. We perform evaluations on an NDR dataset for\\npoint-of-interest (POI) recommendation Pointrec \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Results\",\n      \"paragraphs\": [\n        \"Table 1 presents the performance of the proposed method compared\\nagainst baselines. Here, bold numbers indicate the best-performing\\nmodel, and superscripts indicate statistical significance computed\\nwith two-sided t-tests at  < 0.05.\",\n        \"\\n        Here, we first note the performance of baseline approaches. We\\nsee BM25 outperformed by Contriver, a transformer bi-encoder\\nmodel trained for zero-shot retrieval; this mirrors prior work \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Ablations\",\n      \"paragraphs\": [\n        \"In Table 2, we ablate various design choices in Mint. Diferent\\nchoices result in diferent training sets for the BiEnc and CrEnc\\nmodels. Also, note that in reporting ablation performance for CrEnc,\\nwe still use the performant BiEnc-Mint model for obtaining\\nnegative examples for training and first-stage ranking. Without\\nhighquality negative examples, we found CrEnc to result in much poorer\\nperformance.\",\n        \"No item fil tering. Since synthetic queries are unlikely to\\nrep\\nresent all the items of a user, Mint excludes user items { }=1\\nwhich have a low likelihood of being generated from the document\\n(\\u00a73.2.2). Without this step, we expect the training set for training\\nretrieval models to be larger and noisier. In Table 2, we see that\\nexcluding this step leads to a lower performance for BiEnc and\\nCrEnc, indicating that the quality of data obtained is important for\\nperformance.\",\n        \"\\n        6B LLM for QGen. Mint relies on using an expensive 175B\\nparameter InstructGPT model for QGen. Here, we investigate the\\neficacy for generating  for { }=1 with a 6B parameter\\nInstructGPT model (text-curie-001). We use an identical setup to the\\n175B LLM for this. In Table 2, we see that training on the synthetic\\nnarrative queries of the smaller LLM results in worse models \\u2013\\noften underperforming the baselines in Table 1. This indicates the\\ninability of a smaller model to generate complex narrative queries\\nwhile conditioning on a set of user items. This necessity of a larger\\nLLM for generating queries in complex retrieval tasks has been\\nobserved in prior work \\n        \",\n        \"\\n          MAP\\n6B LLM for Item Queries. We find a smaller 6B LLM to result\\nin poor quality data when used to generate narrative queries\\nconditioned on { }=1. Here we simplify the text generation task \\u2013\\nusing a 6B LLM to generate queries for individual items  . This\\nexperiment also mirrors the setup for generating synthetic queries\\nfor search tasks \\n          \"\n      ]\n    },\n    {\n      \"title\": \"Pointrec\",\n      \"paragraphs\": [\n        \"\\n          MAP\\n6B LLM for Item Queries. We find a smaller 6B LLM to result\\nin poor quality data when used to generate narrative queries\\nconditioned on { }=1. Here we simplify the text generation task \\u2013\\nusing a 6B LLM to generate queries for individual items  . This\\nexperiment also mirrors the setup for generating synthetic queries\\nfor search tasks \\n          \"\n      ]\n    },\n    {\n      \"title\": \"CONCLUSIONS\",\n      \"paragraphs\": [\n        \"In this paper, we present Mint, a data augmentation method for the\\nnarrative-driven recommendation (NDR) task. Mint re-purposes\\nhistorical user-item interaction datasets for NDR by using a 175B\\nparameter large language model to author long-form narrative queries\\nwhile conditioning on the text of items liked by users. We evaluate\\nbi-encoder and cross-encoder models trained on data from Mint on\\nthe publicly available Pointrec test collection for narrative-driven\\npoint of interest recommendation. We demonstrate that the\\nresulting models outperform several strong baselines and ablated models\\nand match or outperform a 175B LLM directly used for NDR in a\\n1-shot setup.\",\n        \"However, Mint also presents some limitations. Given our use of\\nhistorical interaction datasets for generating synthetic training data\\nand the prevalence of popular interests in these datasets longer,\\ntailed interests are unlikely to be present in the generated\\nsynthetic datasets. In turn, causing retrieval models to likely see poorer\\nperformance on these requests. Our use of LLMs to generate\\nsynthetic queries also causes the queries to be repetitive in structure,\\nlikely causing novel longer-tail queries to be poorly served. These\\nlimitations may be addressed in future work.\",\n        \"MAP\",\n        \"Recall@100\",\n        \"Besides this, other avenues also present rich future work. While\\nMint leverages a 175B LLM for generating synthetic queries, smaller\\nparameter LLMs may be explored for this purpose - perhaps by\\ntraining dedicated QGen models. Mint may also be expanded to\\nexplore more active strategies for sampling items and users for\\nwhom narrative queries are authored - this may allow more\\neficient use of large parameter LLMs while ensuring higher quality\\ntraining datasets. Next, the generation of synthetic queries from\\nsets of documents may be explored for a broader range of retrieval\\ntasks beyond NDR given its promise to generate larger training\\nsets \\u2013 a currently underexplored direction. Finally, given the lack of\\nlarger-scale test collections for NDR and the efectiveness of LLMs\\nfor authoring narrative queries from user-item interaction, fruitful\\nfuture work may also explore the creation of larger-scale datasets\\nin a mixed-initiative setup to robustly evaluate models for NDR.\"\n      ]\n    },\n    {\n      \"title\": \"ACKNOWLEDGMENTS\",\n      \"paragraphs\": [\n        \"We thank anonymous reviewers for their invaluable feedback. This\\nwork was partly supported by the Center for Intelligent\\nInformation Retrieval, NSF grants IIS-1922090 and 2143434, the Ofice of\\nNaval Research contract number N000142212688, an Amazon Alexa\\nPrize grant, and the Chan Zuckerberg Initiative under the project\\nScientific Knowledge Base Construction. Any opinions, findings\\nand conclusions or recommendations expressed here are those of\\nthe authors and do not necessarily reflect those of the sponsors.\"\n      ]\n    }\n  ]\n}",
    "{\n  \"DocumentTitle\": \"Framing the News: From Human Perception to Large Language Model Inferences\",\n  \"Auteurs\": \"David Alonso del Barrio, Daniel Gatica-Perez, \",\n  \"Institutions\": \"Idiap Research Institute and EPFL, Idiap Research Institute, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"CCS CONCEPTS\",\n      \"paragraphs\": [\n        \"\\u2022 Computing methodologies \\u2192 Information extraction; \\u2022\\nHuman-centered computing \\u2192 Text input.\\nCovid-19 no-vax, news framing, GPT-3, prompt-engineering,\\ntransformers, large language models\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\",\n        \"ICMR \\u201923, June 12\\u201315, 2023, Thessaloniki, Greece\\n\\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-0178-8/23/06. . . $15.00\\nhttps://doi.org/10.1145/3591106.3592278\"\n      ]\n    },\n    {\n      \"title\": \"INTRODUCTION\",\n      \"paragraphs\": [\n        \"\\n        In recent years, there has been a proliferation in the use of concepts\\nsuch as data journalism, computational journalism, and\\ncomputerassisted reporting \\n        \",\n        \"Frame analysis is a concept from journalism, which consists of\\nstudying the way in which news stories are presented on an issue,\\nand what aspects are emphasized: Is a merely informative vision\\ngiven in an article? Or is it intended to leave a moral lesson? Is\\na news article being presented from an economic point of view?\\nOr from a more human, emotional angle? The examples above\\ncorrespond to diferent frames with which an article can be written.\",\n        \"\\n        The concept of news framing has been studied in computing as\\na step beyond topic modeling and sentiment analysis, and for this\\npurpose, in recent years, pre-trained language models have been\\nused for fine-tuning the classification process of these frames \\n        \",\n        \"Our work aims to address this research gap by posing the\\nfollowing research questions:\",\n        \"RQ1: What are the main frames in the news headlines about\\nthe anti-vaccine movement, as reported in newspapers across 5\\nEuropean countries?\",\n        \"RQ2: Can prompt engineering be used for classification of\\nheadlines according to frames?\",\n        \"By addressing the above research questions, our work makes the\\nfollowing contributions:\",\n        \"Contribution 1. We implemented a process to do human\\nannotation of the main frame of 1786 headlines of articles about the\\nCovid-19 no-vax movement, as reported in 19 newspapers from 5\\nEuropean countries (France, Italy, Spain, Switzerland and United\\nKingdom.) At the headline level, we found that the predominant\\nframe was human interest, where this frame corresponds to a\\npersonification of an event, either through a statement by a person,\\nor the explanation of a specific event that happened to a person.\\nFurthermore, we found a large number of headlines annotated as\\ncontaining no frame, as they simply present information without\\nentering into evaluations. We also found that for all the countries\\ninvolved, the distribution of frame types was very similar, i.e.,\\nhuman interest and no frame are the two predominant frames. Finally,\\nthe generated annotations allowed to subsequently study the\\nperformance of a large language model.\",\n        \"Contribution 2. We studied the performance of GPT-3.5 on\\nthe task of frame classification of headlines. In addition to using\\nthe fine-tuning approach from previous literature, we propose an\\nalternative approach for frame classification that requires no labeled\\ndata for training, namely prompt-engineering using GPT-3.5. The\\nresults show that fine-tuning with GPT-3.5 produces 72% accuracy\\n(slightly higher than other smaller models), and that the\\npromptengineering approach results in lower performance (49% accuracy.)\\nOur analysis also shows that the subjectivity of the human labeling\\ntask has an efect on the obtained accufracy.\",\n        \"The paper is organized as follows. In Section 2, we discuss related\\nwork. In Section 3, we describe the news dataset. In Section 4, we\\ndescribe the methodology for both human labeling and machine\\nclassification of news frames. We present and discuss results for\\nRQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide\\nconclusions in Section 7.\\n2\"\n      ]\n    },\n    {\n      \"title\": \"RELATED WORK\",\n      \"paragraphs\": [\n        \"\\n        Framing has been a concept widely studied in journalism, with a\\ndefinition that is rooted in the study of this domain \\n        \",\n        \"\\n        For frame recognition, there are two main approaches: the\\ninductive approach \\n        \",\n        \"\\n        We now compare the two approaches on a common topic, such\\nas Covid-19. Ebrahim et al. \\n        \",\n        \"We decided to follow the deductive approach because a\\npredeifned list of frames allows to compare among topics, countries,\\nprevious literature, and also because they represent a fixed list of\\nlabels for machine classification models. Furthermore, the\\ninductive approach tends to be more specific to a topic, and from the\\ncomputing viewpoint, past work has tried to justify topic modeling\\nas a technique to extract frames from articles.\",\n        \"\\n        Yl\\u00e4-Antitila et al. \\n        \",\n        \"\\n        From Entman\\u2019s definition of frame \\n        \",\n        \"\\n        Isoaho et al.\\n        \",\n        \"\\n        We also consider that the larger the number of possible frame\\ntypes, the more likely it is to end up doing topic modeling instead of\\nframe analysis. Using a deductive approach, Dallas et al. \\n        \",\n        \"\\n        A final decision in our work was the type of text to analyze,\\nwhether headlines or whole article. For this decision, the chosen\\nclassification method was also going to be important. For example,\\nKhanehzar et al. \\n        \",\n        \"\\n        Continuing with the question of the methods used for\\nclassiifcation, much work has been developed in prompt engineering,\\nespecially since the release of GPT-3. Liu et al.\\n        \",\n        \"\\n        As mentioned before, the emergence of giant models like GPT-3,\\nBLOOM, and ChatGPT are a very active research topic. To the best\\nof our knowledge, on one hand our work extends the computational\\nanalysis of news related to the covid-19 no-vax movement, which\\nillustrates the influence of the press on the ways societies think\\nabout relevant issues \\n        \"\n      ]\n    },\n    {\n      \"title\": \"3 DATA: EUROPEAN COVID-19 NEWS\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"DATASET\",\n      \"paragraphs\": [\n        \"\\n        We used part of the European Covid-19 News dataset collected in\\nour recent work \\n        \",\n        \"In the first phase, annotators had to read the codebook and get\\nfamiliar with the task. In the second phase, they were asked to\\nidentify the main frame in the same subset of 50 headlines. At the\\nend of the second phase, the intercoder reliability (ICR) was 0.58\\nbetween the 2 annotators. We analyzed those cases where there\\nwere discrepancies, and observed that in some cases, there was not a\\nunique main frame, because both annotators had valid arguments to\\nselect one of the frames. In other cases, the discrepancies were due\\nto slight misunderstanding of the definitions. In the third phase, the\\nannotators coded again 50 headlines, and the ICR increased to was\\n0.66. We realized that the possibility of having two frames remained.\\nThey discussed the cases in which they had disagreed, and if the\\nother person\\u2019s arguments were considered valid, it could be said that\\nthere were two frames. After this three-phase training procedure,\\nannotators were ready to annotate the dataset independently. We\\ndivided the dataset into two equal parts, and each person annotated\\n893 headlines.\\n4.2\"\n      ]\n    },\n    {\n      \"title\": \"Fine-tuning GPT-3.5 and BERT-based models\",\n      \"paragraphs\": [\n        \"With the annotated dataset, we investigated two NLP approaches:\\nthe first one involves fine-tuning a pre-trained model; the second\\none is prompt engineering. Pre-trained language models have been\\ntrained with large text strings based on two unsupervised tasks,\\nnext sentence prediction and masked language model. Figure 1\\nsummarizes these techniques.\",\n        \"\\n        In the first approach, a model with a fixed architecture is\\npretrained as a language model (LM), predicting the likelihood of the\\nobserved textual data. This can be done due to the availability of\\nlarge, raw text data needed to train LMs. This learning process can\\nproduce general purpose features of the modeled language. The\\nlearning process produces robust, general-purpose features of the\\nlanguage being modeled. The above pre-trained LM is then adapted\\nto diferent downstream tasks, by introducing additional parameters\\nand adjusting them using task-specific objective functions. In this\\napproach, the focus was primarily on goal engineering, designing\\nthe training targets used in both the pre-training and the fine-tuning\\nstages \\n        \",\n        \"We present an example to illustrate the idea. Imagine that the\\ntask is sentiment analysis, and we have a dataset with sentences\\nand their associated sentiment, and a pre-trained model, which is a\\nsaved neural network trained with a much larger dataset. For that\\npre-trained model to address the target task, we unfreeze a few of\\nthe top layers of the saved model base and jointly train both the\\nnewly-added classifier layers and the last layers of the base model.\\nThis allows to \\\"fine-tune\\\" the higher-order feature representations\\nin the base model to make them more relevant for the sentiment\\nanalysis task. In this way, instead of having to obtain a very large\\ndataset with target labels to train a model, we can reuse the\\npretrained model and use a much smaller train dataset. We use a part\\nof our dataset as examples for the model to learn the task, while\\nthe other part of the dataset is used to evaluate model performance.\",\n        \"Previous works related to frame classification in the computing\\nliterature have used fine-tuning, BERT-based models. In our work,\\nwe have done the same as a baseline, but we aimed to go one step\\nfurther and also produce results using fine-tuning of GPT-3.5.\\n4.3\"\n      ]\n    },\n    {\n      \"title\": \"Prompt-engineering with GPT-3.5\",\n      \"paragraphs\": [\n        \"\\n        Model fine-tuning has been widely used, but with the emergence\\nof generative models such as GPT-3, another way to approach\\nclassification tasks has appeared. The idea is to use the pre-trained\\nmodel directly and convert the task to be performed into a format\\nas close as possible to the tasks for which it has been pre-trained.\\nThat is, if the model has been pre-trained from next word prediction\\nas in the case of GPT-3, classification can be done by defining a\\nprompt, where the input to the model is an incomplete sentence,\\nand the model must complete it with a word or several words, just\\nas it has been trained. This avoids having to use part of the already\\nlabeled dataset to teach the task to be performed to the model, and\\na previous labeling is not needed \\n        \",\n        \"\\n        In this approach, instead of adapting pre-trained LMs to\\ndownstream tasks via objective engineering, downstream tasks are\\nreformulated to look more like those solved during the original LM\\ntraining with the help of a textual prompt. For example, when\\nrecognizing the emotion of a social media post, \\u201cI missed the bus today.\\u201d,\\nwe may continue with a prompt \\u201cI felt so _\\u201d, and ask the LM to\\nifll the blank with an emotion-bearing word. Or if we choose the\\nprompt \\u201cEnglish: I missed the bus today. French: _\\u201d), an LM may\\nbe able to fill in the blank with a French translation. In this way,\\nby selecting the appropriate prompts, we can influence the model\\nbehavior so that the pre-trained LM itself can be used to predict the\\ndesired output, even without any additional task-specific training\\n\\n        \",\n        \"\\n        We use this emerging NLP approach to classify frames at headline\\nlevel. We are not aware of previous uses of this strategy to classify\\nframes as we propose here. The idea is the following. Prompt\\nengineering consists of giving a prompt to the model, and understands\\nthat prompt as an incomplete sentence. To do prompt\\nengineering with our dataset, we needed to define an appropriate prompt\\nthat would produce the headline frames as output. We defined\\nseveral experiments with the Playground of GPT-3, in order to find\\nthe best prompt for our task. In our initial experiments, we\\nfollowed existing approaches in prompt engineering to do sentiment\\nanalysis, where the individual answer was an adjective, and this\\nadjective was matched with a sentiment. In a similar fashion, we\\ndecided to build a thesaurus of adjectives that define each of the\\nframes. For instance, the human interest frame could be\\n\\u2019interesting\\u2019, \\u2019emotional\\u2019, \\u2019personal\\u2019, \\u2019human\\u2019. The conflict frame could be:\\n\\u2019conflictive\\u2019, \\u2019bellicose\\u2019, \\u2019troublesome\\u2019, \\u2019rowdy\\u2019, \\u2019quarrelsome\\u2019,\\n\\u2019troublemaker\\u2019, \\u2019agitator\\u2019, etc. After the list of adjectives was defined,\\nwe needed to define the prompt in order to get, as an answer, one\\nof the adjectives in our thesaurus to match them with the frame.\\nWe used the GPT-3 playground using the headline as input and\\nasking for the frame as output, but the strategy did not work. In\\nour final experiment, instead of giving the headline as input, we\\ngave the definitions of each type of frame plus the headline, and we\\nasked the model to choose between the diferent types of frames\\nas output. In this way, the output of the model was directly one of\\nthe frames, and we avoided the step of matching adjectives with\\nframes. An example is shown in Figure 2.\\nFor the GPT-3 configuration 1, there are 3 main concepts:\\n\\u2022 TEMPERATURE \\n        \",\n        \"After testing with the GPT-3 playground and varying diferent\\nhyper-parameters to assess performance, we set the temperature to\\n0, since the higher the temperature the more random the response.\\nFurthermore, the Top-p parameter was set to 1, as it would likely\\nget a set of the most likely words for the model to choose from. The\\nmaximum number of tokens was set to 2; in this way, the model\\nis asked to choose between one of the responses. As a model, we\\nused the one with the best performance at the time of experimental\\ndesign, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\\n5\"\n      ]\n    },\n    {\n      \"title\": \"RESULTS: HUMAN LABELING OF FRAMES\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"IN NO-VAX NEWS HEADLINES (RQ1)\",\n      \"paragraphs\": [\n        \"In this section, we present and discuss the results of the analysis\\nrelated to our first RQ.\",\n        \"Figure 3 shows the distribution of frames per country at headline\\nlevel, with human interest and no-frame being the predominant\\n1https://beta.openai.com/docs/introduction\\nones. Attribution of responsibility is the third one except in\\nSwitzerland, where the corresponding frame is conflict. Finally, morality\\nand economic are the least represented in the dataset for every\\ncountry.\",\n        \"The monthly distribution of frames aggregated for all countries\\nis shown in Fig. 4. We can see two big peaks, the first one in January\\n2021 and the second one in August 2021. In all countries, the\\nvaccination process started at the end of December 2020, so it makes\\nsense that the no-vax movement started to be more predominant in\\nthe news in January 2021. Human interest is the most predominant\\nframe. Manual inspection shows that this is because the headlines\\nare about personal cases of people who are pro- or anti- vaccine.\\nAttribution of responsibility is also present. Manual inspection\\nindicates that local politicians and health authorities had to make\\ndecisions about who could be vaccinated at the beginning of the\\nprocess. The second peak at the end of summer 2021 coincided\\nwith the health pass (also called Covid passport in some countries),\\nand we can observe a peak in the curve corresponding to the\\nconlfict frame, reflecting the demonstrations against the measure of\\nmandatory health passes taken by country governments.\",\n        \"\\n        In Figure 5, we compare the sentiment per frame and per country,\\nto understand if there were any major diferences. The sentiment\\nanalysis labels were obtained using BERT-sent from the Hugging\\nFace package \\n        \",\n        \"Switzerland, and the United Kingdom,\\n\\u2022 No frame: 20-30% of negative content.\",\n        \"Regarding the results of the annotation process, the fact that the\\ndistribution of the 6 frame types is relatively similar between\\ncountries suggests that the anti-vaccine movement issue was treated\\nin a similar way in these countries. The fact that human interest\\nis the most dominant frame indicates that this issue was treated\\nfrom a more human and emotional approach, with headlines about\\npersonal experiences, celebrities giving their opinion about\\nvaccination, and politicians defending vaccine policies. Moreover, the\\nreason for many headlines being classified as no-frame is partly\\ndue to how data was selected. We chose articles that contained\\nwords related to no-vax, either in the headline or in the article. This\\nresulted in many headlines not containing anything specific related\\nto no-vax, while the no-vax content was actually included in the\\nmain text of the corresponding articles.\",\n        \"It is worth mentioning that prior to obtaining the results, we had\\nexpected that attribution of responsibility would be among the most\\nprominent frames, since governments took many measures such as\\nmandatory health pass requirements to access certain sites; we had\\nalso expected that the conflict frame would be prominent, since\\nthere were many demonstrations in Europe. In reality, however,\\nthese frames categories were not reflected as frequently at the\\nheadline level.\",\n        \"Regarding the analysis at the temporal level, it is clear that certain\\nevents were captured by the press, such as the start of vaccination\\nor the mandatory vaccination passport.\",\n        \"\\n        Finally, the sentiment analysis of the diferent frames shows that\\nthe predominant tone in all of them is neutral or negative, with very\\nsimilar trends between countries. This association between\\nsentiment analysis and frames has been discussed in previous literature\\n\\n        \"\n      ]\n    },\n    {\n      \"title\": \"RESULTS: GPT-3.5 FOR FRAME\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"CLASSIFICATION OF HEADLINES (RQ2)\",\n      \"paragraphs\": [\n        \"Here, we present and discuss the results related to our second RQ.\\n6.1\"\n      ]\n    },\n    {\n      \"title\": \"Fine-tuning GPT-3.5\",\n      \"paragraphs\": [\n        \"Table 4 shows the results of the 6-class classification task using\\n5-cross validation. Three models were used: GPT-3.5 and two\\nBERTbased models. We observe that, on average, GPT-3.5 performs better\\nthan the BERT-based models. This is somehow expected as\\nGPT3.5 is a much larger model. Overall, in the case of fine-tuning, the\\nbest performance for the six-class frame classification task is 72%\\naccuracy, which is promising, with an improvement over previous\\nmodels based on BERT. Yet, it should be noted that the performance\\ndiferences are modest (2% improvement between GPT-3.5 and\\nRoBERTa).\",\n        \"On the other hand, BERT is open-source, while GPT-3 has an\\neconomic cost as the use of the model is not free, which monetarily\\nlimits the number of experiments that can be performed with it,\\nas well as the diferent configurations one can explore to improve\\nperformance. This is important because much of the improvement\\nin performance requires empirical explorations of model parameters\\nMore specifically, the cost of an experiment for each of the folds has\\na cost of 4 dollars (at the time of writing this paper.) This represents\\na limitation in practice.\",\n        \"\\n        Furthermore, GPT-3 has a significant carbon footprint. Similarly,\\nfor prompt engineering (discussed in the next subsection), choosing\\nthe right prompt (i.e., the words that best define the task so that the\\nmodel is able to perform adequately) is also based on trial and error.\\nThis also has an impact on carbon footprint. In connection with\\nthis topic, Strubell et al.\\n        \"\n      ]\n    },\n    {\n      \"title\": \"Prompt-engineering with GPT-3.5\",\n      \"paragraphs\": [\n        \"For each headline, we got the frame that the model considered the\\nmost likely, and we compared these GPT-3.5 inferences with the\\nframes labeled by the annotators. The agreement between model\\nand annotator was of 49%. Analyzing the results, and specifically\\nlooking at the cases where the annotator and GPT-3.5 disagreed,\\nwe discovered that according to the frame definitions, the model\\nin some cases proposed a frame that indeed made sense. This\\nobservation, together with our previous experience in the annotation\\nprocess, where headlines could have more than one valid frame,\\nled us to design a second post-hoc experiment. We took all the\\nheadlines where each of the two annotators had disagreed with\\nGPT-3.5, and we asked the annotators to state whether they would\\nagree (or not) with each GPT-inferred label for a given headline.\\nIt is important to emphasize that the annotators did not know the\\norigin of that label, i.e., they did not know if it was the label they\\nhad originally assigned, or if it was a random one. In this way, we\\ncould quantify how GPT-3.5 worked according to valid arguments\\nprovided by the annotators. In this post-hoc experiment, the model\\nagreed in 76% of cases with the annotators.\",\n        \"\\n        Looking at the results of the classification models, the 49%\\naccuracy of the prompt-engineering approach can be considered low,\\nyet we consider that it is a valid avenue for further investigation,\\nas in the second post-hoc analysis, we found that the model agrees\\nwith human annotators in 76% of the cases. Clearly, framing\\ninvolves aspects of subjectivity \\n        \",\n        \"\\n        News reading is never fully objective, and the annotators\\nengaged in the frame classification task, influenced by their personal\\nstate of mind, experience, and culture, may perceive information\\ndiferently. Monarch afirms that \\\"for simple tasks, like binary labels\\non objective tasks, the statistics are fairly straightforward to decide\\nwhich is the \\u2018correct\\u2019 label when diferent annotators disagree. But\\nfor subjective tasks, or even objective tasks with continuous data,\\nthere are no simple heuristics for deciding what the correct label\\nshould be\\\" \\n        \",\n        \"\\n        Subjectivity is involved in both the generation and perception\\nof information: the assumption that there is only one frame is\\ncomplicated by the point of view of the reader. In the case of news, the\\ninformation sender (the journalist) has an intention, but the receiver\\n(the reader) plays a role and is influenced by it. In psychology, this\\nis known as the lens model of interpersonal communication, where\\nthe sender has certain objectives, but the receiver can interpret\\nor re-interpret what the sender wants to say, with more or less\\naccuracy \\n        \",\n        \"Following this discussion on subjectivity, the question arose as to\\nwhat would happen if, instead of headlines, we used the complete\\narticle as a source of analysis. We wondered if longer text could\\nmake the frame labeling task clearer than when using headlines.\\nYet another possible hypothesis is that having to read longer texts\\ncould lead to the same subject being presented from diferent angles.\\nPlease recall that in the existing literature discussed in Section 2,\\nboth headlines and full articles have been used from frame analysis\\n(see Table 1.) This remains as an issue for future work.\\n7\"\n      ]\n    },\n    {\n      \"title\": \"CONCLUSIONS\",\n      \"paragraphs\": [\n        \"In this paper, we first presented an analysis of human-generated\\nnews frames on the covid-19 no-vax movement in Europe, and\\nthen studied diferent approaches using large language models for\\nautomatic inference of frames. We conclude by answering the two\\nresearch questions we posed:\",\n        \"RQ1: What are the main frames in the news headlines about the\\ncovid-19 anti-vaccine movement in 5 European countries? After\\nannotating the headlines, we found that of the 1786 headlines,\\nthe predominant frame is human interest (45.3% of cases), which\\npresents a news item with an emotional angle, putting a face to a\\nproblem or situation. We also found that a substantial proportion\\nof headlines were annotated as not presenting any frame (40.2% of\\ncases). Finally, the other frame types are found more infrequently.\",\n        \"RQ2: Can prompt engineering be used for classification of\\nheadlines according to frames? We first used fine-tuning of a number of\\nlanguage models, and found that GPT-3.5 produced classicfiation\\naccuracy of 72% on a six-frame classification task. This represented a\\nmodest 2% improvement over BERT-based models, at a significantly\\nlarger environmental cost. We then presented a new way of\\nclassifying frames using prompts. At the headline level, inferences made\\nwith GPT-3.5 reached 49% of agreement with human-generated\\nframe labels. In many cases, the GPT-3.5 model inferred frame\\ntypes that were considered as valid choices by human annotators,\\nand in an post-doc experiment, the human-machine agreement\\nreached 76%. These results have opened several new directions for\\nfuture work.\"\n      ]\n    },\n    {\n      \"title\": \"ACKNOWLEDGMENTS\",\n      \"paragraphs\": [\n        \"This work was supported by the AI4Media project, funded by the\\nEuropean Commission (Grant 951911) under the H2020 Programme\\nICT-48-2020. We also thank the newspapers for sharing their online\\narticles. Finally, we thank our colleagues Haeeun Kim and Emma\\nBouton-Bessac for their support with annotations, and Victor Bros\\nand Oleksii Polegkyi for discussions.\"\n      ]\n    }\n  ]\n}",
    "{\n  \"DocumentTitle\": \"Generating Diverse Code Explanations using the GPT-3 Large Language Model\",\n  \"Auteurs\": \"Stephen MacNeil, Seth Bernstein, Andrew Tran, Erin Ross, Dan Mogil, Ziheng Huang, \",\n  \"Institutions\": \"Temple University, University of California-San Diego, \",\n  \"Abstract\": \"no\",\n  \"Sections\": [\n    {\n      \"title\": \"USE CASES\",\n      \"paragraphs\": [\n        \"\\n        To understand the types of explanations GPT-3 \\n        \",\n        \"\\n        Analyzing and explaining time complexity\\nInstructors rate time complexity as the most dificult programming\\ntopic \\n        \",\n        \"\\n        Identifying common mistakes made by\\nbeginner programmers\\nCommonality exists in how students solve programming\\nproblems \\n        \",\n        \"\\n        Summarizing code at multiple levels of\\nabstraction\\nBefore understanding how a code snippet executes, it is often useful\\nto understand the purpose of the code \\n        \"\n      ]\n    },\n    {\n      \"title\": \"DISCUSSION\",\n      \"paragraphs\": [\n        \"Our three use cases demonstrate the potential for GPT-3 to explain\\ncode for intro CS students. Our poster presentation will feature all\\neight explanation types as a design space of explanations to convey\\nthe diversity of explanations that can be generated by LLMs. We will\\nhighlight best practices for generating efective explanations and\\npitfalls that lead to less efective explanations. We are evaluating\\nthe usefulness of these explanations in a series of summer classes.\"\n      ]\n    }\n  ]\n}",
    "{\n  \"DocumentTitle\": \"Improved stochastic subset optimization method for structural design optimization Mohd Aman Khalid , Sahil Bansal *\",\n  \"Auteurs\": \"Mohd Aman Khalid, Sahil Bansal, \",\n  \"Institutions\": \"Department of Civil Engineering, Indian Institute of Technology Delhi, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"-\",\n      \"paragraphs\": [\n        \"A R T I C L E I N F O\"\n      ]\n    },\n    {\n      \"title\": \"1. Introduction\",\n      \"paragraphs\": [\n        \"\\n        Structural optimization may be defined as the rational establishment\\nof an economical structural design with the available resources while\\nsatisfying specific performance criteria. In general terms, the economy\\nmay be characterized by minimum weight, minimum cost, maximum\\nutility, or even minimum probability of failure. Broadly, structural\\noptimization can be categorized into deterministic and stochastic\\noptimization \\n        \",\n        \"\\n        In any practical situation, several parameters, such as loadings,\\nstructural parameters, geometric parameters, operation conditions, etc.,\\nare either not known at the design stage or are subjected to random\\nlfuctuations that give rise to performance variability and affect the\\nperformance of a system \\n        \",\n        \"\\n        Consider an engineering system that involves deterministic design\\nparameters \\u03c6, and uncertain variables \\u03b8 = \\u03b81\\u22ef\\u03b8n\\u03b8 T \\u2208 \\u0398\\u2282Rn\\u03b8 following\\na joint PDF p(\\u03b8|\\u03c6), where \\u0398 denotes the parameter space of the\\nuncertain variables. The classical statement of stochastic optimization is\\nmathematically expressed as:\\nminimize : E\\u03b8h(\\u03c6, \\u03b8)\\n\\u03c6\\u2208\\u03a6\\n(2)\\nwhere, h(\\u03c6, \\u03b8) : Rn\\u03b8+n\\u03c6 \\u2192R is the structural performance function, and\\nE\\u03b8 \\u22c5  denotes expectation with respect to the PDF for \\u03b8. Note that the\\nobjective function in the optimization problem in (2) is the expectation\\nE\\u03b8h(\\u03c6, \\u03b8) which is a deterministic function. It\\u2019s worth mentioning that\\nstochastic optimization may also involve other stochastic measures such\\nas variance or quantile values. However, these stochastic measures can\\nrarely be evaluated analytically; therefore, several methods have been\\nproposed for solving stochastic optimization problems. These\\nspecialized methods include, for example, sample average approximation,\\nstochastic approximation, stochastic subset optimization, and\\napproaches based on the use of Taylor series expansion \\n        \",\n        \"\\n        Taflanidis and Beck \\n        \",\n        \"\\n        Since the introduction of SSO, several extensions of SSO have been\\nproposed. An extension of SSO termed Non-Parametric SSO, which\\nadopts kernel density estimation to approximate the objective function,\\nis presented in \\n        \",\n        \"In this paper, an improved version of SSO is developed to overcome\\nthe shortcomings of the original SSO. This new version of the algorithm,\\nas mentioned earlier, is named iSSO (improved SSO). Voronoi\\ntessellation is implemented to partition the design space into non-overlapping\\nsubregions (a set of Voronoi cells) using the pool of samples\\ndistributed according to the auxiliary PDF. The admissible set (a set of all\\nadmissible subregions) is then defined as a set containing all subsets of\\nthe set of Voronoi cells. This approach is able to capture the regions with\\nlower objective function values even if they are disjointed or when the\\ndesign space is complex. The details of the Voronoi tessellation are\\npresented in Appendix A. A double-sort algorithm is then implemented\\nto identify the optimal subset containing the smallest volume density.\",\n        \"In the next section, the original SSO is reviewed. Section 3 presents\\nthe general theoretical and computational framework for the iSSO\\nalgorithm. Section 4 considers several optimization problems to illustrate\\nthe effectiveness and efficiency of the proposed iSSO algorithm.\"\n      ]\n    },\n    {\n      \"title\": \"2. Original stochastic subset optimization\",\n      \"paragraphs\": [\n        \"\\n        In SSO, say at the i + 1th iteration, the design space is represented by\\na subset I(i), where I(i) \\u2208 I(i 1)\\u22c5\\u22c5\\u22c5 \\u2208 I(0) \\u2208 \\u03a6. Following the augmented\\nformulation concept initially discussed in \\n        \",\n        \"\\u20d2\\n\\u03c0 \\u03c6, \\u03b8\\u20d2I(i))=\",\n        \"\\u20d2\\nh(\\u03c6, \\u03b8)p \\u03c6, \\u03b8\\u20d2I(i))\",\n        \"E\\u03c6,\\u03b8hs(\\u03c6, \\u03b8)\",\n        \"\\u20d2\\n\\u221dh(\\u03c6, \\u03b8)p \\u03c6, \\u03b8\\u20d2I(i))\\nE\\u03c6,\\u03b8h(\\u03c6, \\u03b8) =\",\n        \"\\u03a6 \\u0398\\nwhere, p(\\u03c6, \\u03b8|I(i)) = p(\\u03b8|\\u03c6)p(\\u03c6|I(i)). Note that if h(\\u03c6, \\u03b8)\\u2264 0, it must be\\nsuitably transformed to ensure that \\u03c0(\\u03c6, \\u03b8|I(i)) \\u2265 0. One way to do this is\\nto define hs(\\u03c6,\\u03b8) = h(\\u03c6, \\u03b8) s, since E\\u03b8hs(\\u03c6,\\u03b8) = E\\u03b8h(\\u03c6,\\u03b8) s, that is,\\nthe two expected values differ only by a constant, and the optimization\\nof the expected value of h( \\u22c5 ) is equivalent, in terms of the optimal design\\nchoice, to optimization for the expected value for hs( \\u22c5 ). In the above\\nequation, the denominator is a normalizing constant given by:\\n\\u222b \\u222b\",\n        \"\\u20d2\\nh(\\u03c6, \\u03b8)p \\u03c6, \\u03b8\\u20d2I(i))d\\u03b8d\\u03c6.\",\n        \"Although this expected value is not explicitly needed, it can be\\ndetermined using any state-of-the-art stochastic simulation method. The\\nobjective function E\\u03b8hs(\\u03c6, \\u03b8) in this context of the auxiliary PDF is\\nexpressed as:\",\n        \"\\u20d2\\n\\u03c0 \\u03c6\\u20d2I(i))\\nE\\u03b8h(\\u03c6, \\u03b8) = p \\u03c6\\u20d2\\u20d2I(i))E\\u03c6,\\u03b8h(\\u03c6, \\u03b8),\",\n        \"\\u20d2\\n\\u03c0 \\u03c6\\u20d2I(i))=\\nwhere, the marginal \\u03c0(\\u03c6|I(i)) is given by:\\n\\u222b\\nI(i)\\n\\u03c0(\\u03c6, \\u03b8)d\\u03b8.\\n(3)\\n(4)\\n(5)\\n(6)\",\n        \"In (5), since E\\u03c6,\\u03b8h(\\u03c6, \\u03b8) is a normalizing constant, minimization of\\nE\\u03b8h(\\u03c6, \\u03b8) is equivalent to minimization of J(\\u03c6), which is equal to:\\n\\u20d2\\nJ \\u03c6\\u20d2\\u20d2I(i))= E\\u03b8hs(\\u03c6, \\u03b8) = \\u03c0 \\u03c6\\u20d2\\u20d2I(i))).\",\n        \"E\\u03c6,\\u03b8hs(\\u03c6, \\u03b8) p \\u03c6\\u20d2I(i)\\n(7)\",\n        \"\\n        The estimation of the marginal \\u03c0(\\u03c6|I(i)) in (7) is necessary to\\nminimize J(\\u03c6|I(i)). Analytical approximations of \\u03c0(\\u03c6|I(i)) based on kernel\\ndensity approaches or the maximum entropy method might be arduous\\nin case of complex problems, such as when design parameters n\\u03c6 are\\nlarge, or the sensitivity for some design parameters is complex \\n        \",\n        \"\\n        The sensitivity of objective function E\\u03b8hs(\\u03c6, \\u03b8) to \\u03c6 is determined by\\nevaluating the average value (or equivalently volume density) of J(\\u03c6|\\nI(i)) over any subset I in I(i), which is denoted by H(I) and defined as:\\n\\u222b \\u222b \\u03c0 \\u03c6|I(i)) \\u222b\\nH(I) = V1I I J \\u03c6|I(i))d\\u03c6 = V1I I p \\u03c6|I(i))d\\u03c6 = VVI(Ii) I \\u03c0 \\u03c6|I(i))d\\u03c6 (8)\\nwhere, VI is the volume of subset I. Based on the samples distributed\\naccording to \\u03c0(\\u03c6|I(i)) belonging to I(i), an estimate of H(I) is provided by:\\nwhere, NI(i) is the number of samples distributed as \\u03c0(\\u03c6|I(i)) belonging to\\nI(i), and NI denotes the number of samples from \\u03c0(\\u03c6|I(i)) belonging to the\\nI (NI < NI(i 1) since I\\u2282I(k 1)). Say NI = p0NI(i 1) . A smaller value of \\u03c1\\nresults in a faster decrease in the size of the identified subsets but with\\npoorer accuracy. The use of \\u03c1 equal to 0.1 - 0.2 is suggested in the\\nliterature \\n        \",\n        \"A deterministic optimization, based on the estimate H(I) of H(I), is\\nnext performed to identify the subset I \\u2208 A(\\u03c1i+1), where A(\\u03c1i+1) is a set of\\nadmissible subsets in I(i), that contains the smallest volume density NI/\\nVI, that is,\\nI(i+1) = argminH(I) = argI\\u2208mA(\\u03c1iin+1)NI\",\n        \"I\\u2208A\\u03c1\\nA(i+1) = {I\\u2282I(i) : \\u03c1 = NI /N(i)}\\n\\u03c1\\n/\",\n        \"VI\",\n        \".\",\n        \"\\n        The effectiveness of SSO is dependent on the correct selection of the\\ngeometrical shape and size of the admissible subsets. Choosing a\\ngeometrical shape that effectively investigates the sensitivity of the\\nobjective function to each design variable is essential. The optimization\\nin (10) determines the subset with the smallest average value of J(\\u03c6|I(i))\\n(or equivalently E\\u03b8hs(\\u03c6, \\u03b8)) within the admissible set A(\\u03c1i+1). I(i + 1) is a\\nsubset of the design space I(i) with a high likelihood of containing the\\noptimal design parameters. The above steps are repeated until the\\nstopping criterion is met. This way, SSO adaptively converges to a\\nrelatively small subregion within the original design space. The\\nimplementation of SSO is demonstrated in Fig. 1. The reader may refer to the\\noriginal publication for a detailed explanation of SSO \\n        \",\n        \"\\n        H(I(i)) expresses the average relative sensitivity of E\\u03b8h(\\u03c6,\\u03b8) to \\u03c6. A\\nlow value of H(I(i)) indicates that E\\u03b8h(\\u03c6,\\u03b8) is more sensitive to \\u03c6, and\\nvice versa. A high value of H(I(i)), close to 1 corresponds to a sample\\ndensity in design space I(i) that approximates a uniform distribution and\\nsuggests that the identified subset I(i) has a low likelihood of containing\\n\\u03c6* \\n        \",\n        \"In the proposed approach, the Voronoi tessellation is implemented to\\npartition the design space into non-overlapping subregions (a set of\\nVoronoi cells) using the pool of samples distributed according to this\\nauxiliary PDF. Conceptually, Voronoi tessellation involves partitioning a\\nspace into convex polygons, called Voronoi cells, such that each cell\\ncontains exactly one sample, called a cell-generating sample. Every\\nsample in a given polygon is closer to its generating sample compared to\\nany other. In the proposed approach, the admissible set (a set of all\\nadmissible subspaces) is defined as a set containing all subsets of the set\\nof Voronoi cells. An alternative approach to identify the optimal subset\\nwithout performing any non-smooth deterministic optimization is also\\npresented. The general theoretical and computational framework for the\\niSSO algorithm is presented in the following subsections, and the\\nalgorithm is demonstrated in Fig. 2.\",\n        \"In the proposed approach, at the i + 1th iteration, say N(i) is the\\nnumber of samples distributed as \\u03c0(\\u03c6|I(i)) belonging to the design space\\nI(i). Let nv = N(i) /(1 + \\u03b3), \\u03b3 \\u2265 0 be the number of unique samples. If\\nsampling techniques such as accept rejection, importance sampling, etc.,\\nare used, then \\u03b3 = 0, and each sample in the design space will be unique.\\nHowever, if MCMC sampling techniques are used, the resulting samples\\nwill be correlated, that is \\u03b3 > 0, and we will have repeated samples.\\nAssume that the design space I(i) is divided into v(ki), k = 1\\u22c5\\u22c5\\u22c5nv, Voronoi\\ncells using nv unique samples, and say the Voronoi cell v(ki) contains \\u03b7(ki)\\nrepeated samples, then, an estimate of \\u03c0(\\u03c6|I(i)) is provided by:\\n\\u03b7(i)\\n\\u03c0 \\u03c6|I(i))= N(i)kV(i) \\u2265 0, \\u2200 \\u03c6 \\u2208 v(ki),\\nk\\n\\u222b\\nwhere, Vk(i) is the volume of the kth Voronoi cell. Obviously, I(i) \\u03c0(\\u03c6|I(i))\\nd\\u03c6 = 1.\",\n        \"Similar to the original SSO, the sensitivity of the objective function J\\n(\\u03c6|I(i)) to \\u03c6 is determined by evaluating the average value of J(\\u03c6|I(i))\\nover any subspace I of the design space I(i). Subset I is any subset of\\nnvVoronoi cells (these cells may be disjointed). Since the design space is\\npartitioned into nv subspaces or Voronoi cells, the number of admissible\\nsubsets (proper subsets) is given by 2n\\u03bd 1. Based on the estimate\\n\\u03c0(\\u03c6|I(i)) provided in (11), an estimate of H(I) is provided as:\\nH(I) = NV((ii))  V\\u03b7((((1i1i)))) ++ V\\u03b7((((i22i)))) ++ \\u22ef\\u22ef ++ \\u03b7V(((iS()Si))) .\\n(11)\\n(12)\\n(13)\",\n        \"A deterministic optimization needs to be performed to identify a\\nsubset I that contains the smallest volume density NI/VI. In the case of\\nunique samples, since \\u03b7((i\\u22c5)) = 1, the solution to the minimization problem\\nin (10) is a set of \\u03c1N(i) Voronoi cells with the largest volume. For the case\\nwith repeated samples, the optimization can be performed using\\nmethods appropriate for non-smooth optimization problems, such as\\nsub-gradient methods, bundle methods, gradient sampling methods, etc.\",\n        \"In this study, we propose an alternative approach to identify the\\noptimal subset without performing any non-smooth deterministic\\noptimization. A double-sort algorithm is proposed, which involves sorting\\nthe Voronoi cells in ascending order of the sample counts and then in\\ngroups of cells with the same sample count in descending order of cell\\nvolume. Finally, the top cells containing \\u03c1N(i)samples are selected as an\\napproximate optimal solution from the sorted list.\",\n        \"One may argue that the optimal subset can be obtained by first\\nsorting the Voronoi cells in ascending order of the cell density, defined\\nas \\u03b7(ki)/Vk(i), and then by selecting the top cells containing \\u03c1N(i) samples\\nfrom the sorted list. However, this argument is erroneous because the\\nobjective is to minimize \\u2211sS=1\\u03b7((is))/ \\u2211sS=1V((si)) and not \\u2211sS=1(\\u03b7((is)) /V((si))). The\\neffectiveness of the proposed double-sort algorithm is demonstrated in\\nSection 4 with the help of examples.\",\n        \"At the i + 1th iteration, \\u03c1N(i) samples distributed as \\u03c0(\\u03c6|I(i + 1)) are\\navailable from the previous iteration. Using these samples as seeds,\\nadditional (1 \\u03c1)N(i + 1) are simulated. The proposed method to\\nsimulate additional samples involves two steps: (a) randomly selecting a\\nVoronoi cell within the subset I(i + 1) based on the estimate \\u03c0(\\u03c6|I(i)) and\\n(b) applying the Metropolis-Hastings algorithm within the selected\\nVoronoi cell.\",\n        \"A Voronoi cell is selected according to the following weights in the\\nifrst step:\",\n        \"/\\nw(ki) = \\u2211\\u03b7(k\\u03b7i)(i)/VVk(i)(i)\\nk\",\n        \".\\nk k\\n(14)\",\n        \"An important issue for the effective implementation of the iSSO is the\\ncreation of the Voronoi cells at the current iteration bounded within the\\nVoronoi cell created at the previous iterations. Although it is possible to\\ncreate such bounded Voronoi cells, due to the geometrical complexities,\\nit is usually unfeasible for the higher dimensional problems (n\\u03c6>2). An\\nalternative approach is proposed in the present study for creating the\\nVoronoi cells at any iteration of the iSSO. The proposed approach\\ninvolves creating Voronoi cells using the samples generated at the current\\nand all previous iterations and then by considering Voronoi cells\\ncorresponding to the samples from the current iteration. This is shown in\\nFig. 3, where Fig. 3(a) shows the N samples at the first iteration and the\\ncorresponding Voronoi cells. Fig. 3(b) shows the \\u03c1N selected Voronoi\\ncells leading to the smallest volume density and the additional (1 \\u03c1)N\\nsamples being generated using these \\u03c1N samples as seeds. Fig. 3(c)\\nshows that the Voronoi cells are generated using all N + (1 \\u03c1)N\\nsamples that are generated in the two iterations. The Voronoi cells\\ncorresponding to the N samples for consideration at the second iteration\\nare also highlighted in Fig. 3(c). Fig. 3(d) shows a zoomed-in version of\\nFig. 3(c) where it can be observed that the area covered by the N Voronoi\\ncells considered in the second iteration is not the same as the area\\ncovered by the \\u03c1N Voronoi cells selected in the first iteration. On the\\ncontrary, the area covered by the Voronoi cells in the second iteration is\\nmore than the area covered by the Voronoi cells corresponding to the\\nseed samples from the first iteration. This is because a new sample\\nwithin the Voronoi cell between an existing sample and the existing\\nVoronoi cell edge results in the relocation of the Voronoi cell edge in a\",\n        \"To simulate a new sample within a selected Voronoi cell, the sample\\nthat generated the selected Voronoi cell or the last simulated sample in\\nthe selected Voronoi cell is used as the seed sample, and the\\nMetropolisHastings algorithm is implemented. A candidate sample \\u03c6c,\\u03b8c is\\nsimulated using the proposal q(\\u03c6c,\\u03b8c|\\u03c6,\\u03b8) and is accepted with the\\nprobability min(1, a0), where, a0 is given as:\\na0 =\\nh(\\u03c6c, \\u03b8c)p(\\u03c6c, \\u03b8c)q(\\u03c6, \\u03b8|\\u03c6c, \\u03b8c).\",\n        \"h(\\u03c6, \\u03b8)p(\\u03c6, \\u03b8)q(\\u03c6c, \\u03b8c|\\u03c6, \\u03b8)\",\n        \"In the present study, the proposed PDF is equal to the uniform PDF\\nfor design parameters and the initial PDF for uncertain variables, i.e., q\\n(\\u03c6, \\u03b8|\\u03c6c,\\u03b8c) = p(\\u03c6, \\u03b8). Therefore, on simplifying (15), a0 is given as:\\na0 =\\nh(\\u03c6c, \\u03b8c).\",\n        \"h(\\u03c6, \\u03b8)\",\n        \"\\n          A new stopping criterion is proposed in this study. The convergence\\nof the expected value of the performance measure h(\\u03c6, \\u03b8) with respect to\\nthe PDF for \\u03c6 and \\u03b8 in consecutive iterations is used as the stopping\\ncriterion. Mathematically the proposed stopping criterion is represented\\nby:\\n\\u20d2\\n\\u20d2E\\u03c6,\\u03b8h(\\u03c6, \\u03b8)i E\\u03c6,\\u03b8h(\\u03c6, \\u03b8)i 1\\u20d2\\u20d2 \\u2264 \\u03b5 (17)\\nwhere, \\u03b5 is a user-specified tolerance limit. Other stopping criteria, as\\nindicated in \\n          \",\n        \"In the iSSO framework, a deterministic optimization problems can\\nalso be handled with the vector of uncertain variables \\u03b8 set equal to a\\nnull vector (n\\u03b8 = 0). Since the determination of the subset at each iSSO\\niteration is solely dependent on the samples distributed as \\u03c0(\\u03c6), no\\nmodification to the iSSO algorithm is required to solve a deterministic\\noptimization problem, and the entire formulation remains valid.\"\n      ]\n    },\n    {\n      \"title\": \"3.1. Partitioning of design space\",\n      \"paragraphs\": [\n        \"In the proposed approach, at the i + 1th iteration, say N(i) is the\\nnumber of samples distributed as \\u03c0(\\u03c6|I(i)) belonging to the design space\\nI(i). Let nv = N(i) /(1 + \\u03b3), \\u03b3 \\u2265 0 be the number of unique samples. If\\nsampling techniques such as accept rejection, importance sampling, etc.,\\nare used, then \\u03b3 = 0, and each sample in the design space will be unique.\\nHowever, if MCMC sampling techniques are used, the resulting samples\\nwill be correlated, that is \\u03b3 > 0, and we will have repeated samples.\\nAssume that the design space I(i) is divided into v(ki), k = 1\\u22c5\\u22c5\\u22c5nv, Voronoi\\ncells using nv unique samples, and say the Voronoi cell v(ki) contains \\u03b7(ki)\\nrepeated samples, then, an estimate of \\u03c0(\\u03c6|I(i)) is provided by:\\n\\u03b7(i)\\n\\u03c0 \\u03c6|I(i))= N(i)kV(i) \\u2265 0, \\u2200 \\u03c6 \\u2208 v(ki),\\nk\\n\\u222b\\nwhere, Vk(i) is the volume of the kth Voronoi cell. Obviously, I(i) \\u03c0(\\u03c6|I(i))\\nd\\u03c6 = 1.\",\n        \"Similar to the original SSO, the sensitivity of the objective function J\\n(\\u03c6|I(i)) to \\u03c6 is determined by evaluating the average value of J(\\u03c6|I(i))\\nover any subspace I of the design space I(i). Subset I is any subset of\\nnvVoronoi cells (these cells may be disjointed). Since the design space is\\npartitioned into nv subspaces or Voronoi cells, the number of admissible\\nsubsets (proper subsets) is given by 2n\\u03bd 1. Based on the estimate\\n\\u03c0(\\u03c6|I(i)) provided in (11), an estimate of H(I) is provided as:\\nH(I) = NV((ii))  V\\u03b7((((1i1i)))) ++ V\\u03b7((((i22i)))) ++ \\u22ef\\u22ef ++ \\u03b7V(((iS()Si))) .\\n(11)\\n(12)\\n(13)\"\n      ]\n    },\n    {\n      \"title\": \"3.2. Identification of an optimal subset\",\n      \"paragraphs\": [\n        \"A deterministic optimization needs to be performed to identify a\\nsubset I that contains the smallest volume density NI/VI. In the case of\\nunique samples, since \\u03b7((i\\u22c5)) = 1, the solution to the minimization problem\\nin (10) is a set of \\u03c1N(i) Voronoi cells with the largest volume. For the case\\nwith repeated samples, the optimization can be performed using\\nmethods appropriate for non-smooth optimization problems, such as\\nsub-gradient methods, bundle methods, gradient sampling methods, etc.\",\n        \"In this study, we propose an alternative approach to identify the\\noptimal subset without performing any non-smooth deterministic\\noptimization. A double-sort algorithm is proposed, which involves sorting\\nthe Voronoi cells in ascending order of the sample counts and then in\\ngroups of cells with the same sample count in descending order of cell\\nvolume. Finally, the top cells containing \\u03c1N(i)samples are selected as an\\napproximate optimal solution from the sorted list.\",\n        \"One may argue that the optimal subset can be obtained by first\\nsorting the Voronoi cells in ascending order of the cell density, defined\\nas \\u03b7(ki)/Vk(i), and then by selecting the top cells containing \\u03c1N(i) samples\\nfrom the sorted list. However, this argument is erroneous because the\\nobjective is to minimize \\u2211sS=1\\u03b7((is))/ \\u2211sS=1V((si)) and not \\u2211sS=1(\\u03b7((is)) /V((si))). The\\neffectiveness of the proposed double-sort algorithm is demonstrated in\\nSection 4 with the help of examples.\"\n      ]\n    },\n    {\n      \"title\": \"3.3. Simulation of conditional samples\",\n      \"paragraphs\": [\n        \"At the i + 1th iteration, \\u03c1N(i) samples distributed as \\u03c0(\\u03c6|I(i + 1)) are\\navailable from the previous iteration. Using these samples as seeds,\\nadditional (1 \\u03c1)N(i + 1) are simulated. The proposed method to\\nsimulate additional samples involves two steps: (a) randomly selecting a\\nVoronoi cell within the subset I(i + 1) based on the estimate \\u03c0(\\u03c6|I(i)) and\\n(b) applying the Metropolis-Hastings algorithm within the selected\\nVoronoi cell.\",\n        \"A Voronoi cell is selected according to the following weights in the\\nifrst step:\",\n        \"/\\nw(ki) = \\u2211\\u03b7(k\\u03b7i)(i)/VVk(i)(i)\\nk\",\n        \".\\nk k\\n(14)\"\n      ]\n    },\n    {\n      \"title\": \"3.5. Implementation issues\",\n      \"paragraphs\": [\n        \"An important issue for the effective implementation of the iSSO is the\\ncreation of the Voronoi cells at the current iteration bounded within the\\nVoronoi cell created at the previous iterations. Although it is possible to\\ncreate such bounded Voronoi cells, due to the geometrical complexities,\\nit is usually unfeasible for the higher dimensional problems (n\\u03c6>2). An\\nalternative approach is proposed in the present study for creating the\\nVoronoi cells at any iteration of the iSSO. The proposed approach\\ninvolves creating Voronoi cells using the samples generated at the current\\nand all previous iterations and then by considering Voronoi cells\\ncorresponding to the samples from the current iteration. This is shown in\\nFig. 3, where Fig. 3(a) shows the N samples at the first iteration and the\\ncorresponding Voronoi cells. Fig. 3(b) shows the \\u03c1N selected Voronoi\\ncells leading to the smallest volume density and the additional (1 \\u03c1)N\\nsamples being generated using these \\u03c1N samples as seeds. Fig. 3(c)\\nshows that the Voronoi cells are generated using all N + (1 \\u03c1)N\\nsamples that are generated in the two iterations. The Voronoi cells\\ncorresponding to the N samples for consideration at the second iteration\\nare also highlighted in Fig. 3(c). Fig. 3(d) shows a zoomed-in version of\\nFig. 3(c) where it can be observed that the area covered by the N Voronoi\\ncells considered in the second iteration is not the same as the area\\ncovered by the \\u03c1N Voronoi cells selected in the first iteration. On the\\ncontrary, the area covered by the Voronoi cells in the second iteration is\\nmore than the area covered by the Voronoi cells corresponding to the\\nseed samples from the first iteration. This is because a new sample\\nwithin the Voronoi cell between an existing sample and the existing\\nVoronoi cell edge results in the relocation of the Voronoi cell edge in a\",\n        \"To simulate a new sample within a selected Voronoi cell, the sample\\nthat generated the selected Voronoi cell or the last simulated sample in\\nthe selected Voronoi cell is used as the seed sample, and the\\nMetropolisHastings algorithm is implemented. A candidate sample \\u03c6c,\\u03b8c is\\nsimulated using the proposal q(\\u03c6c,\\u03b8c|\\u03c6,\\u03b8) and is accepted with the\\nprobability min(1, a0), where, a0 is given as:\\na0 =\\nh(\\u03c6c, \\u03b8c)p(\\u03c6c, \\u03b8c)q(\\u03c6, \\u03b8|\\u03c6c, \\u03b8c).\",\n        \"h(\\u03c6, \\u03b8)p(\\u03c6, \\u03b8)q(\\u03c6c, \\u03b8c|\\u03c6, \\u03b8)\",\n        \"In the present study, the proposed PDF is equal to the uniform PDF\\nfor design parameters and the initial PDF for uncertain variables, i.e., q\\n(\\u03c6, \\u03b8|\\u03c6c,\\u03b8c) = p(\\u03c6, \\u03b8). Therefore, on simplifying (15), a0 is given as:\\na0 =\\nh(\\u03c6c, \\u03b8c).\",\n        \"h(\\u03c6, \\u03b8)\"\n      ]\n    },\n    {\n      \"title\": \"3.4. Stopping criteria\",\n      \"paragraphs\": [\n        \"\\n          A new stopping criterion is proposed in this study. The convergence\\nof the expected value of the performance measure h(\\u03c6, \\u03b8) with respect to\\nthe PDF for \\u03c6 and \\u03b8 in consecutive iterations is used as the stopping\\ncriterion. Mathematically the proposed stopping criterion is represented\\nby:\\n\\u20d2\\n\\u20d2E\\u03c6,\\u03b8h(\\u03c6, \\u03b8)i E\\u03c6,\\u03b8h(\\u03c6, \\u03b8)i 1\\u20d2\\u20d2 \\u2264 \\u03b5 (17)\\nwhere, \\u03b5 is a user-specified tolerance limit. Other stopping criteria, as\\nindicated in \\n          \"\n      ]\n    },\n    {\n      \"title\": \"3.6. Special case: deterministic optimization\",\n      \"paragraphs\": [\n        \"In the iSSO framework, a deterministic optimization problems can\\nalso be handled with the vector of uncertain variables \\u03b8 set equal to a\\nnull vector (n\\u03b8 = 0). Since the determination of the subset at each iSSO\\niteration is solely dependent on the samples distributed as \\u03c0(\\u03c6), no\\nmodification to the iSSO algorithm is required to solve a deterministic\\noptimization problem, and the entire formulation remains valid.\"\n      ]\n    },\n    {\n      \"title\": \"4. Illustrative examples\",\n      \"paragraphs\": [\n        \"In this section, typical optimization problems are considered to\\ndemonstrate the effectiveness and efficiency of the proposed approach.\",\n        \"First, deterministic optimization problems are considered. These\\nproblems include several local and global minima. Next, stochastic\\noptimization problems are illustrated. The second example presents an RDO\\nproblem of the TMD. In this example, the variance minimization of the\\nprotected structure\\u2019s displacement (TMD attached to the structure) is\\nperformed. In the third example, the mean minimization of 120 bars\\ntruss problems is explored to demonstrate the applicability of the\\nproposed approach to a high-dimensional stochastic design problem.\",\n        \"Finally, the fourth example investigates the reliability-based\\noptimization of a base isolation system for a 10-story building.\",\n        \"In this study, after implementing iSSO, the optimal design solution is\\nidentified as follows. Let \\u03b8j, j = 1\\u22c5\\u22c5\\u22c5n be a set of independent, identically\\ndistributed realizations of \\u03b8, and let h(\\u03c6, \\u03b8j) be the structural\\nperformance function realization for \\u03b8j. The expected structural performance\\nfunction is approximated by the average of the realizations as:\\nE\\u03b8h(\\u03c6, \\u03b8) \\u2248\\n1 \\u2211n\\nn j=1\",\n        \")\\nh \\u03c6, \\u03b8j .\",\n        \"E\\u03b8h(\\u03c6, \\u03b8) is evaluated for all unique \\u03c6 samples obtained at the last\\niteration of the iSSO, and the \\u03c6 sample resulting in the smallest value of\\nE\\u03b8h(\\u03c6, \\u03b8) is taken as the optimal solution. Alternatively, as the\\nrighthand side of (18) is deterministic, any deterministic optimization\\nmethod can also be used to solve the optimization problem with the\\napproximate expectation.\",\n        \"In the following examples, both iSSO and SSO are implemented with\\nN = 1000n\\u03c6, \\u03c1 = 0.20 and the stopping criteria as stated in (17). Here, a\\nvalue of \\u03b5 = 10 3 is adopted.\",\n        \"(18)\",\n        \"\\n        In this section, three two-dimensional benchmark deterministic\\noptimization problems are considered. Results are also compared with\\nthe SSO. The test functions are:\\n\\u20d2\\u20d2\\u20d2 (\\u20d2\\u20d2\\u20d2\\nminh(\\u03c6) = \\u20d2\\u20d2sin(\\u03c61)cos(\\u03c62)exp \\u20d2\\u20d21\\ns.t.\\u03c6 = \\n        \",\n        \"\\u20d2\\n\\u03c0 \\u20d2\\u20d2 \\u20d2\\u20d2,\\n(21))\\na) Griewank function:\\nminh(\\u03c6) =\\n\\u221a\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u20d2\\u0305)\\u0305\\u20d2\\u20d2\\u0305\\u0305\\u0305\\u0305)\\u03050\\u0305.1\\u0305\\n\\u03c621\\u03c0+ \\u03c622\\u20d2\\u20d2\\u20d2\\u20d2 \\u20d2\\u20d2\\u20d2 + 1 ,\\n(19)\\n(20)\",\n        \"The results for the Griewank function are presented in Fig. 4. Fig. 4(a,\\nb) shows that the function has multiple closely spaced local minima with\\na single global minimum. Fig. 4(c, d) shows the SSO optimization using\\nhyper-rectangle and hyper-ellipse as shapes of admissible subsets. It is\\nseen that these shapes fail to capture the region containing the optimal\\ndesign due to the presence of multiple local minima. Next, the iSSO is\\nimplemented, where the Voronoi cells selected at the first and last\\niteration are shown in Fig. 4(e, f). It is observed that at the first iteration,\\nthe selected Voronoi cells effectively capture both the local and global\\nminima and in the subsequent iterations, the selected cells are more\\nconcentrated near the global minimum. The region selected at the last\\niteration captures the optimal global solution.\",\n        \"The Cross-in-Tray function has a relatively complex design space\\ncompared to the Griewank function. Fig. 5(a, b) shows multiple local\\nand global minima. Minimization by using SSO is demonstrated in Fig. 5\\n(c, d). It is found that both the hyper-rectangle and hyper-ellipse are\\ntrapped around any one of the global minima. At the same time, the iSSO\\nis able to capture the regions that include all of the global minima, as\\nseen in Fig. 5(e, f).\",\n        \"The Holder Table function has multiple local and global minima; the\\nglobal minima are placed at the boundary of the design space, as shown\\nin Fig. 6(a, b). Once again, it is seen that both the hyper-rectangle and\\nhyper-ellipse are trapped around any of one of the global minima, and\\non the other hand, the iSSO is able to capture the regions that include all\\nof the global minima, as seen in Fig. 6(e, f).\",\n        \"The results from the three examples demonstrate that the proposed\\niSSO is able to capture the regions containing the optimal solution\\neffectively.\",\n        \"Next, the statistics of the results of 50 independent runs, both for SSO\\nand iSSO are presented in Table 1. It also includes the results obtained by\\nusing state-of-the-art approaches, such as the Genetic algorithm, particle\\nswarm optimization, and the gradient based optimization approach\\n(interior-point algorithm). The proposed iSSO outperforms all other\\napproaches as more successes in determining the optimal solution are\\nobserved in all three optimization problems. It is also seen that both SSO\\nand iSSO result in a similar value of volume reduction for the same\\nstopping criterion; however, with SSO, the number of iterations required\\nto achieve this volume reduction are relatively higher. The proposed\\napproach outperformed the state-of-the-art approaches, as indicated by\\nthe number of successes. These examples demonstrate that the main\\nadvantage of implementing Voronoi tessellation is an effective\\nexploration of the design space.\",\n        \"Next, the performance of the proposed \\\"double sort algorithm\\\" for\\nselecting the optimal subset is studied by using the above-mentioned\\nthree functions. Fig. 7 shows the value of H(I(1)) for the 50\\nindependent simulation runs, which is estimated by implementing the proposed\\ndouble sort algorithm and by using the Genetic algorithm. It can be\\nnoted that for each run, the H(I(1)) values obtained using the proposed\\ndouble sort algorithm and Genetic algorithm are well matched, thereby\\nconfirming the adequacy of the proposed double sort algorithm.\",\n        \"At any iteration of iSSO, new samples are simulated using the seed\\nsamples. In the proposed approach, the volume of the Voronoi cells\\ncorresponding to the seed and new samples is greater than the volume of\\nthe Voronoi cells corresponding only to the seed samples. Fig. 8 shows\\nthis change in volume V(seeds+Vn(eswee)ds)V(seeds) due to the creation of Voronoi cells\\nat any generation of iSSO using the procedure mentioned in Section 3.4.\",\n        \"The increase is observed to be small which further reduces with an\\nincrease in the iteration number. It is also observed that the increase in\\nvolume decreases with an increase in sample size at each iteration and\\nincreases with an increase in the dimension of the problem.\\n4.2. Robust design optimization of the tuned mass damper\",\n        \"\\n        This example considers a stochastic design problem involving a\\nTuned Mass Damper (TMD) attached to a Single Degree of Freedom\\n(SDOF) system. The problem is taken from \\n        \",\n        \"In this problem, the system is excited by a white noise signal with a\\nmean zero and unit variance. The performance measure is the variance\\nof the displacement of the system \\u03c3x2s . The mass mS, stiffness kS, and\\nGA = genetic algorithm, PSO = particle swarm optimization, GBA = gradient-based optimization approach, NF = no. of. failure, NS = no. of. success, BV = best value,\\nWV = worst value, AV = average value, c.o.v = coefficient of variation, FE = no. of. function evaluations, Gen = generations, VR = volume reduction percentage, * =\\nefficiently applicable only for deterministic problems.\\ndamping cS of the system are taken as uncertain parameters, following\\nindependent Gaussian distribution. The mean value of these variables is\\ntaken to be 105 kg, 107 N/m, and 4 \\u00d7 104 Ns/m respectively. To account\\nfor uncertainty, the c.o.v value for each variable taken is 0.05. The\\nfrequency ratio \\u03b2 = \\u03c9T /\\u03c9S and damping \\u03beT of the TMD are considered\\ndesign parameters. The TMD has a mass ratio, mT/ms, of 0.10. The\\nparameters mT,\\u03c9T,and\\u03c9S a\\u221are\\u0305, \\u0305in\\u0305\\u0305o\\u0305r\\u0305d\\u0305e\\u0305r\\u0305,\\u0305t\\u0305h\\u0305e\\u0305m\\u0305ass of the TMD, the natural\\nfrequency of the TMD ( kT/mT), and the natural frequency of the\",\n        \"\\u221a\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\u0305\\nstructure ( ks/ms). The optimization problem is written as:\\n( )2\",\n        \"\\n        (22)\\niSSO are shown. SAA is applied with a sample size of 103, as mentioned\\nin \\n        \",\n        \"\\n          The third example involves minimizing the mean of the compliance\\nof a 120-bar linear elastic truss structure shown in Fig. 10 under the\\nweight constraint W \\u2264 15, 000kg. Because of structural symmetry,\\ndesign parameters corresponding to the cross-sectional areas of elements\\nare divided into seven groups, each with a minimum area of 10 4 m2.\\nThe Young\\u2019s modulus for the bar groups are assumed as uncorrelated\\nnormal random variables with mean values equal to 210 GPa and the c.\\no.v equal to 0.10 respectively. The density of the material is 7971.89 kg/\\nm3. The dome is subjected to concentrated vertical loads acting\\ndownward at the top node, normally distributed with a mean equal to 60 kN\\nand c.o.v equal to 0.20. In addition, the mass of bars is concentrated at\\nthe nodes. The problem is taken from \\n          \",\n        \"Table 3 presents the best of 10 independent run results obtained with\\nSSO and iSSO. Once again, the SSO and iSSO solutions agree well,\\nthereby demonstrating the effectiveness of the proposed approach. At\\nthe same time, the number of function evaluations is substantially less in\\nthe case of iSSO, indicating the efficiency of the proposed approach.\",\n        \"\\n          This example, adapted from \\n          \",\n        \"A6\\n(cm2)\\n55.6\\n49.5\\n56.5\",\n        \"A7\\n(cm2)\\nwhere, IF(\\u03c6,\\u03b8) is the function that indicates failure, and it equals 1 when\\nthe system fails, i.e., when unacceptable performance occurs. Notably,\\nin this problem h(\\u03c6, \\u03b8) = IF(\\u03c6,\\u03b8).\",\n        \"The 10-story building is considered as a shear structure with\\nuncertain inter-story stiffness and damping. Each story has a total mass of\\n207 ton. The inter-story stiffness ki of all stories are parameterized by ki\\n= \\u0302ki\\u03b8i, i = 1, \\u2026, 10 where the most probable values of the inter-story\\nstiffness are \\u0302ki = 687.1, 613.1, 540.1, 481.1, 421.7, 353.7, 286.6,\\n225.6, 184.5, 104.5 MN/m. The entity \\u03b8i is a set of non-dimensional\\nuncertain variables that are considered to be correlated Gaussian\\nvariables with a unit mean value \\u0302\\u03b8i= 1, \\u2200i and a covariance matrix defined\\nas:\\nE(\\u03b8i \\u0302\\u03b8i) \\u03b8j \\u0302\\u03b8j)= (0.2)2exp (j i)2 /22. (25)\",\n        \"The damping ratios are considered independent Gaussian variables\\nwith mean values of 0.025 and c.o.v of 0.10 for all modes. The\\nKanaiTajimi model is used to simulate the ground excitation modelled as a\\nifltered white noise process, with the power spectral density function\\ngiven as:\\nS(\\u03c9) = S0(\\n\\u03c94g +)4\\u03b6g2\\u03c92g\\u03c92\",\n        \"2\\n\\u03c92\\ng\\n\\u03c92\\n+ 4\\u03b6g2\\u03c92g\\u03c92\\n,\\n(26)\\n(27)\\n(28)\\nS0 = \\u03c32\\u03c9\",\n        \"(2\\u03b6g\\n\\u03c0\\u03c9g 4\\u03b6g2 + 1\",\n        \"/\\n)m2\",\n        \"\\n          s3,\\nwhere, \\u03c9g, \\u03b6gand \\u03c3\\u03c9 are the resonant frequency, damping, and RMS of the\\nacceleration input of the filter, respectively. These are also considered\\nuncertain variables with mean values of 2\\u03c0rad/s, 0.5, 0.2g and a c.o.v\\nequal to 0.20. The non-stationarity of the excitation is modeled by\\nmultiplying the filter output with the envelope function as:\\ne(t) = \\u03bb3t\\u03bb1 exp( \\u03bb2t),\\nwith parameters \\u03bb1 = 1.25, \\u03bb2 = 0.2 and \\u03bb3 = 0.353 chosen to simulate\\nstrong earthquake excitation for a duration of 40 s with a sampling time\\nof 0.02 s. The base-isolation system considered is a lead\\u2013rubber bilinear\\nisolator with an additional viscous damper. The base has a 247-ton mass.\\nThe design parameters \\u03c6 for the base isolation structure system are the\\nstiffness before yielding Kprand after yielding Kp, the yield force is Fy,\\nand the damping coefficient cd. The reader may refer to \\n          \",\n        \"\\n          Failure is indicated when any of the normalized base displacements\\nor inter-story drifts exceeds unity. The normalization constants are 0.5 m\\nand 0.033 m respectively. The design interval for each variable is\\nspecified as Kpr = \\n          \",\n        \"Table 4 shows the optimization results for the best 10 independent\\nsimulation runs. The comparison of the results obtained using SSO, SAA\\n(with a sample size of 103), and iSSO shows that the optimal design\\nobtained using the proposed approach iSSO is in good agreement. The\\nfailure probability of the structure is reduced from 0.95 (without the\\nbase isolation system) to 0.0326 after installing the optimally designed\\nbase isolation system.\"\n      ]\n    },\n    {\n      \"title\": \"4.3. 120-bars truss structure\",\n      \"paragraphs\": [\n        \"\\n          The third example involves minimizing the mean of the compliance\\nof a 120-bar linear elastic truss structure shown in Fig. 10 under the\\nweight constraint W \\u2264 15, 000kg. Because of structural symmetry,\\ndesign parameters corresponding to the cross-sectional areas of elements\\nare divided into seven groups, each with a minimum area of 10 4 m2.\\nThe Young\\u2019s modulus for the bar groups are assumed as uncorrelated\\nnormal random variables with mean values equal to 210 GPa and the c.\\no.v equal to 0.10 respectively. The density of the material is 7971.89 kg/\\nm3. The dome is subjected to concentrated vertical loads acting\\ndownward at the top node, normally distributed with a mean equal to 60 kN\\nand c.o.v equal to 0.20. In addition, the mass of bars is concentrated at\\nthe nodes. The problem is taken from \\n          \",\n        \"Table 3 presents the best of 10 independent run results obtained with\\nSSO and iSSO. Once again, the SSO and iSSO solutions agree well,\\nthereby demonstrating the effectiveness of the proposed approach. At\\nthe same time, the number of function evaluations is substantially less in\\nthe case of iSSO, indicating the efficiency of the proposed approach.\"\n      ]\n    },\n    {\n      \"title\": \"4.4. Reliability-based design of a base isolated structure\",\n      \"paragraphs\": [\n        \"\\n          This example, adapted from \\n          \",\n        \"A6\\n(cm2)\\n55.6\\n49.5\\n56.5\",\n        \"A7\\n(cm2)\\nwhere, IF(\\u03c6,\\u03b8) is the function that indicates failure, and it equals 1 when\\nthe system fails, i.e., when unacceptable performance occurs. Notably,\\nin this problem h(\\u03c6, \\u03b8) = IF(\\u03c6,\\u03b8).\",\n        \"The 10-story building is considered as a shear structure with\\nuncertain inter-story stiffness and damping. Each story has a total mass of\\n207 ton. The inter-story stiffness ki of all stories are parameterized by ki\\n= \\u0302ki\\u03b8i, i = 1, \\u2026, 10 where the most probable values of the inter-story\\nstiffness are \\u0302ki = 687.1, 613.1, 540.1, 481.1, 421.7, 353.7, 286.6,\\n225.6, 184.5, 104.5 MN/m. The entity \\u03b8i is a set of non-dimensional\\nuncertain variables that are considered to be correlated Gaussian\\nvariables with a unit mean value \\u0302\\u03b8i= 1, \\u2200i and a covariance matrix defined\\nas:\\nE(\\u03b8i \\u0302\\u03b8i) \\u03b8j \\u0302\\u03b8j)= (0.2)2exp (j i)2 /22. (25)\",\n        \"The damping ratios are considered independent Gaussian variables\\nwith mean values of 0.025 and c.o.v of 0.10 for all modes. The\\nKanaiTajimi model is used to simulate the ground excitation modelled as a\\nifltered white noise process, with the power spectral density function\\ngiven as:\\nS(\\u03c9) = S0(\\n\\u03c94g +)4\\u03b6g2\\u03c92g\\u03c92\",\n        \"2\\n\\u03c92\\ng\\n\\u03c92\\n+ 4\\u03b6g2\\u03c92g\\u03c92\\n,\\n(26)\\n(27)\\n(28)\\nS0 = \\u03c32\\u03c9\",\n        \"(2\\u03b6g\\n\\u03c0\\u03c9g 4\\u03b6g2 + 1\",\n        \"/\\n)m2\",\n        \"\\n          s3,\\nwhere, \\u03c9g, \\u03b6gand \\u03c3\\u03c9 are the resonant frequency, damping, and RMS of the\\nacceleration input of the filter, respectively. These are also considered\\nuncertain variables with mean values of 2\\u03c0rad/s, 0.5, 0.2g and a c.o.v\\nequal to 0.20. The non-stationarity of the excitation is modeled by\\nmultiplying the filter output with the envelope function as:\\ne(t) = \\u03bb3t\\u03bb1 exp( \\u03bb2t),\\nwith parameters \\u03bb1 = 1.25, \\u03bb2 = 0.2 and \\u03bb3 = 0.353 chosen to simulate\\nstrong earthquake excitation for a duration of 40 s with a sampling time\\nof 0.02 s. The base-isolation system considered is a lead\\u2013rubber bilinear\\nisolator with an additional viscous damper. The base has a 247-ton mass.\\nThe design parameters \\u03c6 for the base isolation structure system are the\\nstiffness before yielding Kprand after yielding Kp, the yield force is Fy,\\nand the damping coefficient cd. The reader may refer to \\n          \",\n        \"\\n          Failure is indicated when any of the normalized base displacements\\nor inter-story drifts exceeds unity. The normalization constants are 0.5 m\\nand 0.033 m respectively. The design interval for each variable is\\nspecified as Kpr = \\n          \",\n        \"Table 4 shows the optimization results for the best 10 independent\\nsimulation runs. The comparison of the results obtained using SSO, SAA\\n(with a sample size of 103), and iSSO shows that the optimal design\\nobtained using the proposed approach iSSO is in good agreement. The\\nfailure probability of the structure is reduced from 0.95 (without the\\nbase isolation system) to 0.0326 after installing the optimally designed\\nbase isolation system.\"\n      ]\n    },\n    {\n      \"title\": \"5. Conclusion\",\n      \"paragraphs\": [\n        \"This study attempts to provide an optimization approach called\\n\\\"iSSO\\\", which is an improved version of SSO, primarily for stochastic\\noptimization problems while it retains utility for deterministic\\noptimization problems as well. Two novel ideas are introduced in this study:\\nifrst, a better characterization of the design space is offered by\\npartitioning the design space into non-overlapping subregions using Voronoi\\ntessellation which improves the effectiveness and efficiency of the\\nproposed iSSO considerably in comparison to SSO. Second, a novel \\\"double\\nsort\\\" approach is proposed, eliminating the need for optimization to\\nidentify the subregions for the optimal design at each iSSO iteration.\\nSeveral mathematical and engineering design examples, including TMD,\\n120 bars truss structure, and base-isolated structure, are included in this\\nstudy to demonstrate the efficacy of the proposed iSSO. The results show\\nthat the proposed iSSO effectively identifies the reduced design space for\\ncomplex design problems with multiple global and local minima. This is\\nattributable to the Voronoi tessellation, which eliminates the\\nrequirement of the presumed admissible design space form to resemble the\\ncontour of the original design. Voronoi tessellation enabled better\\ndesign space exploration, allowing multiple global minima scattered\\nthroughout the design pace to be effectively identified. Due to the\\ndiscretization of the design space via Voronoi tessellation, computation\\ndemand is significantly reduced as the number of function evaluations\\nfor all examples is lower vis-a-vis the original SSO. Moreover, the novel\\nidea of the double sort approach achieves the requisite precision in\\nidentifying the subregions for optimal solutions and makes iSSO\\nimplementation simple and effective.\",\n        \"The applicability of the approach is dependent on the creation of the\"\n      ]\n    },\n    {\n      \"title\": \"Appendix-A: Voronoi Tessellation\",\n      \"paragraphs\": [\n        \"Voronoi cells. At present the methods available in the literation for\\ncreating the Voronoi tessellation are computationally demanding when\\nconsidering problems of very high dimension. Future work will focus on\\ndeveloping a method for creating the Voronoi tessellation in higher\\ndimensions, particularly those greater than ten.\"\n      ]\n    },\n    {\n      \"title\": \"CRediT authorship contribution statement\",\n      \"paragraphs\": [\n        \"Mohd Aman Khalid: Investigation, Methodology, Formal analysis,\\nSoftware, Visualization, Writing \\u2013 original draft. Sahil Bansal:\\nConceptualization, Methodology, Supervision.\"\n      ]\n    },\n    {\n      \"title\": \"Declaration of Competing Interest\",\n      \"paragraphs\": [\n        \"The authors declare that they have no known competing financial\\ninterests or personal relationships that could have appeared to influence\\nthe work reported in this paper.\"\n      ]\n    },\n    {\n      \"title\": \"Data availability\",\n      \"paragraphs\": [\n        \"No data was used for the research described in the article.\",\n        \"Voronoi tessellation is a mathematical concept named after the Russian mathematician Georgy Voronoi. It is also known as the Voronoi diagram or\\nDirichlet tessellation. A Voronoi tessellation of a set of points P in a plane is a partition of the plane into a set of non-overlapping convex polygons, with\\neach polygon including precisely one point of P and each point in a polygon being closer to its associated point in P than to any other point in P. Each\\npolygon is referred to as a Voronoi cell or a Dirichlet region. The boundary of each cell is constituted of points that are equidistant to two or more\\npoints in P. Fig. 12 shows the Voronoi diagram in a two-dimensional design space.\",\n        \"\\n        There are several efficient algorithms for creating Voronoi diagrams. One such basic algorithm is to start with a set of points and then compute the\\nVoronoi cells by dividing the space into regions based on the distance to the nearest point. The Bowyer-Watson algorithm \\n        \",\n        \"\\n        It can be summarized that Voronoi tessellation is a powerful mathematical concept that aids in dividing space into regions based on the distance to\\na set of points. Voronoi tessellation finds widespread applications in areas such as image processing \\n        \"\n      ]\n    }\n  ]\n}",
    "{\n  \"DocumentTitle\": \"SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes\",\n  \"Auteurs\": \"Paul Saves, R\\u00e9mi Lafage, Nathalie Bartoli, Youssef Diouane, Jasper Bussemaker, Thierry Lefebvre, John T. Hwang, Joseph Morlier, Joaquim R.R.A. Martins g, \",\n  \"Institutions\": \"German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, ICA, Universite\\u0301 de Toulouse, ISAE-SUPAERO, Universite\\u0301 de Toulouse, ONERA/DTIS, Universite\\u0301 de Toulouse, Polytechnique Montre\\u0301al, University of California San Diego, Department of Mechanical and Aerospace Engineering, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"-\",\n      \"paragraphs\": [\n        \"A R T I C L E\",\n        \"I N F O\\nDataset link: https://colab.research.google.com\\n/github/SMTorg/smt/blob/master/tutorial/No\\ntebookRunTestCases_Paper_SMT_v2.ipynb\"\n      ]\n    },\n    {\n      \"title\": \"1. Motivation and significance\",\n      \"paragraphs\": [\n        \"With the increasing complexity and accuracy of numerical models, it\\nhas become more challenging to run complex simulations and computer\\ncodes 1,2. As a consequence, surrogate models have been recognized\\nas a key tool for engineering tasks such as design space exploration,\\nuncertainty quantification, and optimization 3. In practice, surrogate\\nmodels are used to reduce the computational effort of these tasks by\\nreplacing expensive numerical simulations with closed-form\\napproximations 4, Ch. 10. To build such a model, we start by evaluating\\nthe original expensive simulation at a set of points through a Design\\nof Experiments (DoE). Then, the corresponding evaluations are used to\\nbuild the surrogate model according to the chosen approximation, such\\nas Kriging, quadratic interpolation, or least squares regression.\",\n        \"The Surrogate Modeling Toolbox (SMT) is an open-source\\nframework that provides functions to efficiently build surrogate models 5.\",\n        \"\\n        A B S T R A C T\\nThe Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate\\nmodeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major\\nnew release of SMT that introduces significant upgrades and new features to the toolbox. This release adds\\nthe capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables\\nare becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT\\nby extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives\\nfor Kriging. This release also includes new functions to handle noisy and use multi-fidelity data. To the best of\\nour knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical\\nand mixed inputs. This open-source software is distributed under the New BSD license.2\\nKriging models (also known as Gaussian processes) that take advantage\\nof derivative information are one of SMT\\u2019s key features 6. Numerical\\nexperiments have shown that SMT achieved lower prediction error\\nand computational cost than Scikit-learn 7 and UQLab \\n        \",\n        \"\\n        In systems engineering, architecture-level choices significantly\\ninfluence the final system performance, and therefore, it is desirable to\\nconsider such choices in the early design phases \\n        \",\n        \"Reference\\nLicense\\nLanguage\\nMixed var.\",\n        \"\\n        GD kernel\\nCR kernel\\nHH kernel\\nEHH kernel\\nHierarchical var.\\n\\n        \",\n        \"\\n        When architectural choices lead to different sets of design variables,\\nwe have hierarchical variables \\n        \",\n        \"\\n        Handling hierarchical and mixed variables requires specialized\\nsurrogate modeling techniques \\n        \",\n        \"\\n        There are two new major capabilities in SMT 2.0: the ability\\nto build surrogate models involving mixed variables and the support\\nfor hierarchical variables within Kriging models. To handle mixed\\nvariables in Kriging models, existing libraries such as BoTorch \\n        \",\n        \"\\n        SMT 2.0 introduces other enhancements, such as additional\\nsampling procedures, new surrogate models, new Kriging kernels (and their\\nderivatives), Kriging variance derivatives, and an adaptive criterion for\\nhigh-dimensional problems. SMT 2.0 adds applications of Bayesian\\noptimization (BO) with hierarchical and mixed variables or noisy\\ncoKriging that have been successfully applied to aircraft design \\n        \",\n        \"The remainder of the paper is organized as follows. First, we\\nintroduce the organization and the main implemented features of the\\nrelease in Section 2. Then, we describe the mixed-variable Kriging\\nmodel with an example in Section 3. Similarly, we describe and provide\\nan example for a hierarchical-variable Kriging model in Section 4.\",\n        \"The Bayesian optimization models and applications are described in\\nSection 5. Finally, we describe the other relevant contributions in\\nSection 6 and conclude in Section 7.\",\n        \"From a software point of view, SMT 2.0 maintains and improves\\nthe modularity and generality of the original SMT version 5. In this\\nsection, we describe the software as follows. Section 2.1 describes the\\nlegacy of SMT 0.2. Then, Section 2.2 describes the organization of the\\nrepository. Finally, Section 2.3 shows the new capabilities implemented\\nin the SMT 2.0 update.\",\n        \"SMT 5 is an open-source collaborative work originally developed\\nby ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of\\nMichigan. Now, both Polytechnique Montr\\u00e9al and the University of\\nCalifornia San Diego are also contributors. SMT 2.0 updates and\\nextends the original SMT repository capabilities among which the original\\npublication 5 focuses on different types of derivatives for surrogate\\nmodels detailed hereafter.\\n3 http://smt.readthedocs.io/en/latest\\n4 https://github.com/SMTorg/smt\\n5 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\",\n        \"\\n            original main motivations for SMT was derivative support. In fact, none\\nof the existing packages for surrogate modeling such as Scikit-learn in\\nPython 7, SUMO in Matlab \\n            \",\n        \"\\n            nized along three main sub-modules that implement a set of sampling\\ntechniques (sampling_methods), benchmarking functions (problems),\\nand surrogate modeling techniques (surrogate_models). The toolbox\\ndocumentation6 is created using reStructuredText and Sphinx, a\\ndocumentation generation package for Python, with custom extensions.\\nCode snippets in the documentation pages are taken directly from\\nactual tests in the source code and are automatically updated. The\\noutput from these code snippets and tables of options are generated\\ndynamically by custom Sphinx extensions. This leads to high-quality\\ndocumentation with minimal effort. Along with user documentation,\\ndeveloper documentation is also provided to explain how to contribute\\nto SMT. This includes a list of API methods for the SurrogateModel,\\nSamplingMethod, and Problem classes, that must be implemented\\nto create a new surrogate modeling method, sampling technique, or\\nbenchmarking problem. When a developer submits a pull request, it is\\nmerged only after passing the automated tests and receiving approval\\nfrom at least one reviewer. The repository on GitHub7 is linked to\\ncontinuous integration tests (GitHub Actions) for Windows, Linux and\\nMacOS, to a coverage test on coveralls.io and to a dependency version\\ncheck for Python with DependaBot. Various parts of the source code\\nhave been accelerated using Numba \\n            \",\n        \"The main features of the open-source repository SMT 2.0 are\\ndescribed in Fig. 1. More precisely, Sampling Methods, Problems\\nand Surrogate models are kept from SMT 0.2 and two new\\nsections Models applications and Interactive notebooks\\nhave been added to the architecture of the code. These sections are\\n6 https://smt.readthedocs.org\\n7 https://github.com/SMTorg/smt\\nhighlighted in blue and detailed on Fig. 1. The new major features\\nimplemented in SMT 2.0 are highlighted in lavender whereas the\\nlegacy features that were already in present in the original publication\\nfor SMT 0.2 5 are in black.\\n2.3. New features within SMT 2.0\",\n        \"The main objective of this new release is to enable Kriging surrogate\\nmodels for use with both hierarchical and mixed variables. Moreover,\\nfor each of these five sub-modules described in Section 2.2, several\\nimprovements have been made between the original version and the\\nSMT 2.0 release.\",\n        \"\\n            Hierarchical and mixed design space. A new design space definition\\nclass DesignSpace has been added that implements hierarchical\\nand mixed functionalities. Design variables can either be\\ncontinuous (FloatVariable), ordered (OrdinalVariable) or categorical\\n(CategoricalVariable). The integer type (IntegerVariable)\\nrepresents a special case of the ordered variable, specified by bounds\\n(inclusive) rather than a list of possible values. The hierarchical\\nstructure of the design space can be defined using declare_decreed_var:\\nthis function declares that a variable is a decreed variable that is\\nactivated when the associated meta variable takes one of a set of\\nspecified values, see Section 4 for background. The DesignSpace\\nclass also implements mechanisms for sampling valid design vectors\\n(i.e. design vectors that adhere to the hierarchical structure of the\\ndesign space) using any of the below-mentioned samplers, for\\ncorrecting and imputing design vectors, and for requesting which design\\nvariables are acting in a given design vector. Correction ensures that\\nvariables have valid values (e.g. integers for discrete variables) \\n            \",\n        \"\\n            Sampling. SMT implements three methods for sampling. The first one\\nis a na\\u00efve approach, called Random that draws uniformly points along\\nevery dimension. The second sampling method is called Full\\nFactorial and draws a point for every cross combination of variables,\\nto have an \\u2018\\u2018exhaustive\\u2019\\u2019 design of experiments. The last one is the\\nLatin Hypercube Sampling (LHS) \\n            \",\n        \"Problems. SMT implements two new engineering problems: a mixed\\nvariant of a cantilever beam described in Section 3 and a hierarchical\\nneural network described in Section 4.\",\n        \"\\n            Surrogate models. In order to keep up with state-of-art, several\\nreleases done from the original version developed new options for the\\nalready existing surrogates. In particular, compared to the original\\npublication 5, SMT 2.0 adds gradient-enhanced neural networks \\n            \",\n        \"In the following, Section 3 details the Kriging based surrogate\\nmodels for mixed variables, and Section 4 presents our new Kriging\\nsurrogate for hierarchical variables. Section 5 details the EGO\\napplication and the other new relevant features aforementioned are described\\nsuccinctly in Section 6.\",\n        \"8 https://github.com/SMTorg/smt/tree/master/tutorial\\n9 https://colab.research.google.com/github/SMTorg/smt/\\n10 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\",\n        \"As mentioned in Section 1, design variables can be either of\\ncontinuous or discrete type, and a problem with both types is a mixed-variable\\nproblem. Discrete variables can be ordinal or categorical. A discrete\\nvariable is ordinal if there is an order relation within the set of possible\\nvalues. An example of an ordinal design variable is the number of\\nengines in an aircraft. A possible set of values in this case could be\\n2, 4, 8. A discrete variable is categorical if no order relation is known\\nbetween the possible choices the variable can take. One example of a\\ncategorical variable is the color of a surface. A possible example of a\\nset of choices could be blue, red, green. The possible choices are called\\nthe levels of the variable.\",\n        \"\\n            Several methods have been proposed to address the recent increase\\ninterest in mixed Kriging based models \\n            \",\n        \"The continuous and ordinal variables are both treated similarly\\nin SMT 2.0 with a continuous kernel, where the ordinal values are\\nconverted to continuous through relaxation. For categorical variables,\\nfour models (GD, CR, EHH and HH) can be used in SMT 2.0 if\\nspecified by the API. This is why we developed a unified mathematical\\nformulation that allows a unique implementation for any model.\",\n        \"Denote  the number of categorical variables. For a given  \\u2208\\n{1, \\u2026 , }, the th categorical variable is denoted  and its number\\nof levels is denoted . The hyperparameter matrix peculiar to this\\nvariable  is\",\n        \"\\u23a1 1,1 . \\u23a4\\n = \\u23a2\\u23a2\\u23a2 \\u22ee1,2 \\u22f12,2 \\u22f1 \\u23a5\\u23a5\\u23a5 ,\",\n        \"\\n            \\u23a3\\u23a21, \\u2026 \\u22121, , \\u23a6\\u23a5\\nand the categorical parameters are defined as  = {1, \\u2026 , }. For\\ntwo given inputs in the DoE, for example, the th and th points, let\\n and  be the associated categorical variables taking respectively\\nthe  and the  level on the categorical variable . The categorical\\n\\ncorrelation kernel is defined by\\n(, , ) =\\n\\n\\u220f ((), ) ((), )((), )((), )\\n=1        \\n(1)\\nwhere  is either a positive definite kernel or identity and (.) is a\\nsymmetric positive definite (SPD) function such that the matrix ()\\nis SPD if  is SPD. For an exponential kernel, Table 3 gives the\\nparameterizations of  and  that correspond to GD, CR, HH, and\\nEHH kernels. The complexity of these different kernels depends on\\nthe number of hyperparameters that characterizes them. As defined\\nby Saves et al. \\n            \",\n        \"\\n            Another Kriging based model that can use mixed variables is Kriging\\nwith partial least squares (KPLS) \\n            \",\n        \"\\n            A classic engineering problem commonly used for model validation\\nis the beam bending problem \\n            \",\n        \"\\n            To compare the mixed Kriging models of SMT 2.0, we draw a 98\\npoint LHS as training set and the validation set is a grid of 12 \\u00d7 30 \\u00d7\\n30 = 10800 points. For the four implemented methods, displacement\\nerror (computed with a root-mean-square error criterion), likelihood,\\nnumber of hyperparameters and computational time for every model\\nare shown in Table 4. For the continuous variables, we use the square\\nexponential kernel. More details are found in \\n            \",\n        \"\\n          To introduce the newly developed Kriging model for hierarchical\\nvariables implemented in SMT 2.0, we present the general\\nmathematical framework for hierarchical and mixed variables established\\nby Audet et al. \\n          \",\n        \"\\n            A problem structure is classified as hierarchical when the sets of\\nactive variables depend on architectural choices. This occurs frequently\\nin industrial design problems. In hierarchical problems, we can classify\\nvariables as neutral, meta (also known as dimensional) or decreed\\n(also known as conditionally active) as detailed in Audet et al. \\n            \",\n        \"However, the wing aspect ratio being neutral, it is not affected by this\\nhierarchy.\",\n        \"\\n            Problems involving hierarchical variables are generally dependant\\non discrete architectures and as such involve mixed variables. Hence,\\nin addition to their role (neutral, meta or decreed), each variable also\\nhas a variable type amongst categorical, ordinal or continuous. For the\\nsake of simplicity and because both continuous and ordinal variables\\nare treated similarly \\n            \",\n        \"\\n            To explain the framework and the new Kriging model, we illustrate\\nthe inputs variables of the model using a classical machine\\nlearning problem related to the hyperparameters optimization of a\\nfullyconnected Multi-Layer Perceptron (MLP) \\n            \",\n        \"According to their types, the MLP input variables can be classified as\\nfollows:\\n4. The meta variable \\u2018\\u2018# of hidden layers\\u2019\\u2019 is an integer and, as\\nqnt\\nsuch, is represented by the component met .\\n5. The decreed variables \\u2018\\u2018# of neurons hidden layer \\u2019\\u2019 are integers\\nqnt\\nand, as such, are represented by the component dec.\\n6. The \\u2018\\u2018Learning rate\\u2019\\u2019, \\u2018\\u2018Momentum\\u2019\\u2019, \\u2018\\u2018Activation function\\u2019\\u2019 and\\n\\u2018\\u2018Batch size\\u2019\\u2019 are, respectively, continuous, for the first two\\n(every value between two bounds), categorical (qualitative between\\nthree choices) and integer (quantitative between 6 choices).\\nTherefore, the \\u2018\\u2018Activation function\\u2019\\u2019 and the \\u2018\\u2018Momentum\\u2019\\u2019 are\\nrepresented by the component cnaetu. The \\u2018\\u2018Learning rate\\u2019\\u2019 and the\\nqnt\\n\\u2018\\u2018Batch size\\u2019\\u2019 are represented by the component neu.\",\n        \"\\n            To model hierarchical variables, as proposed in \\n            \",\n        \"In this section, a new method to build a Kriging model with\\nhierarchical variables is introduced based on the framework aforementioned.\\nThe proposed methods are included in SMT 2.0.\",\n        \"\\n            Assuming that the decreed variables are quantitative, Hutter and\\nOsborne \\n            \",\n        \"In the following, we describe our new method to build a correlation\\nkernel for hierarchical variables. In particular, we introduce a new\\nalgebraic kernel called Alg-Kernel that behaves like the Arc-Kernel\\nwhilst correcting most of its drawbacks. In particular, our kernel does\\nnot add any hyperparameters, and the normalization is handled in a\\nnatural way.\",\n        \"\\n            For modeling purposes, we assume that the decreed space is\\nquantitative, i.e., \\ue244dec = \\ue244dqenct . Let  \\u2208 \\ue244 be an input point partitioned as\\n = (neu, met , inc(met )) and, similarly,  \\u2208 \\ue244 is another input such\\nthat  = (neu, met , inc(met )). The new kernel  that we propose for\\nhierarchical variables is given by\\n(, ) = neu(neu, neu) \\u00d7 met (met , met )\\n\\u00d7 met,dec(met , inc(met ), met , inc(met )),\\n(2)\\nwhere neu, met and met,dec are as follows:\\n\\u2022 neu represents the neutral kernel that encompasses both\\ncategorical and quantitative neutral variables, i.e., neu can be\\ndecomposed into two parts neu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, nqenut).\\nqnt\\nThe categorical kernel, denoted cat , could be any Symmetric\\nPositive Definite (SPD) \\n            \",\n        \"qnt (qnt , qnt ) = \\u220f exp(\\u2212(qnt , qnt )).\",\n        \"=1\\n\\u2022 met is the meta variables related kernel. It is also separated into\\ntwo parts: met (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett ) where the\\nquantitative kernel is ordered and not continuous because meta\\nvariables take value in a finite set.\\n\\u2022 met,dec is an SPD kernel that models the correlations between the\\nmeta levels (all the possible subspaces) and the decreed variables.\",\n        \"In what comes next, we detailed this kernel.\",\n        \"\\n            Meta-decreed kernels like the imputation kernel or the\\nArc-Kernel were first proposed in \\n            \",\n        \"Our proposed Alg-Kernel kernel is given by\",\n        \"= amlegt (met , met ) \\u00d7 daelgc(inc(met ), inc(met )).\",\n        \"Mathematically, we could consider that there is only one meta variable\\nwhose levels correspond to every possible included subspace. Let sub\\ndenotes the components indices of possible subspaces, the subspaces\\nparameterized by the meta component met are defined as \\ue244inc(met =\\n),  \\u2208 sub. It follows that the fully extended continuous decreed\\nspace writes as \\ue244dec = \\u22c3\\u2208sub \\ue244inc(met = ) and dec is the set of the\\nassociated indices. Let  denotes the set of components related to\\n,\\nthe space \\ue244inc(met , met ) containing the variables decreed-included in\\nboth \\ue244inc(met ) and \\ue244inc(met ).\",\n        \"Since the decreed variables are quantitative, one has\\ndaelgc(inc(met ), inc(met )) = qnt (inc(met ), inc(met ))\\n= \\u220f qnt (inc(met ), inc(met ))\\n(4)\\n(5)\\n\\u2208,\\nThe construction of the quantitative kernel qnt depends on a given\\ndistance denoted alg. The kernel amlegt is an induced meta kernel that\\ndepends on the same distance alg to preserve the SPD property of\\nalg\\nmet,dec. For every  \\u2208 dec, if  \\u2208 , the new algebraic distance is\\n,\\ngiven by\\nalg(inc(met ), inc(met )) = \\u239c\\u239c\\u239b \\u221a 2|inc(met ) \\u2212\\u221ainc(met )| \\u239f\\u239f\\u239e ,\\n\\u239c inc(met )2 + 1 inc(met )2 + 1 \\u239f\\n\\u239d \\u23a0\\nwhere  \\u2208 R+ is a continuous hyperparameter. Otherwise, if  \\u2208 dec\\nbut  \\u2209 , there should be a non-zero residual distance between the\\n,\\ntwo different subspaces \\ue244inc(met ) and \\ue244inc(met ) to ensure the kernel\\nSPD property. To have a residual not depending on the decreed values,\\nour model considers that there is a unit distance\\nalg(inc(met ), inc(met )) = 1.0 , \\u2200 \\u2208 dec \\u29f5 .\\n,\\nThe induced meta kernel amlegt (met , met ) to preserve the SPD property\\nof alg is defined as:\\n(6)\\n(7)\\n(3)\\namlegt (met , met ) = \\u220f qnt (1.0 ).\",\n        \"\\u2208met\",\n        \"\\n            Not only our kernel of Eq. (2) uses less hyperparameters than the\\nArcKernel (as we cut off its extra parameters) but it is also a more flexible\\nkernel as it allows different kernels for meta and decreed variables.\\nMoreover, another advantage of our kernel is that it is numerically\\nmore stable thanks to the new non-stationary \\n            \",\n        \"In what comes next, we will refer to the implementation of the\\nkernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and\\nSMT Alg-Kernel . We note also that the implementation of SMT\\nArc-Kernel differs slightly from the original Arc-Kernel as we\\nfixed some hyperparameters to 1 in order to avoid adding extra\\nhyperparameters and use the formulation of Eq. (2) and rescaling of the\\ndata.\",\n        \"In this section, we illustrate the hierarchical Arc-Kernel on the\\nMLP example. For that sake, we consider two design variables  and\\n such that  = (2.10\\u22124, 0.9, ReLU, 16, 2, 55, 51) and  = (5.10\\u22123, 0.8,\\nSigmoid, 64, 3, 50, 54, 53). Since the value of met (i.e., the number of\\nhidden layers) differs from one point to another (namely, 2 for  and 3\\nfor ), the associated variables inc(met ) have either 2 or 3 variables\\nfor the number of neurons in each layer (namely 55 and 51 for ,\\nand 50, 54 and 53 for the point ). In our case, 8\\nhyperparameters (11,2, 1, \\u2026 , 7) will have to be optimized where  is given by\\nEq. (2). These 7 hyperparameters can be described using our proposed\\nframework as follows:\\n\\u2022 For the neutral components, we have neu = (2.10\\u22124, 0.9, ReLU, 16)\\nand neu = (5.10\\u22123, 0.8, Sigmoid, 64). Therefore, for a categorical\\nmatrix kernel 1 and a square exponential quantitative kernel,\\nqnt qnt\\nneu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, neu)\\n= 11,2 exp \\u22121(2.10\\u22124 \\u2212 5.10\\u22123)2\",\n        \"exp \\u22122(0.9 \\u2212 0.8)2 exp \\u22123(16 \\u2212 64)2.\",\n        \"The values 11,2, 1, 2 and 3 need to be optimized. Here,\\n11,2 is the correlation between \\\"ReLU\\\" and \\\"Sigmoid\\\".\\n\\u2022 For the meta components, we have met = 2 and met = 3.\",\n        \"Therefore, for a square exponential quantitative kernel,\\nmet (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett )\",\n        \"= exp \\u22124(3 \\u2212 2)2.\",\n        \"The value 4 needs to be optimized.\\n\\u2022 For the meta-decreed kernel, we have met , inc(met ) = 2, (55, 51)\\nand met , inc(met ) = 3, (50, 54, 53) which gives\\nalg\\nwhere the meta induced component is met (met , met ) = exp \\u22127\\nbecause the decreed value 53 in  has nothing to be compared\\nwith in  as in Eq. (7). The values 5, 6 and 7 need to be\\noptimized which complete the description of the hyperparameters.\\nWe note that for the MLP problem, Alg-Kernel models use\\n10 hyperparameters whereas the Arc-Kernel would require\\n12 hyperparameters without the meta kernel (4) but with 3\\nextra decreed hyperparameters and the Wedge-Kernel would\\nrequire 15 hyperparameters. For deep learning applications, a\\nmore complex perceptron with up to 10 hidden layers would\\nrequire 17 hyperparameters with SMT 2.0 models against 26\\nfor Arc-Kernel and 36 for Wedge-Kernel . The next section\\nillustrates the interest of our method to build a surrogate model\\nfor this neural network engineering problem.\\n4.3. A neural network test-case using SMT 2.0\",\n        \"In this section, we apply our models to the hyperparameters\\noptimization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0\\nan example illustrates this MLP problem. For the sake of showing the\\nKriging surrogate abilities, we implemented a dummy function with no\\nsignificance to replace the real black-box that would require training\\na whole Neural Network (NN) with big data. This function requires a\\nnumber of variables that depends on the value of the meta variable,\\ni.e the number of hidden layers. To simplify, we have chosen only\\n1, 2 or 3 hidden layers and therefore, we have 3 decreed variables\\nbut deep neural networks could also be investigated as our model can\\ntackle a few dozen variables. A test case (test_hierarchical_variables_NN )\\nshows that our model SMT Alg-Kernel interpolates the data\\nproperly, checks that the data dimension is correct and also asserts that\\nthe inactive decreed variables have no influence over the prediction.\\nIn Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical\\nand mixed variables based on the implementation of SMT 2.0 for\\ntest_hierarchical_variables_NN.\",\n        \"\\n            To compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel\\nand SMT Arc-Kernel ) with the state-of-the-art imputation method\\npreviously used on industrial application (Imp-Kernel ) \\n            \",\n        \"\\n            Efficient global optimization (EGO) is a sequential Bayesian\\noptimization algorithm designed to find the optimum of a black-box\\nfunction that may be expensive to evaluate \\n            \",\n        \"\\n            Because SMT 2.0 implements Kriging models that handle mixed\\nand hierarchical variables, we can use EGO to solve problems\\ninvolving such design variables. Other Bayesian optimization algorithms\\noften adopt approaches based on solving subproblems with\\ncontinuous or non-hierarchical Kriging. This subproblem approach is less\\nefficient and scales poorly, but it can only solve simple problems.\\nSeveral Bayesian optimization software packages can handle mixed or\\nhierarchical variables with such a subproblem approach. The\\npackages include BoTorch \\n            \",\n        \"\\n            Fig. 6 compares the four EGO methods implemented in SMT 2.0:\\nSMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that\\nillustrates Bayesian optimization is a toy test case \\n            \",\n        \"\\n            In Fig. 7 we illustrate how to use EGO with mixed variables based\\non the implementation of SMT 2.0. The illustrated problem is a mixed\\nvariant of the Branin function \\n            \",\n        \"\\n            Note that a dedicated notebook is available to reproduce the results\\npresented in this paper and the mixed integer notebook also includes\\nan extra mechanical application with composite materials \\n            \",\n        \"\\n            The hierarchical test case considered in this paper to illustrate\\nBayesian optimization is a modified Goldstein function \\n            \",\n        \"\\n          The new release SMT 2.0 introduces several improvements\\nbesides Kriging for hierarchical and mixed variables. This section details\\nthe most important new contributions. Recall from Section 2.2 that\\nfive sub-modules are present in the code: Sampling, Problems,\\nSurrogate Models, Applications and Notebooks.\\n6.1. Contributions to Sampling\\nPseudo-random sampling. The Latin Hypercube Sampling (LHS) is a\\nstochastic sampling technique to generate quasi-random sampling\\ndistributions. It is among the most popular sampling method in computer\\nexperiments thanks to its simplicity and projection properties with\\nhigh-dimensional problems. The LHS method uses the pyDOE package\\n(Design Of Experiments for Python). Five criteria for the construction\\nof LHS are implemented in SMT. The first four criteria (center,\\nmaximin, centermaximin, correlation) are the same as in\\npyDOE.12 The last criterion ese, is implemented by the authors of\\nSMT \\n          \",\n        \"\\n            based on hyperparameters and on a correlation kernel. Four\\ncorrelation kernels are now implemented in SMT 2.0 \\n            \",\n        \"\\n            Noisy Kriging. In engineering and in big data contexts with real\\nexperiments, surrogate models for noisy data are of significant interest. In\\nparticular, there is a growing need for techniques like noisy Kriging\\nand noisy Multi-Fidelity Kriging (MFK) for data fusion \\n            \",\n        \"\\n            problems, the toolbox implements Kriging with partial least squares\\n(KPLS) \\n            \",\n        \"\\n            Marginal Gaussian process. SMT 2.0 implements Marginal Gaussian\\nProcess (MGP) surrogate models for high dimensional problems \\n            \",\n        \"\\n            Gradient-enhanced neural network. The new release SMT 2.0\\nimplements Gradient-Enhanced Neural Network (GENN) models \\n            \",\n        \"\\n            Parallel Bayesian optimization. Due to the recent progress made in\\nhardware configurations, it has been of high interest to perform parallel\\noptimizations. A parallel criterion called qEI \\n            \"\n      ]\n    },\n    {\n      \"title\": \"2. SMT 2.0 : an improved surrogate modeling toolbox\",\n      \"paragraphs\": [\n        \"From a software point of view, SMT 2.0 maintains and improves\\nthe modularity and generality of the original SMT version 5. In this\\nsection, we describe the software as follows. Section 2.1 describes the\\nlegacy of SMT 0.2. Then, Section 2.2 describes the organization of the\\nrepository. Finally, Section 2.3 shows the new capabilities implemented\\nin the SMT 2.0 update.\",\n        \"SMT 5 is an open-source collaborative work originally developed\\nby ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of\\nMichigan. Now, both Polytechnique Montr\\u00e9al and the University of\\nCalifornia San Diego are also contributors. SMT 2.0 updates and\\nextends the original SMT repository capabilities among which the original\\npublication 5 focuses on different types of derivatives for surrogate\\nmodels detailed hereafter.\\n3 http://smt.readthedocs.io/en/latest\\n4 https://github.com/SMTorg/smt\\n5 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\",\n        \"\\n            original main motivations for SMT was derivative support. In fact, none\\nof the existing packages for surrogate modeling such as Scikit-learn in\\nPython 7, SUMO in Matlab \\n            \",\n        \"\\n            nized along three main sub-modules that implement a set of sampling\\ntechniques (sampling_methods), benchmarking functions (problems),\\nand surrogate modeling techniques (surrogate_models). The toolbox\\ndocumentation6 is created using reStructuredText and Sphinx, a\\ndocumentation generation package for Python, with custom extensions.\\nCode snippets in the documentation pages are taken directly from\\nactual tests in the source code and are automatically updated. The\\noutput from these code snippets and tables of options are generated\\ndynamically by custom Sphinx extensions. This leads to high-quality\\ndocumentation with minimal effort. Along with user documentation,\\ndeveloper documentation is also provided to explain how to contribute\\nto SMT. This includes a list of API methods for the SurrogateModel,\\nSamplingMethod, and Problem classes, that must be implemented\\nto create a new surrogate modeling method, sampling technique, or\\nbenchmarking problem. When a developer submits a pull request, it is\\nmerged only after passing the automated tests and receiving approval\\nfrom at least one reviewer. The repository on GitHub7 is linked to\\ncontinuous integration tests (GitHub Actions) for Windows, Linux and\\nMacOS, to a coverage test on coveralls.io and to a dependency version\\ncheck for Python with DependaBot. Various parts of the source code\\nhave been accelerated using Numba \\n            \",\n        \"The main features of the open-source repository SMT 2.0 are\\ndescribed in Fig. 1. More precisely, Sampling Methods, Problems\\nand Surrogate models are kept from SMT 0.2 and two new\\nsections Models applications and Interactive notebooks\\nhave been added to the architecture of the code. These sections are\\n6 https://smt.readthedocs.org\\n7 https://github.com/SMTorg/smt\\nhighlighted in blue and detailed on Fig. 1. The new major features\\nimplemented in SMT 2.0 are highlighted in lavender whereas the\\nlegacy features that were already in present in the original publication\\nfor SMT 0.2 5 are in black.\\n2.3. New features within SMT 2.0\",\n        \"The main objective of this new release is to enable Kriging surrogate\\nmodels for use with both hierarchical and mixed variables. Moreover,\\nfor each of these five sub-modules described in Section 2.2, several\\nimprovements have been made between the original version and the\\nSMT 2.0 release.\",\n        \"\\n            Hierarchical and mixed design space. A new design space definition\\nclass DesignSpace has been added that implements hierarchical\\nand mixed functionalities. Design variables can either be\\ncontinuous (FloatVariable), ordered (OrdinalVariable) or categorical\\n(CategoricalVariable). The integer type (IntegerVariable)\\nrepresents a special case of the ordered variable, specified by bounds\\n(inclusive) rather than a list of possible values. The hierarchical\\nstructure of the design space can be defined using declare_decreed_var:\\nthis function declares that a variable is a decreed variable that is\\nactivated when the associated meta variable takes one of a set of\\nspecified values, see Section 4 for background. The DesignSpace\\nclass also implements mechanisms for sampling valid design vectors\\n(i.e. design vectors that adhere to the hierarchical structure of the\\ndesign space) using any of the below-mentioned samplers, for\\ncorrecting and imputing design vectors, and for requesting which design\\nvariables are acting in a given design vector. Correction ensures that\\nvariables have valid values (e.g. integers for discrete variables) \\n            \",\n        \"\\n            Sampling. SMT implements three methods for sampling. The first one\\nis a na\\u00efve approach, called Random that draws uniformly points along\\nevery dimension. The second sampling method is called Full\\nFactorial and draws a point for every cross combination of variables,\\nto have an \\u2018\\u2018exhaustive\\u2019\\u2019 design of experiments. The last one is the\\nLatin Hypercube Sampling (LHS) \\n            \",\n        \"Problems. SMT implements two new engineering problems: a mixed\\nvariant of a cantilever beam described in Section 3 and a hierarchical\\nneural network described in Section 4.\",\n        \"\\n            Surrogate models. In order to keep up with state-of-art, several\\nreleases done from the original version developed new options for the\\nalready existing surrogates. In particular, compared to the original\\npublication 5, SMT 2.0 adds gradient-enhanced neural networks \\n            \",\n        \"In the following, Section 3 details the Kriging based surrogate\\nmodels for mixed variables, and Section 4 presents our new Kriging\\nsurrogate for hierarchical variables. Section 5 details the EGO\\napplication and the other new relevant features aforementioned are described\\nsuccinctly in Section 6.\",\n        \"8 https://github.com/SMTorg/smt/tree/master/tutorial\\n9 https://colab.research.google.com/github/SMTorg/smt/\\n10 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\",\n        \"As mentioned in Section 1, design variables can be either of\\ncontinuous or discrete type, and a problem with both types is a mixed-variable\\nproblem. Discrete variables can be ordinal or categorical. A discrete\\nvariable is ordinal if there is an order relation within the set of possible\\nvalues. An example of an ordinal design variable is the number of\\nengines in an aircraft. A possible set of values in this case could be\\n2, 4, 8. A discrete variable is categorical if no order relation is known\\nbetween the possible choices the variable can take. One example of a\\ncategorical variable is the color of a surface. A possible example of a\\nset of choices could be blue, red, green. The possible choices are called\\nthe levels of the variable.\",\n        \"\\n            Several methods have been proposed to address the recent increase\\ninterest in mixed Kriging based models \\n            \",\n        \"The continuous and ordinal variables are both treated similarly\\nin SMT 2.0 with a continuous kernel, where the ordinal values are\\nconverted to continuous through relaxation. For categorical variables,\\nfour models (GD, CR, EHH and HH) can be used in SMT 2.0 if\\nspecified by the API. This is why we developed a unified mathematical\\nformulation that allows a unique implementation for any model.\",\n        \"Denote  the number of categorical variables. For a given  \\u2208\\n{1, \\u2026 , }, the th categorical variable is denoted  and its number\\nof levels is denoted . The hyperparameter matrix peculiar to this\\nvariable  is\",\n        \"\\u23a1 1,1 . \\u23a4\\n = \\u23a2\\u23a2\\u23a2 \\u22ee1,2 \\u22f12,2 \\u22f1 \\u23a5\\u23a5\\u23a5 ,\",\n        \"\\n            \\u23a3\\u23a21, \\u2026 \\u22121, , \\u23a6\\u23a5\\nand the categorical parameters are defined as  = {1, \\u2026 , }. For\\ntwo given inputs in the DoE, for example, the th and th points, let\\n and  be the associated categorical variables taking respectively\\nthe  and the  level on the categorical variable . The categorical\\n\\ncorrelation kernel is defined by\\n(, , ) =\\n\\n\\u220f ((), ) ((), )((), )((), )\\n=1        \\n(1)\\nwhere  is either a positive definite kernel or identity and (.) is a\\nsymmetric positive definite (SPD) function such that the matrix ()\\nis SPD if  is SPD. For an exponential kernel, Table 3 gives the\\nparameterizations of  and  that correspond to GD, CR, HH, and\\nEHH kernels. The complexity of these different kernels depends on\\nthe number of hyperparameters that characterizes them. As defined\\nby Saves et al. \\n            \",\n        \"\\n            Another Kriging based model that can use mixed variables is Kriging\\nwith partial least squares (KPLS) \\n            \",\n        \"\\n            A classic engineering problem commonly used for model validation\\nis the beam bending problem \\n            \",\n        \"\\n            To compare the mixed Kriging models of SMT 2.0, we draw a 98\\npoint LHS as training set and the validation set is a grid of 12 \\u00d7 30 \\u00d7\\n30 = 10800 points. For the four implemented methods, displacement\\nerror (computed with a root-mean-square error criterion), likelihood,\\nnumber of hyperparameters and computational time for every model\\nare shown in Table 4. For the continuous variables, we use the square\\nexponential kernel. More details are found in \\n            \"\n      ]\n    },\n    {\n      \"title\": \"2.1. Background on SMT former version: SMT 0.2\",\n      \"paragraphs\": [\n        \"SMT 5 is an open-source collaborative work originally developed\\nby ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of\\nMichigan. Now, both Polytechnique Montr\\u00e9al and the University of\\nCalifornia San Diego are also contributors. SMT 2.0 updates and\\nextends the original SMT repository capabilities among which the original\\npublication 5 focuses on different types of derivatives for surrogate\\nmodels detailed hereafter.\\n3 http://smt.readthedocs.io/en/latest\\n4 https://github.com/SMTorg/smt\\n5 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\"\n      ]\n    },\n    {\n      \"title\": \"A Python surrogate modeling framework with derivatives. One of the\",\n      \"paragraphs\": [\n        \"\\n            original main motivations for SMT was derivative support. In fact, none\\nof the existing packages for surrogate modeling such as Scikit-learn in\\nPython 7, SUMO in Matlab \\n            \"\n      ]\n    },\n    {\n      \"title\": \"Software architecture, documentation, and automatic testing. SMT is orga\",\n      \"paragraphs\": [\n        \"\\n            nized along three main sub-modules that implement a set of sampling\\ntechniques (sampling_methods), benchmarking functions (problems),\\nand surrogate modeling techniques (surrogate_models). The toolbox\\ndocumentation6 is created using reStructuredText and Sphinx, a\\ndocumentation generation package for Python, with custom extensions.\\nCode snippets in the documentation pages are taken directly from\\nactual tests in the source code and are automatically updated. The\\noutput from these code snippets and tables of options are generated\\ndynamically by custom Sphinx extensions. This leads to high-quality\\ndocumentation with minimal effort. Along with user documentation,\\ndeveloper documentation is also provided to explain how to contribute\\nto SMT. This includes a list of API methods for the SurrogateModel,\\nSamplingMethod, and Problem classes, that must be implemented\\nto create a new surrogate modeling method, sampling technique, or\\nbenchmarking problem. When a developer submits a pull request, it is\\nmerged only after passing the automated tests and receiving approval\\nfrom at least one reviewer. The repository on GitHub7 is linked to\\ncontinuous integration tests (GitHub Actions) for Windows, Linux and\\nMacOS, to a coverage test on coveralls.io and to a dependency version\\ncheck for Python with DependaBot. Various parts of the source code\\nhave been accelerated using Numba \\n            \",\n        \"The main features of the open-source repository SMT 2.0 are\\ndescribed in Fig. 1. More precisely, Sampling Methods, Problems\\nand Surrogate models are kept from SMT 0.2 and two new\\nsections Models applications and Interactive notebooks\\nhave been added to the architecture of the code. These sections are\\n6 https://smt.readthedocs.org\\n7 https://github.com/SMTorg/smt\\nhighlighted in blue and detailed on Fig. 1. The new major features\\nimplemented in SMT 2.0 are highlighted in lavender whereas the\\nlegacy features that were already in present in the original publication\\nfor SMT 0.2 5 are in black.\\n2.3. New features within SMT 2.0\",\n        \"The main objective of this new release is to enable Kriging surrogate\\nmodels for use with both hierarchical and mixed variables. Moreover,\\nfor each of these five sub-modules described in Section 2.2, several\\nimprovements have been made between the original version and the\\nSMT 2.0 release.\",\n        \"\\n            Hierarchical and mixed design space. A new design space definition\\nclass DesignSpace has been added that implements hierarchical\\nand mixed functionalities. Design variables can either be\\ncontinuous (FloatVariable), ordered (OrdinalVariable) or categorical\\n(CategoricalVariable). The integer type (IntegerVariable)\\nrepresents a special case of the ordered variable, specified by bounds\\n(inclusive) rather than a list of possible values. The hierarchical\\nstructure of the design space can be defined using declare_decreed_var:\\nthis function declares that a variable is a decreed variable that is\\nactivated when the associated meta variable takes one of a set of\\nspecified values, see Section 4 for background. The DesignSpace\\nclass also implements mechanisms for sampling valid design vectors\\n(i.e. design vectors that adhere to the hierarchical structure of the\\ndesign space) using any of the below-mentioned samplers, for\\ncorrecting and imputing design vectors, and for requesting which design\\nvariables are acting in a given design vector. Correction ensures that\\nvariables have valid values (e.g. integers for discrete variables) \\n            \",\n        \"\\n            Sampling. SMT implements three methods for sampling. The first one\\nis a na\\u00efve approach, called Random that draws uniformly points along\\nevery dimension. The second sampling method is called Full\\nFactorial and draws a point for every cross combination of variables,\\nto have an \\u2018\\u2018exhaustive\\u2019\\u2019 design of experiments. The last one is the\\nLatin Hypercube Sampling (LHS) \\n            \",\n        \"Problems. SMT implements two new engineering problems: a mixed\\nvariant of a cantilever beam described in Section 3 and a hierarchical\\nneural network described in Section 4.\",\n        \"\\n            Surrogate models. In order to keep up with state-of-art, several\\nreleases done from the original version developed new options for the\\nalready existing surrogates. In particular, compared to the original\\npublication 5, SMT 2.0 adds gradient-enhanced neural networks \\n            \",\n        \"In the following, Section 3 details the Kriging based surrogate\\nmodels for mixed variables, and Section 4 presents our new Kriging\\nsurrogate for hierarchical variables. Section 5 details the EGO\\napplication and the other new relevant features aforementioned are described\\nsuccinctly in Section 6.\",\n        \"8 https://github.com/SMTorg/smt/tree/master/tutorial\\n9 https://colab.research.google.com/github/SMTorg/smt/\\n10 https://github.com/SMTorg/smt/tree/master/tutorial/\\nNotebookRunTestCases_Paper_SMT_v2.ipynb\",\n        \"As mentioned in Section 1, design variables can be either of\\ncontinuous or discrete type, and a problem with both types is a mixed-variable\\nproblem. Discrete variables can be ordinal or categorical. A discrete\\nvariable is ordinal if there is an order relation within the set of possible\\nvalues. An example of an ordinal design variable is the number of\\nengines in an aircraft. A possible set of values in this case could be\\n2, 4, 8. A discrete variable is categorical if no order relation is known\\nbetween the possible choices the variable can take. One example of a\\ncategorical variable is the color of a surface. A possible example of a\\nset of choices could be blue, red, green. The possible choices are called\\nthe levels of the variable.\",\n        \"\\n            Several methods have been proposed to address the recent increase\\ninterest in mixed Kriging based models \\n            \"\n      ]\n    },\n    {\n      \"title\": \"3.1. Mixed Gaussian processes\",\n      \"paragraphs\": [\n        \"The continuous and ordinal variables are both treated similarly\\nin SMT 2.0 with a continuous kernel, where the ordinal values are\\nconverted to continuous through relaxation. For categorical variables,\\nfour models (GD, CR, EHH and HH) can be used in SMT 2.0 if\\nspecified by the API. This is why we developed a unified mathematical\\nformulation that allows a unique implementation for any model.\",\n        \"Denote  the number of categorical variables. For a given  \\u2208\\n{1, \\u2026 , }, the th categorical variable is denoted  and its number\\nof levels is denoted . The hyperparameter matrix peculiar to this\\nvariable  is\",\n        \"\\u23a1 1,1 . \\u23a4\\n = \\u23a2\\u23a2\\u23a2 \\u22ee1,2 \\u22f12,2 \\u22f1 \\u23a5\\u23a5\\u23a5 ,\",\n        \"\\n            \\u23a3\\u23a21, \\u2026 \\u22121, , \\u23a6\\u23a5\\nand the categorical parameters are defined as  = {1, \\u2026 , }. For\\ntwo given inputs in the DoE, for example, the th and th points, let\\n and  be the associated categorical variables taking respectively\\nthe  and the  level on the categorical variable . The categorical\\n\\ncorrelation kernel is defined by\\n(, , ) =\\n\\n\\u220f ((), ) ((), )((), )((), )\\n=1        \\n(1)\\nwhere  is either a positive definite kernel or identity and (.) is a\\nsymmetric positive definite (SPD) function such that the matrix ()\\nis SPD if  is SPD. For an exponential kernel, Table 3 gives the\\nparameterizations of  and  that correspond to GD, CR, HH, and\\nEHH kernels. The complexity of these different kernels depends on\\nthe number of hyperparameters that characterizes them. As defined\\nby Saves et al. \\n            \",\n        \"\\n            Another Kriging based model that can use mixed variables is Kriging\\nwith partial least squares (KPLS) \\n            \"\n      ]\n    },\n    {\n      \"title\": \"3.2. An engineering design test-case\",\n      \"paragraphs\": [\n        \"\\n            A classic engineering problem commonly used for model validation\\nis the beam bending problem \\n            \",\n        \"\\n            To compare the mixed Kriging models of SMT 2.0, we draw a 98\\npoint LHS as training set and the validation set is a grid of 12 \\u00d7 30 \\u00d7\\n30 = 10800 points. For the four implemented methods, displacement\\nerror (computed with a root-mean-square error criterion), likelihood,\\nnumber of hyperparameters and computational time for every model\\nare shown in Table 4. For the continuous variables, we use the square\\nexponential kernel. More details are found in \\n            \"\n      ]\n    },\n    {\n      \"title\": \"4. Surrogate models with hierarchical variables in SMT 2.0\",\n      \"paragraphs\": [\n        \"\\n          To introduce the newly developed Kriging model for hierarchical\\nvariables implemented in SMT 2.0, we present the general\\nmathematical framework for hierarchical and mixed variables established\\nby Audet et al. \\n          \",\n        \"\\n            A problem structure is classified as hierarchical when the sets of\\nactive variables depend on architectural choices. This occurs frequently\\nin industrial design problems. In hierarchical problems, we can classify\\nvariables as neutral, meta (also known as dimensional) or decreed\\n(also known as conditionally active) as detailed in Audet et al. \\n            \",\n        \"However, the wing aspect ratio being neutral, it is not affected by this\\nhierarchy.\",\n        \"\\n            Problems involving hierarchical variables are generally dependant\\non discrete architectures and as such involve mixed variables. Hence,\\nin addition to their role (neutral, meta or decreed), each variable also\\nhas a variable type amongst categorical, ordinal or continuous. For the\\nsake of simplicity and because both continuous and ordinal variables\\nare treated similarly \\n            \",\n        \"\\n            To explain the framework and the new Kriging model, we illustrate\\nthe inputs variables of the model using a classical machine\\nlearning problem related to the hyperparameters optimization of a\\nfullyconnected Multi-Layer Perceptron (MLP) \\n            \",\n        \"According to their types, the MLP input variables can be classified as\\nfollows:\\n4. The meta variable \\u2018\\u2018# of hidden layers\\u2019\\u2019 is an integer and, as\\nqnt\\nsuch, is represented by the component met .\\n5. The decreed variables \\u2018\\u2018# of neurons hidden layer \\u2019\\u2019 are integers\\nqnt\\nand, as such, are represented by the component dec.\\n6. The \\u2018\\u2018Learning rate\\u2019\\u2019, \\u2018\\u2018Momentum\\u2019\\u2019, \\u2018\\u2018Activation function\\u2019\\u2019 and\\n\\u2018\\u2018Batch size\\u2019\\u2019 are, respectively, continuous, for the first two\\n(every value between two bounds), categorical (qualitative between\\nthree choices) and integer (quantitative between 6 choices).\\nTherefore, the \\u2018\\u2018Activation function\\u2019\\u2019 and the \\u2018\\u2018Momentum\\u2019\\u2019 are\\nrepresented by the component cnaetu. The \\u2018\\u2018Learning rate\\u2019\\u2019 and the\\nqnt\\n\\u2018\\u2018Batch size\\u2019\\u2019 are represented by the component neu.\",\n        \"\\n            To model hierarchical variables, as proposed in \\n            \",\n        \"In this section, a new method to build a Kriging model with\\nhierarchical variables is introduced based on the framework aforementioned.\\nThe proposed methods are included in SMT 2.0.\",\n        \"\\n            Assuming that the decreed variables are quantitative, Hutter and\\nOsborne \\n            \",\n        \"In the following, we describe our new method to build a correlation\\nkernel for hierarchical variables. In particular, we introduce a new\\nalgebraic kernel called Alg-Kernel that behaves like the Arc-Kernel\\nwhilst correcting most of its drawbacks. In particular, our kernel does\\nnot add any hyperparameters, and the normalization is handled in a\\nnatural way.\",\n        \"\\n            For modeling purposes, we assume that the decreed space is\\nquantitative, i.e., \\ue244dec = \\ue244dqenct . Let  \\u2208 \\ue244 be an input point partitioned as\\n = (neu, met , inc(met )) and, similarly,  \\u2208 \\ue244 is another input such\\nthat  = (neu, met , inc(met )). The new kernel  that we propose for\\nhierarchical variables is given by\\n(, ) = neu(neu, neu) \\u00d7 met (met , met )\\n\\u00d7 met,dec(met , inc(met ), met , inc(met )),\\n(2)\\nwhere neu, met and met,dec are as follows:\\n\\u2022 neu represents the neutral kernel that encompasses both\\ncategorical and quantitative neutral variables, i.e., neu can be\\ndecomposed into two parts neu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, nqenut).\\nqnt\\nThe categorical kernel, denoted cat , could be any Symmetric\\nPositive Definite (SPD) \\n            \",\n        \"qnt (qnt , qnt ) = \\u220f exp(\\u2212(qnt , qnt )).\",\n        \"=1\\n\\u2022 met is the meta variables related kernel. It is also separated into\\ntwo parts: met (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett ) where the\\nquantitative kernel is ordered and not continuous because meta\\nvariables take value in a finite set.\\n\\u2022 met,dec is an SPD kernel that models the correlations between the\\nmeta levels (all the possible subspaces) and the decreed variables.\",\n        \"In what comes next, we detailed this kernel.\",\n        \"\\n            Meta-decreed kernels like the imputation kernel or the\\nArc-Kernel were first proposed in \\n            \",\n        \"Our proposed Alg-Kernel kernel is given by\",\n        \"= amlegt (met , met ) \\u00d7 daelgc(inc(met ), inc(met )).\",\n        \"Mathematically, we could consider that there is only one meta variable\\nwhose levels correspond to every possible included subspace. Let sub\\ndenotes the components indices of possible subspaces, the subspaces\\nparameterized by the meta component met are defined as \\ue244inc(met =\\n),  \\u2208 sub. It follows that the fully extended continuous decreed\\nspace writes as \\ue244dec = \\u22c3\\u2208sub \\ue244inc(met = ) and dec is the set of the\\nassociated indices. Let  denotes the set of components related to\\n,\\nthe space \\ue244inc(met , met ) containing the variables decreed-included in\\nboth \\ue244inc(met ) and \\ue244inc(met ).\",\n        \"Since the decreed variables are quantitative, one has\\ndaelgc(inc(met ), inc(met )) = qnt (inc(met ), inc(met ))\\n= \\u220f qnt (inc(met ), inc(met ))\\n(4)\\n(5)\\n\\u2208,\\nThe construction of the quantitative kernel qnt depends on a given\\ndistance denoted alg. The kernel amlegt is an induced meta kernel that\\ndepends on the same distance alg to preserve the SPD property of\\nalg\\nmet,dec. For every  \\u2208 dec, if  \\u2208 , the new algebraic distance is\\n,\\ngiven by\\nalg(inc(met ), inc(met )) = \\u239c\\u239c\\u239b \\u221a 2|inc(met ) \\u2212\\u221ainc(met )| \\u239f\\u239f\\u239e ,\\n\\u239c inc(met )2 + 1 inc(met )2 + 1 \\u239f\\n\\u239d \\u23a0\\nwhere  \\u2208 R+ is a continuous hyperparameter. Otherwise, if  \\u2208 dec\\nbut  \\u2209 , there should be a non-zero residual distance between the\\n,\\ntwo different subspaces \\ue244inc(met ) and \\ue244inc(met ) to ensure the kernel\\nSPD property. To have a residual not depending on the decreed values,\\nour model considers that there is a unit distance\\nalg(inc(met ), inc(met )) = 1.0 , \\u2200 \\u2208 dec \\u29f5 .\\n,\\nThe induced meta kernel amlegt (met , met ) to preserve the SPD property\\nof alg is defined as:\\n(6)\\n(7)\\n(3)\\namlegt (met , met ) = \\u220f qnt (1.0 ).\",\n        \"\\u2208met\",\n        \"\\n            Not only our kernel of Eq. (2) uses less hyperparameters than the\\nArcKernel (as we cut off its extra parameters) but it is also a more flexible\\nkernel as it allows different kernels for meta and decreed variables.\\nMoreover, another advantage of our kernel is that it is numerically\\nmore stable thanks to the new non-stationary \\n            \",\n        \"In what comes next, we will refer to the implementation of the\\nkernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and\\nSMT Alg-Kernel . We note also that the implementation of SMT\\nArc-Kernel differs slightly from the original Arc-Kernel as we\\nfixed some hyperparameters to 1 in order to avoid adding extra\\nhyperparameters and use the formulation of Eq. (2) and rescaling of the\\ndata.\",\n        \"In this section, we illustrate the hierarchical Arc-Kernel on the\\nMLP example. For that sake, we consider two design variables  and\\n such that  = (2.10\\u22124, 0.9, ReLU, 16, 2, 55, 51) and  = (5.10\\u22123, 0.8,\\nSigmoid, 64, 3, 50, 54, 53). Since the value of met (i.e., the number of\\nhidden layers) differs from one point to another (namely, 2 for  and 3\\nfor ), the associated variables inc(met ) have either 2 or 3 variables\\nfor the number of neurons in each layer (namely 55 and 51 for ,\\nand 50, 54 and 53 for the point ). In our case, 8\\nhyperparameters (11,2, 1, \\u2026 , 7) will have to be optimized where  is given by\\nEq. (2). These 7 hyperparameters can be described using our proposed\\nframework as follows:\\n\\u2022 For the neutral components, we have neu = (2.10\\u22124, 0.9, ReLU, 16)\\nand neu = (5.10\\u22123, 0.8, Sigmoid, 64). Therefore, for a categorical\\nmatrix kernel 1 and a square exponential quantitative kernel,\\nqnt qnt\\nneu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, neu)\\n= 11,2 exp \\u22121(2.10\\u22124 \\u2212 5.10\\u22123)2\",\n        \"exp \\u22122(0.9 \\u2212 0.8)2 exp \\u22123(16 \\u2212 64)2.\",\n        \"The values 11,2, 1, 2 and 3 need to be optimized. Here,\\n11,2 is the correlation between \\\"ReLU\\\" and \\\"Sigmoid\\\".\\n\\u2022 For the meta components, we have met = 2 and met = 3.\",\n        \"Therefore, for a square exponential quantitative kernel,\\nmet (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett )\",\n        \"= exp \\u22124(3 \\u2212 2)2.\",\n        \"The value 4 needs to be optimized.\\n\\u2022 For the meta-decreed kernel, we have met , inc(met ) = 2, (55, 51)\\nand met , inc(met ) = 3, (50, 54, 53) which gives\\nalg\\nwhere the meta induced component is met (met , met ) = exp \\u22127\\nbecause the decreed value 53 in  has nothing to be compared\\nwith in  as in Eq. (7). The values 5, 6 and 7 need to be\\noptimized which complete the description of the hyperparameters.\\nWe note that for the MLP problem, Alg-Kernel models use\\n10 hyperparameters whereas the Arc-Kernel would require\\n12 hyperparameters without the meta kernel (4) but with 3\\nextra decreed hyperparameters and the Wedge-Kernel would\\nrequire 15 hyperparameters. For deep learning applications, a\\nmore complex perceptron with up to 10 hidden layers would\\nrequire 17 hyperparameters with SMT 2.0 models against 26\\nfor Arc-Kernel and 36 for Wedge-Kernel . The next section\\nillustrates the interest of our method to build a surrogate model\\nfor this neural network engineering problem.\\n4.3. A neural network test-case using SMT 2.0\",\n        \"In this section, we apply our models to the hyperparameters\\noptimization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0\\nan example illustrates this MLP problem. For the sake of showing the\\nKriging surrogate abilities, we implemented a dummy function with no\\nsignificance to replace the real black-box that would require training\\na whole Neural Network (NN) with big data. This function requires a\\nnumber of variables that depends on the value of the meta variable,\\ni.e the number of hidden layers. To simplify, we have chosen only\\n1, 2 or 3 hidden layers and therefore, we have 3 decreed variables\\nbut deep neural networks could also be investigated as our model can\\ntackle a few dozen variables. A test case (test_hierarchical_variables_NN )\\nshows that our model SMT Alg-Kernel interpolates the data\\nproperly, checks that the data dimension is correct and also asserts that\\nthe inactive decreed variables have no influence over the prediction.\\nIn Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical\\nand mixed variables based on the implementation of SMT 2.0 for\\ntest_hierarchical_variables_NN.\",\n        \"\\n            To compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel\\nand SMT Arc-Kernel ) with the state-of-the-art imputation method\\npreviously used on industrial application (Imp-Kernel ) \\n            \",\n        \"\\n            Efficient global optimization (EGO) is a sequential Bayesian\\noptimization algorithm designed to find the optimum of a black-box\\nfunction that may be expensive to evaluate \\n            \",\n        \"\\n            Because SMT 2.0 implements Kriging models that handle mixed\\nand hierarchical variables, we can use EGO to solve problems\\ninvolving such design variables. Other Bayesian optimization algorithms\\noften adopt approaches based on solving subproblems with\\ncontinuous or non-hierarchical Kriging. This subproblem approach is less\\nefficient and scales poorly, but it can only solve simple problems.\\nSeveral Bayesian optimization software packages can handle mixed or\\nhierarchical variables with such a subproblem approach. The\\npackages include BoTorch \\n            \",\n        \"\\n            Fig. 6 compares the four EGO methods implemented in SMT 2.0:\\nSMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that\\nillustrates Bayesian optimization is a toy test case \\n            \",\n        \"\\n            In Fig. 7 we illustrate how to use EGO with mixed variables based\\non the implementation of SMT 2.0. The illustrated problem is a mixed\\nvariant of the Branin function \\n            \",\n        \"\\n            Note that a dedicated notebook is available to reproduce the results\\npresented in this paper and the mixed integer notebook also includes\\nan extra mechanical application with composite materials \\n            \",\n        \"\\n            The hierarchical test case considered in this paper to illustrate\\nBayesian optimization is a modified Goldstein function \\n            \"\n      ]\n    },\n    {\n      \"title\": \"4.1. The hierarchical variables framework\",\n      \"paragraphs\": [\n        \"\\n            A problem structure is classified as hierarchical when the sets of\\nactive variables depend on architectural choices. This occurs frequently\\nin industrial design problems. In hierarchical problems, we can classify\\nvariables as neutral, meta (also known as dimensional) or decreed\\n(also known as conditionally active) as detailed in Audet et al. \\n            \",\n        \"However, the wing aspect ratio being neutral, it is not affected by this\\nhierarchy.\",\n        \"\\n            Problems involving hierarchical variables are generally dependant\\non discrete architectures and as such involve mixed variables. Hence,\\nin addition to their role (neutral, meta or decreed), each variable also\\nhas a variable type amongst categorical, ordinal or continuous. For the\\nsake of simplicity and because both continuous and ordinal variables\\nare treated similarly \\n            \",\n        \"\\n            To explain the framework and the new Kriging model, we illustrate\\nthe inputs variables of the model using a classical machine\\nlearning problem related to the hyperparameters optimization of a\\nfullyconnected Multi-Layer Perceptron (MLP) \\n            \",\n        \"According to their types, the MLP input variables can be classified as\\nfollows:\\n4. The meta variable \\u2018\\u2018# of hidden layers\\u2019\\u2019 is an integer and, as\\nqnt\\nsuch, is represented by the component met .\\n5. The decreed variables \\u2018\\u2018# of neurons hidden layer \\u2019\\u2019 are integers\\nqnt\\nand, as such, are represented by the component dec.\\n6. The \\u2018\\u2018Learning rate\\u2019\\u2019, \\u2018\\u2018Momentum\\u2019\\u2019, \\u2018\\u2018Activation function\\u2019\\u2019 and\\n\\u2018\\u2018Batch size\\u2019\\u2019 are, respectively, continuous, for the first two\\n(every value between two bounds), categorical (qualitative between\\nthree choices) and integer (quantitative between 6 choices).\\nTherefore, the \\u2018\\u2018Activation function\\u2019\\u2019 and the \\u2018\\u2018Momentum\\u2019\\u2019 are\\nrepresented by the component cnaetu. The \\u2018\\u2018Learning rate\\u2019\\u2019 and the\\nqnt\\n\\u2018\\u2018Batch size\\u2019\\u2019 are represented by the component neu.\",\n        \"\\n            To model hierarchical variables, as proposed in \\n            \",\n        \"In this section, a new method to build a Kriging model with\\nhierarchical variables is introduced based on the framework aforementioned.\\nThe proposed methods are included in SMT 2.0.\"\n      ]\n    },\n    {\n      \"title\": \"4.2.1. Motivation and state-of-the-art\",\n      \"paragraphs\": [\n        \"\\n            Assuming that the decreed variables are quantitative, Hutter and\\nOsborne \\n            \",\n        \"In the following, we describe our new method to build a correlation\\nkernel for hierarchical variables. In particular, we introduce a new\\nalgebraic kernel called Alg-Kernel that behaves like the Arc-Kernel\\nwhilst correcting most of its drawbacks. In particular, our kernel does\\nnot add any hyperparameters, and the normalization is handled in a\\nnatural way.\"\n      ]\n    },\n    {\n      \"title\": \"4.2.2. A new hierarchical correlation kernel\",\n      \"paragraphs\": [\n        \"\\n            For modeling purposes, we assume that the decreed space is\\nquantitative, i.e., \\ue244dec = \\ue244dqenct . Let  \\u2208 \\ue244 be an input point partitioned as\\n = (neu, met , inc(met )) and, similarly,  \\u2208 \\ue244 is another input such\\nthat  = (neu, met , inc(met )). The new kernel  that we propose for\\nhierarchical variables is given by\\n(, ) = neu(neu, neu) \\u00d7 met (met , met )\\n\\u00d7 met,dec(met , inc(met ), met , inc(met )),\\n(2)\\nwhere neu, met and met,dec are as follows:\\n\\u2022 neu represents the neutral kernel that encompasses both\\ncategorical and quantitative neutral variables, i.e., neu can be\\ndecomposed into two parts neu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, nqenut).\\nqnt\\nThe categorical kernel, denoted cat , could be any Symmetric\\nPositive Definite (SPD) \\n            \",\n        \"qnt (qnt , qnt ) = \\u220f exp(\\u2212(qnt , qnt )).\",\n        \"=1\\n\\u2022 met is the meta variables related kernel. It is also separated into\\ntwo parts: met (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett ) where the\\nquantitative kernel is ordered and not continuous because meta\\nvariables take value in a finite set.\\n\\u2022 met,dec is an SPD kernel that models the correlations between the\\nmeta levels (all the possible subspaces) and the decreed variables.\",\n        \"In what comes next, we detailed this kernel.\"\n      ]\n    },\n    {\n      \"title\": \"4.2.3. Towards an algebraic meta-decreed kernel\",\n      \"paragraphs\": [\n        \"\\n            Meta-decreed kernels like the imputation kernel or the\\nArc-Kernel were first proposed in \\n            \",\n        \"Our proposed Alg-Kernel kernel is given by\",\n        \"= amlegt (met , met ) \\u00d7 daelgc(inc(met ), inc(met )).\",\n        \"Mathematically, we could consider that there is only one meta variable\\nwhose levels correspond to every possible included subspace. Let sub\\ndenotes the components indices of possible subspaces, the subspaces\\nparameterized by the meta component met are defined as \\ue244inc(met =\\n),  \\u2208 sub. It follows that the fully extended continuous decreed\\nspace writes as \\ue244dec = \\u22c3\\u2208sub \\ue244inc(met = ) and dec is the set of the\\nassociated indices. Let  denotes the set of components related to\\n,\\nthe space \\ue244inc(met , met ) containing the variables decreed-included in\\nboth \\ue244inc(met ) and \\ue244inc(met ).\",\n        \"Since the decreed variables are quantitative, one has\\ndaelgc(inc(met ), inc(met )) = qnt (inc(met ), inc(met ))\\n= \\u220f qnt (inc(met ), inc(met ))\\n(4)\\n(5)\\n\\u2208,\\nThe construction of the quantitative kernel qnt depends on a given\\ndistance denoted alg. The kernel amlegt is an induced meta kernel that\\ndepends on the same distance alg to preserve the SPD property of\\nalg\\nmet,dec. For every  \\u2208 dec, if  \\u2208 , the new algebraic distance is\\n,\\ngiven by\\nalg(inc(met ), inc(met )) = \\u239c\\u239c\\u239b \\u221a 2|inc(met ) \\u2212\\u221ainc(met )| \\u239f\\u239f\\u239e ,\\n\\u239c inc(met )2 + 1 inc(met )2 + 1 \\u239f\\n\\u239d \\u23a0\\nwhere  \\u2208 R+ is a continuous hyperparameter. Otherwise, if  \\u2208 dec\\nbut  \\u2209 , there should be a non-zero residual distance between the\\n,\\ntwo different subspaces \\ue244inc(met ) and \\ue244inc(met ) to ensure the kernel\\nSPD property. To have a residual not depending on the decreed values,\\nour model considers that there is a unit distance\\nalg(inc(met ), inc(met )) = 1.0 , \\u2200 \\u2208 dec \\u29f5 .\\n,\\nThe induced meta kernel amlegt (met , met ) to preserve the SPD property\\nof alg is defined as:\\n(6)\\n(7)\\n(3)\\namlegt (met , met ) = \\u220f qnt (1.0 ).\",\n        \"\\u2208met\",\n        \"\\n            Not only our kernel of Eq. (2) uses less hyperparameters than the\\nArcKernel (as we cut off its extra parameters) but it is also a more flexible\\nkernel as it allows different kernels for meta and decreed variables.\\nMoreover, another advantage of our kernel is that it is numerically\\nmore stable thanks to the new non-stationary \\n            \",\n        \"In what comes next, we will refer to the implementation of the\\nkernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and\\nSMT Alg-Kernel . We note also that the implementation of SMT\\nArc-Kernel differs slightly from the original Arc-Kernel as we\\nfixed some hyperparameters to 1 in order to avoid adding extra\\nhyperparameters and use the formulation of Eq. (2) and rescaling of the\\ndata.\"\n      ]\n    },\n    {\n      \"title\": \"4.2.4. Illustration on the MLP problem\",\n      \"paragraphs\": [\n        \"In this section, we illustrate the hierarchical Arc-Kernel on the\\nMLP example. For that sake, we consider two design variables  and\\n such that  = (2.10\\u22124, 0.9, ReLU, 16, 2, 55, 51) and  = (5.10\\u22123, 0.8,\\nSigmoid, 64, 3, 50, 54, 53). Since the value of met (i.e., the number of\\nhidden layers) differs from one point to another (namely, 2 for  and 3\\nfor ), the associated variables inc(met ) have either 2 or 3 variables\\nfor the number of neurons in each layer (namely 55 and 51 for ,\\nand 50, 54 and 53 for the point ). In our case, 8\\nhyperparameters (11,2, 1, \\u2026 , 7) will have to be optimized where  is given by\\nEq. (2). These 7 hyperparameters can be described using our proposed\\nframework as follows:\\n\\u2022 For the neutral components, we have neu = (2.10\\u22124, 0.9, ReLU, 16)\\nand neu = (5.10\\u22123, 0.8, Sigmoid, 64). Therefore, for a categorical\\nmatrix kernel 1 and a square exponential quantitative kernel,\\nqnt qnt\\nneu(neu, neu) = cat (cnaetu, cnaetu)qnt (neu, neu)\\n= 11,2 exp \\u22121(2.10\\u22124 \\u2212 5.10\\u22123)2\",\n        \"exp \\u22122(0.9 \\u2212 0.8)2 exp \\u22123(16 \\u2212 64)2.\",\n        \"The values 11,2, 1, 2 and 3 need to be optimized. Here,\\n11,2 is the correlation between \\\"ReLU\\\" and \\\"Sigmoid\\\".\\n\\u2022 For the meta components, we have met = 2 and met = 3.\",\n        \"Therefore, for a square exponential quantitative kernel,\\nmet (met , met ) = cat (cmaett , cmaett )qnt (qmnett , qmnett )\",\n        \"= exp \\u22124(3 \\u2212 2)2.\",\n        \"The value 4 needs to be optimized.\\n\\u2022 For the meta-decreed kernel, we have met , inc(met ) = 2, (55, 51)\\nand met , inc(met ) = 3, (50, 54, 53) which gives\\nalg\\nwhere the meta induced component is met (met , met ) = exp \\u22127\\nbecause the decreed value 53 in  has nothing to be compared\\nwith in  as in Eq. (7). The values 5, 6 and 7 need to be\\noptimized which complete the description of the hyperparameters.\\nWe note that for the MLP problem, Alg-Kernel models use\\n10 hyperparameters whereas the Arc-Kernel would require\\n12 hyperparameters without the meta kernel (4) but with 3\\nextra decreed hyperparameters and the Wedge-Kernel would\\nrequire 15 hyperparameters. For deep learning applications, a\\nmore complex perceptron with up to 10 hidden layers would\\nrequire 17 hyperparameters with SMT 2.0 models against 26\\nfor Arc-Kernel and 36 for Wedge-Kernel . The next section\\nillustrates the interest of our method to build a surrogate model\\nfor this neural network engineering problem.\\n4.3. A neural network test-case using SMT 2.0\",\n        \"In this section, we apply our models to the hyperparameters\\noptimization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0\\nan example illustrates this MLP problem. For the sake of showing the\\nKriging surrogate abilities, we implemented a dummy function with no\\nsignificance to replace the real black-box that would require training\\na whole Neural Network (NN) with big data. This function requires a\\nnumber of variables that depends on the value of the meta variable,\\ni.e the number of hidden layers. To simplify, we have chosen only\\n1, 2 or 3 hidden layers and therefore, we have 3 decreed variables\\nbut deep neural networks could also be investigated as our model can\\ntackle a few dozen variables. A test case (test_hierarchical_variables_NN )\\nshows that our model SMT Alg-Kernel interpolates the data\\nproperly, checks that the data dimension is correct and also asserts that\\nthe inactive decreed variables have no influence over the prediction.\\nIn Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical\\nand mixed variables based on the implementation of SMT 2.0 for\\ntest_hierarchical_variables_NN.\",\n        \"\\n            To compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel\\nand SMT Arc-Kernel ) with the state-of-the-art imputation method\\npreviously used on industrial application (Imp-Kernel ) \\n            \",\n        \"\\n            Efficient global optimization (EGO) is a sequential Bayesian\\noptimization algorithm designed to find the optimum of a black-box\\nfunction that may be expensive to evaluate \\n            \",\n        \"\\n            Because SMT 2.0 implements Kriging models that handle mixed\\nand hierarchical variables, we can use EGO to solve problems\\ninvolving such design variables. Other Bayesian optimization algorithms\\noften adopt approaches based on solving subproblems with\\ncontinuous or non-hierarchical Kriging. This subproblem approach is less\\nefficient and scales poorly, but it can only solve simple problems.\\nSeveral Bayesian optimization software packages can handle mixed or\\nhierarchical variables with such a subproblem approach. The\\npackages include BoTorch \\n            \"\n      ]\n    },\n    {\n      \"title\": \"5.1. A mixed optimization problem\",\n      \"paragraphs\": [\n        \"\\n            Fig. 6 compares the four EGO methods implemented in SMT 2.0:\\nSMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that\\nillustrates Bayesian optimization is a toy test case \\n            \",\n        \"\\n            In Fig. 7 we illustrate how to use EGO with mixed variables based\\non the implementation of SMT 2.0. The illustrated problem is a mixed\\nvariant of the Branin function \\n            \",\n        \"\\n            Note that a dedicated notebook is available to reproduce the results\\npresented in this paper and the mixed integer notebook also includes\\nan extra mechanical application with composite materials \\n            \",\n        \"\\n            The hierarchical test case considered in this paper to illustrate\\nBayesian optimization is a modified Goldstein function \\n            \"\n      ]\n    },\n    {\n      \"title\": \"6. Other relevant contributions in SMT 2.0\",\n      \"paragraphs\": [\n        \"\\n          The new release SMT 2.0 introduces several improvements\\nbesides Kriging for hierarchical and mixed variables. This section details\\nthe most important new contributions. Recall from Section 2.2 that\\nfive sub-modules are present in the code: Sampling, Problems,\\nSurrogate Models, Applications and Notebooks.\\n6.1. Contributions to Sampling\\nPseudo-random sampling. The Latin Hypercube Sampling (LHS) is a\\nstochastic sampling technique to generate quasi-random sampling\\ndistributions. It is among the most popular sampling method in computer\\nexperiments thanks to its simplicity and projection properties with\\nhigh-dimensional problems. The LHS method uses the pyDOE package\\n(Design Of Experiments for Python). Five criteria for the construction\\nof LHS are implemented in SMT. The first four criteria (center,\\nmaximin, centermaximin, correlation) are the same as in\\npyDOE.12 The last criterion ese, is implemented by the authors of\\nSMT \\n          \",\n        \"\\n            based on hyperparameters and on a correlation kernel. Four\\ncorrelation kernels are now implemented in SMT 2.0 \\n            \",\n        \"\\n            Noisy Kriging. In engineering and in big data contexts with real\\nexperiments, surrogate models for noisy data are of significant interest. In\\nparticular, there is a growing need for techniques like noisy Kriging\\nand noisy Multi-Fidelity Kriging (MFK) for data fusion \\n            \",\n        \"\\n            problems, the toolbox implements Kriging with partial least squares\\n(KPLS) \\n            \",\n        \"\\n            Marginal Gaussian process. SMT 2.0 implements Marginal Gaussian\\nProcess (MGP) surrogate models for high dimensional problems \\n            \",\n        \"\\n            Gradient-enhanced neural network. The new release SMT 2.0\\nimplements Gradient-Enhanced Neural Network (GENN) models \\n            \",\n        \"\\n            Parallel Bayesian optimization. Due to the recent progress made in\\nhardware configurations, it has been of high interest to perform parallel\\noptimizations. A parallel criterion called qEI \\n            \"\n      ]\n    },\n    {\n      \"title\": \"New kernels and their derivatives for Kriging. Kriging surrogates are\",\n      \"paragraphs\": [\n        \"\\n            based on hyperparameters and on a correlation kernel. Four\\ncorrelation kernels are now implemented in SMT 2.0 \\n            \",\n        \"\\n            Noisy Kriging. In engineering and in big data contexts with real\\nexperiments, surrogate models for noisy data are of significant interest. In\\nparticular, there is a growing need for techniques like noisy Kriging\\nand noisy Multi-Fidelity Kriging (MFK) for data fusion \\n            \"\n      ]\n    },\n    {\n      \"title\": \"Kriging with partial least squares. Beside MGP, for high-dimensional\",\n      \"paragraphs\": [\n        \"\\n            problems, the toolbox implements Kriging with partial least squares\\n(KPLS) \\n            \",\n        \"\\n            Marginal Gaussian process. SMT 2.0 implements Marginal Gaussian\\nProcess (MGP) surrogate models for high dimensional problems \\n            \",\n        \"\\n            Gradient-enhanced neural network. The new release SMT 2.0\\nimplements Gradient-Enhanced Neural Network (GENN) models \\n            \",\n        \"\\n            Parallel Bayesian optimization. Due to the recent progress made in\\nhardware configurations, it has been of high interest to perform parallel\\noptimizations. A parallel criterion called qEI \\n            \"\n      ]\n    },\n    {\n      \"title\": \"7. Conclusion\",\n      \"paragraphs\": [\n        \"SMT 2.0 introduces significant upgrades to the Surrogate Modeling\\nToolbox. This new release adds support for hierarchical and mixed\\nvariables and improves the surrogate models with a particular focus\\non Kriging (Gaussian process) models. SMT 2.0 is distributed through\\nan open-source license and is freely available online.14 We provide\\ndocumentation that caters to both users and potential developers.15\\nSMT 2.0 enables users and developers collaborating on the same\\nproject to have a common surrogate modeling tool that facilitates the\\nexchange of methods and reproducibility of results.\",\n        \"SMT has been widely used in aerospace and mechanical modeling\\napplications. Moreover, the toolbox is general and can be useful for\\nanyone who needs to use or develop surrogate modeling techniques,\\nregardless of the targeted applications. SMT is currently the only\\nopensource toolbox that can build hierarchical and mixed surrogate models.\"\n      ]\n    },\n    {\n      \"title\": \"Declaration of competing interest\",\n      \"paragraphs\": [\n        \"The authors declare that they have no known competing\\nfinancial interests or personal relationships that could have appeared to\\ninfluence the work reported in this paper.\"\n      ]\n    },\n    {\n      \"title\": \"Data availability\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Acknowledgments\",\n      \"paragraphs\": [\n        \"Data will be made available on request. Results can be reproduced\\nfreely online at https://colab.research.google.com/github/SMTorg/smt/\\nblob/master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\",\n        \"We want to thank all those who contribute to this release. Namely,\\nM. A. Bouhlel, I. Cardoso, R. Carreira Rufato, R. Charayron, R. Conde\\nArenzana, S. Dubreuil, A. F. L\\u00f3pez-Lopera, M. Meliani, M. Menz, N.\\nMo\\u00ebllo, A. Thouvenot, R. Priem, E. Roux and F. Vergnes. This work is\\npart of the activities of ONERA - ISAE - ENAC joint research group. We\\nalso acknowledge the partners institutions: ONERA, NASA Glenn,\\nISAESUPAERO, Institut Cl\\u00e9ment Ader (ICA), the University of Michigan,\\nPolytechnique Montr\\u00e9al and the University of California San Diego.\",\n        \"The research presented in this paper has been performed in the\\nframework of the AGILE 4.0 project (Towards cyber-physical\\ncollaborative aircraft development), funded by the European Union Horizon\\n2020 research and innovation framework programme under grant\\nagreement n\\u25e6 815122 and in the COLOSSUS project (Collaborative\\nSystem of Systems Exploration of Aviation Products, Services and\\n13 https://smt.readthedocs.io/en/latest/_src_docs/examples/airfoil_\\nparameters/learning_airfoil_parameters.html\\n14 https://github.com/SMTorg/SMT\\n15 https://smt.readthedocs.io/en/latest/\",\n        \"Business Models) funded by the European Union Horizon Europe\\nresearch and innovation framework programme under grant agreement\\nn\\u25e6 101097120.\",\n        \"We also are grateful to E. Hall\\u00e9-Hannan from Polytechnique\\nMontr\\u00e9al for the hierarchical variables framework.\"\n      ]\n    },\n    {\n      \"title\": \"Appendix A. Toy test function\",\n      \"paragraphs\": [\n        \"This Appendix gives the detail of the toy function of Section 5.1.16\\nFirst, we recall the optimization problem:\\nmin  (cat , qnt )\\nw.r.t. cat = 1 \\u2208 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\",\n        \"qnt = 1 \\u2208 0, 1\\nThe toy function  is defined as\\n (, 1) =11=0 cos(3.6( \\u2212 2)) +  \\u2212 1\\n+11=1 2 cos(1.1 exp()) \\u2212  + 2\",\n        \"2\\n+11=2 cos(2) +\\n1\",\n        \"2\\n+11=3 (cos(3.4( \\u2212 1)) \\u2212  \\u2212 1 )\",\n        \"2\\n+11=4 \\u2212 \\n2\\n2\\n+11=5 2 cos(0.25 exp(\\u22124))2 \\u2212  + 1\",\n        \"2\\n+11=6  cos(3.4) \\u2212  + 1\",\n        \"2\\n+11=7 \\u2212 (cos(3.5) +  ) + 2\",\n        \"2\\n+11=8 \\u2212 \\n2\\n5\"\n      ]\n    },\n    {\n      \"title\": \"Appendix B. Hierarchical Goldstein test function\",\n      \"paragraphs\": [\n        \"This Appendix gives the detail of the hierarchical Goldstein problem\\nof Section 5.2.17 First, we recall the optimization problem:\\nmin  (cnaetu, nqenut, cat , qdenct)\\nThe hierarchical and mixed function  is defined as a hierarchical\\nfunction that depends on 0, 1, 2 and cont as describes in the\\nfollowing.\\n (1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2) =\",\n        \"11=00(1, 2, 1, 2, 3, 4, 5, 2)\\n+ 11=11(1, 2, 3, 2, 3, 4, 5, 2)\\n+ 11=22(1, 2, 4, 1, 3, 4, 5, 2)\\n+ 11=3cont(1, 2, 3, 4, 3, 4, 5, 2).\\n16 https://github.com/jbussemaker/SBArchOpt\\n17 https://github.com/jbussemaker/SBArchOpt\\nThen, the functions 0, 1 and 2 are defined as mixed variants of\\ncont as such\\n0(1, 2, 1, 2, 3, 4, 5, 2) =\\n1\\n1\\n1\\n2=0\",\n        \"(11=0cont(1, 2, 20, 20, 3, 4, 5, 2)\\n+ 11=1cont(1, 2, 50, 20, 3, 4, 5, 2)\\n+ 11=2cont(1, 2, 80, 20, 3, 4, 5, 2) )\\n2=1\",\n        \"(11=0cont(1, 2, 20, 50, 3, 4, 5, 2)\\n+ 11=1cont(1, 2, 50, 50, 3, 4, 5, 2)\\n+ 11=2cont(1, 2, 80, 50, 3, 4, 5, 2) )\\n2=2\",\n        \"(11=0cont(1, 2, 20, 80, 3, 4, 5, 2)\\n+ 11=1cont(1, 2, 50, 80, 3, 4, 5, 2)\\n+ 11=2cont(1, 2, 80, 80, 3, 4, 5, 2) )\\n1(1, 2, 3, 2, 3, 4, 5, 2) =\\n12=0cont(1, 2, 3, 20, 3, 4, 5, 2)\\n+ 12=1cont(1, 2, 3, 50, 3, 4, 5, 2)\\n+ 12=2cont(1, 2, 3, 80, 3, 4, 5, 2)\\n2(1, 2, 4, 1, 3, 4, 5, 2) =\\n11=0cont(1, 2, 20, 4, 3, 4, 5, 2)\\n+ 11=1cont(1, 50, 2, 4, 3, 4, 5, 2)\\n+ 11=2cont(1, 2, 80, 4, 3, 4, 5, 2)\\nTo finish with, the function cont is given by\\n(B.3)\\ncont(1, 2, 3, 4, 3, 4, 5, 2) = 53.3108 + 0.1849011\\n\\u2212 5.0291413.10\\u22126 + 7.7252213 .10\\u22128 \\u2212 0.08707752 \\u2212 0.1069593\\n+ 7.9877234 .10\\u22126 + 0.002424824 + 1.3285143.10\\u22126 \\u2212 0.0014639312\",\n        \"More at https://colab.research.google.com/github/SMTorg/smt/blob/\\nmaster/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\",\n        \"Supplementary material related to this article can be found online\\nat https://doi.org/10.1016/j.advengsoft.2023.103571.\\n1 Mader CA, Martins JRRA, Alonso JJ, van der Weide E. ADjoint: An approach\\nfor the rapid development of discrete adjoint solvers. AIAA J 2008;46:863\\u201373.\\n2 Kennedy M, O\\u2019Hagan A. Bayesian calibration of computer models. J R Stat Soc\",\n        \"Ser B Stat Methodol 2001;63:425\\u201364.\\n3 Hwang JT, Martins JRRA. A fast-prediction surrogate model for large datasets.\",\n        \"Aerosp Sci Technol 2018;75:74\\u201387.\\n4 Martins JRRA, Ning A. Engineering design optimization. Cambridge University\",\n        \"Press; 2021.\\n5 Bouhlel MA, Hwang JT, Bartoli N, Lafage R, Morlier J, Martins JRA.\",\n        \"A Python surrogate modeling framework with derivatives. Adv Eng Softw\\n2019;135:102662.\\n6 Bouhlel MA, Martins J. Gradient-enhanced kriging for high-dimensional\\nproblems. Eng Comput 2019;35:157\\u201373.\\n7 Pedregosa F, Varoquaux G, Gramfort A, Thirion VMB, Grisel O, et al. Scikit-learn:\\nMachine learning in Python. J Mach Learn Res 2011;12:2825\\u201330.\"\n      ]\n    }\n  ]\n}"
  ]
}