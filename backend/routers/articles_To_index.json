{
  "data": [
    "{\n  \"DocumentTitle\": \"The Programmer's Assistant: Conversational Interaction with a Large Language Model for Software Development\",\n  \"Auteurs\": \"Steven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller, Justin D. Weisz, code-fluent large language models, foundation models, conversa-, \",\n  \"Institutions\": \"IBM Argentina, IBM Research AI, IBM Research AI, tional interaction, \",\n  \"Abstract\": \"\\n        \",\n  \"Sections\": [\n    {\n      \"title\": \"CCS CONCEPTS\",\n      \"paragraphs\": [\n        \"\\u2022 Human-centered computing \\u2192 HCI theory, concepts and\\nmodels; \\u2022 Software and its engineering \\u2192 Designing software;\\n\\u2022 Computing methodologies \\u2192 Generative and developmental\\napproaches.\\n1\"\n      ]\n    },\n    {\n      \"title\": \"INTRODUCTION\",\n      \"paragraphs\": [\n        \"\\n        Software development is a highly skilled task that requires\\nknowledge, focus, and creativity \\n        \",\n        \"\\n        The pioneering work of Rich and Waters on The Programmer\\u2019s\\nApprentice \\n        \",\n        \"\\n        Recently, models leveraging the transformer architecture \\n        \",\n        \"Although autocompletion interfaces are useful and valuable\\nwhen the system can discern the developer\\u2019s intent, there are many\\ninstances where that is insuficient. For example, the developer\\nmay have a good idea of what they want to do, but may be unclear\\non what functions, libraries, or even algorithms to employ. They\\nmay even have general programming questions that need to be\\nanswered before they are able to write any code.\",\n        \"In this paper, we seek to understand whether modern\\ndevelopments in code-fluent foundation models \\u2013 large language models\\nthat have been fine-tuned on source code data \\u2013 are suficient to\\nsupport a conversational agent that can act as an assistant in the\\nsoftware development process. We developed the Programmer\\u2019s\\nAssistant to explore the capabilities that conversational interaction\\ncould enable and the extent to which users would find\\nconversational assistance with programming tasks desirable and useful.\",\n        \"\\n        We hypothesize that a conversational system may provide a\\nlfexible and natural means for interacting with a code-fluent LLM.\\nConversational interaction could enable users to pursue their\\nquestions in a multiple exchange dialog (as observed by Barke et al.\\n\\n        \",\n        \"Our paper makes the following contributions to the IUI\\ncommunity:\\n\\u2022 We provide empirical evidence that a conversational\\nprogramming assistant based on a state-of-the-art, code-fluent\\nfoundation model provides valuable assistance to software\\nengineers in a myriad of ways: by answering general\\nprogramming questions, by generating context-relevant code,\\nby enabling the model to exhibit emergent behaviors, and by\\nenabling users to ask follow-up questions that depend upon\\ntheir conversational and code contexts.\\n\\u2022 We show how diferent interaction models \\u2013 conversation,\\ndirect manipulation, and search \\u2013 provide complementary\\ntypes of support to software engineers with tradeofs\\nbetween the user\\u2019s focus and attention, the relevance of\\nsupport to their code context, the provenance of that support,\\nand their ability to ask follow-up questions.\\n\\u2022 We motivate the need to further understand how to design\\nhuman-centered AI systems that enhance the joint\\nperformance of the human-AI collaboration.\\n2\"\n      ]\n    },\n    {\n      \"title\": \"RELATED WORK\",\n      \"paragraphs\": [\n        \"\\n        We discuss three areas of related work that have either motivated\\nour study of conversational programming assistance or provided\\nthe technical foundations for it. We begin by briefly summarizing\\nRich and Waters\\u2019 visionary work on the Programmer\\u2019s\\nApprentice \\n        \"\n      ]\n    },\n    {\n      \"title\": \"The Programmer\\u2019s Apprentice\",\n      \"paragraphs\": [\n        \"\\n        Our work is inspired by the vision laid out by Rich and Waters \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Code-fluent Foundation Models and\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Human-Centered Evaluations of\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Programming Assistance\",\n      \"paragraphs\": [\n        \"\\n        Generative models based on the transformer architecture \\n        \",\n        \"\\n        New tools based on code-fluent LLMs are actively being\\ndeveloped. GitHub Copilot1 is described as \\u201cYour AI pair programmer.\\u201d It\\nis optimized for the code autocompletion use case: given a starting\\nsnippet such as a method\\u2019s documentation, signature, or partial\\nimplementation, Copilot completes the implementation. Copilot is\\nbased on the OpenAI Codex model \\n        \",\n        \"\\n          this model have shown that, although the quality of its outputs is\\nquite good, those outputs may still be problematic \\n          \",\n        \"\\n          A grounded theory analysis of how programmers interact with\\nCopilot \\n          \"\n      ]\n    },\n    {\n      \"title\": \"1https://github.com/features/copilot\",\n      \"paragraphs\": [\n        \"\\n          this model have shown that, although the quality of its outputs is\\nquite good, those outputs may still be problematic \\n          \",\n        \"\\n          A grounded theory analysis of how programmers interact with\\nCopilot \\n          \"\n      ]\n    },\n    {\n      \"title\": \"Conversational Interaction and Analysis\",\n      \"paragraphs\": [\n        \"\\n        2.3.1 Conversational Interaction. Using natural language to\\ninteract with technology has had a long research history \\n        \",\n        \"Task-oriented chatbots are typically built with frameworks such\\nas the Microsoft Bot Framework2, Google DialogFlow3, and IBM\\nWatson Assistant4. They operate using pre-defined dialogue trees\\nand use natural language processing to detect conversational intents\\nand extract contextual entities. This structure enables the creation of\\nspecial purpose, but fairly limited and rigid, conversational agents.\",\n        \"\\n        There have been several recent attempts to investigate\\nconversational programming assistance. Kuttal et al. \\n        \",\n        \"\\n          environments. PACT is structured as a discrete question-answering\\nsystem based on a neural machine translation approach, but it\\ndoesn\\u2019t maintain a conversational context.\\n2.3.2 Conversation Analysis. Conversation is a form of interaction\\nbetween people that enables robust communication. Conversation\\nAnalysis \\n          \"\n      ]\n    },\n    {\n      \"title\": \"2https://dev.botframework.com/ 3https://cloud.google.com/dialogflow 4https://www.ibm.com/products/watson-assistant/artificial-intelligence\",\n      \"paragraphs\": [\n        \"\\n          environments. PACT is structured as a discrete question-answering\\nsystem based on a neural machine translation approach, but it\\ndoesn\\u2019t maintain a conversational context.\\n2.3.2 Conversation Analysis. Conversation is a form of interaction\\nbetween people that enables robust communication. Conversation\\nAnalysis \\n          \"\n      ]\n    },\n    {\n      \"title\": \"THE PROGRAMMER\\u2019S ASSISTANT\",\n      \"paragraphs\": [\n        \"\\n        In order to explore conversational programming assistance, we\\ncreated a functional prototype system called The Programmer\\u2019s\\nAssistant. Our prototype, shown in Figure 1, combines a code editor\\nwith a chat interface. The code editor was implemented using the\\nMicrosoft Monaco Editor5 embedded in a React wrapper6. The\\nchat user interface was implemented using the React-Chatbot-Kit7\\nframework. To drive the conversational interaction, we employed\\nOpenAI\\u2019s Codex model \\n        \",\n        \"We developed our prototype as a lightweight coding\\nenvironment in order to examine the user experience of interacting with\\na conversational assistant. Our work was exploratory in nature,\\nand thus we did not have specific design goals for the prototype\\nbeyond integrating a code editor with a code-fluent LLM. We also\\ndid not attempt to target the prototype for a specific class of users\\n(e.g. novices or experts) or use cases (e.g. writing code vs. learning a\\nnew programming language), as we wanted any value provided by\\nconversational assistance to emerge from our user study. We also\\ndid not implement the ability to run or debug code in our prototype\\nas we wanted to explore the nature of the conversational\\ninteraction rather than having users focus extensively on the production\\nof working code.\",\n        \"\\n        When designing how users would interact with the\\nProgrammer\\u2019s Assistant, we decided that it should be available on demand\\nand not monitor the user\\u2019s work in progress or give unsolicited\\nsuggestions or advice, in keeping with the conversational agent\\ninteraction model proposed by Ross et al. \\n        \",\n        \"When a user interacts with the assistant, we keep track of their\\nselection state in the code editor. If a user sends a message to the\\nassistant without any code selected in the editor, then that message\\n(along with the prior conversational context) is passed directly to\\nthe model. If a user sends a message to the assistant with new code\",\n        \"selected in the editor (i.e. code that wasn\\u2019t previously selected when\\nthey sent their last message), then that code is appended to the\\nmessage before being communicated to the model.\",\n        \"The model may produce multiple types of responses to a user\\u2019s\\nmessage. We treat each type of response diferently in the UI.\\n\\u2022 Responses that do not contain code are always rendered in\\nthe chat UI (Figure 1E).\\n\\u2022 Responses containing short code snippets (\\u2264 10 lines) are\\nrendered inline in the chat UI (Figure 1G).\\n\\u2022 Responses containing longer code snippets (> 10 lines) show\\nthe code in a pop-up window (Figure 2A), with a proxy entry\\nin the chat transcript (Figure 2B) that allows users to\\nredisplay the code window after it has been closed. Non-code\\ntext in the response remains in the chat transcript.\",\n        \"The assistant never directly modifies the contents of the user\\u2019s\\nsource code; rather, any code the user desires to transfer from the\\nchat takes place via copy/paste.\",\n        \"Figure 1 shows a screenshot of a real, sample conversation, in\\nwhich the user asks a question that results in an inline response,\\nthen requests an explanation of some code in the editor, and then\\nrequests further elaboration. Figure 2 shows an example conversation\\nthat resulted in the generation of a longer code sample, shown in a\\npopup window. This example shows how the assistant produced an\\nincomplete solution, followed by criticism from the user regarding\\nthe missing code, and resulting in an apology and the generation\\nof a complete solution.\\n3.1\"\n      ]\n    },\n    {\n      \"title\": \"5https://microsoft.github.io/monaco-editor/ 6https://www.npmjs.com/package/@monaco-editor/react 7https://fredrikoseberg.github.io/react-chatbot-kit-docs/\",\n      \"paragraphs\": [\n        \"selected in the editor (i.e. code that wasn\\u2019t previously selected when\\nthey sent their last message), then that code is appended to the\\nmessage before being communicated to the model.\",\n        \"The model may produce multiple types of responses to a user\\u2019s\\nmessage. We treat each type of response diferently in the UI.\\n\\u2022 Responses that do not contain code are always rendered in\\nthe chat UI (Figure 1E).\\n\\u2022 Responses containing short code snippets (\\u2264 10 lines) are\\nrendered inline in the chat UI (Figure 1G).\\n\\u2022 Responses containing longer code snippets (> 10 lines) show\\nthe code in a pop-up window (Figure 2A), with a proxy entry\\nin the chat transcript (Figure 2B) that allows users to\\nredisplay the code window after it has been closed. Non-code\\ntext in the response remains in the chat transcript.\",\n        \"The assistant never directly modifies the contents of the user\\u2019s\\nsource code; rather, any code the user desires to transfer from the\\nchat takes place via copy/paste.\",\n        \"Figure 1 shows a screenshot of a real, sample conversation, in\\nwhich the user asks a question that results in an inline response,\\nthen requests an explanation of some code in the editor, and then\\nrequests further elaboration. Figure 2 shows an example conversation\\nthat resulted in the generation of a longer code sample, shown in a\\npopup window. This example shows how the assistant produced an\\nincomplete solution, followed by criticism from the user regarding\\nthe missing code, and resulting in an apology and the generation\\nof a complete solution.\\n3.1\"\n      ]\n    },\n    {\n      \"title\": \"Supporting Conversational Interaction\",\n      \"paragraphs\": [\n        \"\\n        We enabled Codex to conduct a conversational interaction by\\nprompting it with a conversational transcript and a request to produce\\nthe next conversational turn. The prompt establishes a pattern of\\nconversation between a user and a programming assistant named\\nSocrates. It provides several examples of Socrates responding to\\ngeneral coding questions, generating code in response to a request, and\\naccepting code as input. It establishes a convention for delimiting\\ncode in the conversation, making it easy to parse for display in the\\nUI. It also establishes an interaction style for the assistant, directing\\nit to be polite, eager, helpful, and humble, and to present its\\nresponses in a non-authoritative manner8. Because of the possibility\\nthat the model might produce erroneous answers or incorrect code\\n(as discussed in Weisz et al. \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Architecture & UI Design\",\n      \"paragraphs\": [\n        \"The Programmer\\u2019s Assistant communicates with the Codex API via\\na proxy server that forwards requests from the React client. The\\nproxy also rate-limits access to conform to the API\\u2019s policy, and\\nit logs UI events from the client (e.g. requests, responses, and UI\\n8The assistant\\u2019s use of non-authoritative responses was encoded into the LLM prompt;\\noutput token probabilities from the LLM were not utilized to influence the assistant\\u2019s\\nresponse.\\ninteractions) in a back-end database. To address inconsistencies\\nin the style or formatting of code generated by Codex, the proxy\\nserver reformats all code segments using the Black code formatter\\n9 before transmitting them to the client UI.\",\n        \"The client maintains the transcript of the ongoing conversation.\\nEach time the user sends a message in the chat, the client constructs\\na new prompt for the model by concatenating the initial prompt,\\nthe chat transcript, and the user\\u2019s new utterance, and makes a\\nrequest for the model to complete the transcript. This completion\\nrequest also specifies a stop sequence of tokens to prevent the\\nmodel from generating both sides of the conversation (e.g. what\\nthe model thinks the user\\u2019s next utterance might be after the\\nassistant\\u2019s response). Given the API\\u2019s limitation on context length\\n(4,096 tokens for both the prompt and model response), we silently\\n\\u201cforget\\u201d older exchanges in the chat transcript when constructing\\nthe prompt to ensure that our completion request remains within\\nbounds. Nonetheless, the entire conversational history remains\\nvisible to the user in the UI.\",\n        \"The client UI provides a loose coupling between the source code\\neditor and the chat interface. Users can hide the chat pane when\\nthey wish to focus solely on their code, and re-engage with it\\nwhen they desire assistance. Code selected in the editor is included\\nin the conversation in order to couple the code context with the\\nconversation. Easily-accessible buttons are provided in the UI to\\ncopy code responses from the assistant to the clipboard.\\n3.3\"\n      ]\n    },\n    {\n      \"title\": \"Handling Model Limitations\",\n      \"paragraphs\": [\n        \"While developing the Programmer\\u2019s Assistant, and in early pilot\\ntesting, we experienced some quirks and shortcomings of the model\\nand our approach to using it for conversational interaction. One\\nlimitation stemmed from the fact that the model sometimes\\nproduced incorrect responses (e.g. code with syntax errors), incomplete\\nresponses (e.g. code that was missing functionality), irrelevant\\nresponses (e.g. responses not related to the user\\u2019s question), or\\ninsubstantial responses (e.g. \\u201cI don\\u2019t know\\u201d). Because of the probabilistic\\nnature of model inference, re-prompting the model would\\nsometimes produce a more correct or appropriate response. Thus, we\\nadded the ability for users to \\u201ctry again,\\u201d either by asking in the chat\\nor by clicking a button in the UI (Figure 1C). This feature removes\\nthe assistant\\u2019s last response from the context presented to the model\\nand then re-invokes the model with an increased temperature10.\",\n        \"Although it is possible for transformer models such as Codex to\\nproduce multiple possible responses to a single prompt, we only\\nrequest a single response in order to speed up response time as\\nwell as to preserve the token budget for conversational context.\\nThus, the \\u201ctry again\\u201d feature provides an alternate way to produce\\na wider variety of responses.\",\n        \"During pilot testing, we noticed that the assistant sometimes\\nhappened to generate the same response to multiple, unrelated\\nrequests. In these cases, the assistant tended to get \\u201cstuck\\u201d in a pattern\\nof repeating the same response and was unable to resume normal\\nconversation. To avoid this problem, we automatically execute a\",\n        \"10Temperature is a parameter in a generative model that specifies the amount of\\nvariation in the generation process. Higher temperatures result in greater variability\\nin the model\\u2019s output.\\nA\\nB\",\n        \"C\",\n        \"D\",\n        \"E\\nF\\nG\\n\\u201ctry again\\u201d operation in the background when we see identical\\nconsecutive responses from the assistant.\",\n        \"Finally, we noticed that the accumulation of conversational\\ncontext sometimes resulted in the assistant becoming fixated on some\\nportion of the earlier conversation. For example, it might respond\\nto a question with portions of the prompt or of earlier conversation,\\nand become less responsive to newer requests. To address this issue,\\nwe introduced a \\u201cstart over\\u201d feature, accessible via the chat or by\\nclicking a button in the UI (Figure 1D), that resets the context to the\\noriginal prompt, forgetting the rest of the conversational history.\\nWe preserve the chat transcript in the UI, but delineate the break in\\nthe assistant\\u2019s memory with an annotation in the chat transcript.\\nThese annotations are added both for \\u201ctry again\\u201d and \\u201cstart over.\\u201d\"\n      ]\n    },\n    {\n      \"title\": \"9https://black.readthedocs.io/en/stable/\",\n      \"paragraphs\": [\n        \"10Temperature is a parameter in a generative model that specifies the amount of\\nvariation in the generation process. Higher temperatures result in greater variability\\nin the model\\u2019s output.\\nA\\nB\",\n        \"C\",\n        \"D\",\n        \"E\\nF\\nG\\n\\u201ctry again\\u201d operation in the background when we see identical\\nconsecutive responses from the assistant.\",\n        \"Finally, we noticed that the accumulation of conversational\\ncontext sometimes resulted in the assistant becoming fixated on some\\nportion of the earlier conversation. For example, it might respond\\nto a question with portions of the prompt or of earlier conversation,\\nand become less responsive to newer requests. To address this issue,\\nwe introduced a \\u201cstart over\\u201d feature, accessible via the chat or by\\nclicking a button in the UI (Figure 1D), that resets the context to the\\noriginal prompt, forgetting the rest of the conversational history.\\nWe preserve the chat transcript in the UI, but delineate the break in\\nthe assistant\\u2019s memory with an annotation in the chat transcript.\\nThese annotations are added both for \\u201ctry again\\u201d and \\u201cstart over.\\u201d\"\n      ]\n    },\n    {\n      \"title\": \"Sample Conversation\",\n      \"paragraphs\": [\n        \"We provide a real sample conversation with the Programmer\\u2019s\\nAssistant in Listing 1. This conversation begins with the assistant\\ngreeting the user (line 1). Next, the user asks a general Python\\nprogramming question (line 4), to which the assistant responds with\\na non-authoritative remark (\\u201cI think...\\u201d) and a code snippet (line\\n9). The user next asks a follow-up question that depends on their\\nprevious question and the assistant\\u2019s response (line 11), to which\\nthe assistant provides another code snippet (line 15), satisfying the\\nuser\\u2019s request.\",\n        \"The user then switches topics and asks the assistant to write\\na Fibonacci function (line 17), and the assistant again responds\\nwith a non-authoritative remark (\\u201cI will give it a try,\\u201d line 20) and\\na block of code. The user then asks how the function works (line\\n30) and the assistant provides an adequate description (line 32).\\nNext, the user asks the assistant to re-implement the function in a\\ndiferent way (line 37), again leveraging the ability to ask follow-up\\nquestions. The assistant produces an alternative implementation\\nthat conforms to the user\\u2019s request (line 41). The user follows up\\nwith a question that depends on multiple past utterances and\\nresponses in the chat transcript (line 47), and the assistant produces\\na relevant response (line 49). The conversation closes with the user\\nthanking the assistant (line 53) and the assistant acknowledging\\ntheir gratitude (line 55).\",\n        \"Code presented by the assistant is listed in bold face.\\n1 Socrates : Hello ! My name is Socrates your programming\\n2 assistant . What can I help you with today ?\\n3\\n4 User : How do I get a list of the keys in a\\n5 Python dictionary ?\\n6\\n7 Socrates : I think you would do something like this :\\n8\\n9 list(my_dict.keys())\\n10\\n11 User : What if I just wanted to iterate over them ?\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n4\",\n        \"Socrates : I think so . How about this ?\\ndef fib(n):\\na,b = 0,1\\nfor i in range(n):\",\n        \"a,b = b,a+b\\nreturn\\nUser :\",\n        \"Which version is better ?\\nSocrates : I think the recursive version is more\\nelegant , but the iterative version is more\\nefficient .\",\n        \"User :\",\n        \"Thanks !\\nSocrates : You ' re welcome !\"\n      ]\n    },\n    {\n      \"title\": \"Listing 1: A conversation with the Programmer\\u2019s Assistant.\",\n      \"paragraphs\": [\n        \"Code presented by the assistant is listed in bold face.\\n1 Socrates : Hello ! My name is Socrates your programming\\n2 assistant . What can I help you with today ?\\n3\\n4 User : How do I get a list of the keys in a\\n5 Python dictionary ?\\n6\\n7 Socrates : I think you would do something like this :\\n8\\n9 list(my_dict.keys())\\n10\\n11 User : What if I just wanted to iterate over them ?\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n4\",\n        \"Socrates : I think so . How about this ?\\ndef fib(n):\\na,b = 0,1\\nfor i in range(n):\",\n        \"a,b = b,a+b\\nreturn\\nUser :\",\n        \"Which version is better ?\\nSocrates : I think the recursive version is more\\nelegant , but the iterative version is more\\nefficient .\",\n        \"User :\",\n        \"Thanks !\\nSocrates : You ' re welcome !\"\n      ]\n    },\n    {\n      \"title\": \"EMPIRICAL STUDY OF CONVERSATIONAL\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"PROGRAMMING ASSISTANCE\",\n      \"paragraphs\": [\n        \"\\n        We conducted an empirical user study of the Programmer\\u2019s\\nAssistant to assess whether conversational assistance provides value in\\na software engineering context11. Our inquiry focused on the user\\nexperience and value of conversational interactions grounded in\\ncode. We therefore designed a qualitative study to investigate\\nattitudes toward a conversational programming assistant: do people\\nenjoy interacting conversationally, what kinds of questions do they\\nask, and how does the experience compare to other forms of\\nprogramming support such as searching the web? We note that prior\\nstudies (e.g. \\n        \",\n        \"To address our questions, we deployed the Programmer\\u2019s\\nAssistant within our organization \\u2013 a global technology company \\u2013 and\\ninvited people to try it out and give us feedback on their experience.\\nWe invited people with varying levels of programming skill in order\\nto obtain a wide range of feedback on the kinds of use cases for\\nwhich the tool could provide assistance.\\n4.1\"\n      ]\n    },\n    {\n      \"title\": \"Tasks\",\n      \"paragraphs\": [\n        \"\\n        We set up the Programmer\\u2019s Assistant as a playground environment\\nthat participants could try out with a few sample programming\\nproblems. We created a tutorial to orient participants to the\\nassistant, its capabilities, and how to interact with it. We also created\\nfour programming challenges focused on writing code,\\ndocumenting code, and writing tests for code. We designed these challenges\\nto expose participants to a broad range of the assistant\\u2019s capabilities.\\nFor each of these challenges, we explicitly did not evaluate metrics\\nsuch as the participant\\u2019s productivity, the quality of their solutions,\\nor the time taken to produce them, as the focus of our study was to\\nunderstand the utility of conversational interaction. We selected\\nPython as the language used for the tutorial and challenges because\\nof its general popularity \\n        \",\n        \"After participants completed their solution to a challenge, they\\nsubmitted it by clicking a button in the UI. The code editor used\\nin the Programmer\\u2019s Assistant was not a fully-functional IDE and\\ndid not provide syntax checking or the ability to run, test, or debug\\ncode. Due to these limitations, participants were asked to submit\\ntheir solutions when they felt they had completed the challenge to\\ntheir own satisfaction.\\n4.2\"\n      ]\n    },\n    {\n      \"title\": \"Participants\",\n      \"paragraphs\": [\n        \"To recruit participants for our study, we posted internal\\nadvertisements in various communications channels focused on software\\nengineering. Our advertisements stated that we were evaluating a\\nconversational programming assistant, but were kept deliberately\\nvague in order to minimize the impact on peoples\\u2019 expectations of\\nthe experience.\",\n        \"\\n        Our advertisement yielded a pool of 140 potential participants.\\nIn order to recruit a diverse sample, we used a screening survey\\nthat asked about their job role, their familiarity with and recency\\nof use of Python, and their availability to participate in our study.\\nWe accepted participants into the study on a rolling basis, selecting\\nparticipants to capture a range of programming experiences and\\nensure balanced gender representation. We conducted periodic\\nreviews to determine whether we were learning something new from\\neach participant or if we had reached the point of saturation \\n        \",\n        \"Our participants had the following self-identified characteristics:\\n\\u2022 Job role: 19 Software Engineers, 12 Researcher/Scientists, 3\\nSoftware Architects, 2 Data Scientists, 1 Machine Learning\\nEngineer, 1 Systems Test Engineer, 1 Business Analyst, 1\\nManager, 1 Marketer, and 1 Consultant.\\n\\u2022 Gender: 21 Female, 19 Male, 1 Gender Variant /\\nNon-conforming, and 1 Preferred not to say.\\n\\u2022 Python Experience: 17 participants had 3+ years of Python\\nexperience, 11 had 1-3 years, 11 had less than 1 year, and 3\\nwere not familiar with Python.\\n\\u2022 Recency of Python Use: 29 participants had written Python\\ncode within the past month, 4 within the past year, 5 within\\nthe past 5 years, and 4 had not written Python code within\\nthe past 5 years.\",\n        \"We provide full demographic information for individual\\nparticipants in Appendix E.\\n4.3\"\n      ]\n    },\n    {\n      \"title\": \"Procedure\",\n      \"paragraphs\": [\n        \"Participants completed the study on their own time, independently\\nand without moderation. Each participant was provided with a web\\nlink to a pre-study survey that described the nature of the study and\\nthe tasks that they would be expected to perform. They were then\\ndirected to the Programmer\\u2019s Assistant to complete the tutorial and\\nthe four programming challenges. When participants indicated they\\nwere finished with the challenges 12, they were directed to a final\\npost-study survey. Complete sessions generally required about an\\nhour of efort, though some participants spread their efort across\\na longer period of time and across multiple sessions. Participants\\nwere compensated for their time at a rate equivalent to US $15/hr.\\n4.4\"\n      ]\n    },\n    {\n      \"title\": \"Measures\",\n      \"paragraphs\": [\n        \"We collected a variety of data in our study from three sources:\\n(1) Surveys. We employed three surveys in the study: a\\nprestudy survey to collect demographic information, a pre-task\\nsurvey to gauge expectations of the conversational user\\nexperience, and a post-task survey to assess actual user\\nexperience. We describe these survey questions in the relevant\\ncontext of our results, and we provide a complete listing of\\nall survey instruments in Appendix A.\\n(2) Event logs. The Programmer\\u2019s Assistant was instrumented\\nto collect data on participants\\u2019 usage. The event logs\\nprovided timestamped records of interaction events, including\\nconversational exchanges, hiding/showing the assistant, use\\nof the \\u201ctry again\\u201d and \\u201cstart over\\u201d features, and use of\\ncopy/paste.\\n(3) Conversation logs. From the event logs, we extracted\\nconversational transcripts between each participant and the\\nProgrammer\\u2019s Assistant.\\n5\\n5.1\"\n      ]\n    },\n    {\n      \"title\": \"RESULTS\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Data & Analysis\",\n      \"paragraphs\": [\n        \"We collected a wealth of data in our study: 126 survey responses\\nfrom three surveys per participant, containing 296 written\\ncomments in open-ended survey questions, and 4,877 instances of 23\\ndiferent types of UI events, including 1,699 conversational\\nexchanges13 in the event logs. We also compute, for each participant,\\ncounts or durations for 21 diferent metrics from the event logs.\",\n        \"In our analysis, we deliberately exclude the portion of our data\\ncollected during the tutorial exercise. We exclude this data because\\nthat activity was guided by the tutorial instructions, not by our\\nparticipants\\u2019 own initiative. Thus, our final sample consists of 3,172\\n12We did not enforce that participants actually complete all of the challenges.\\nNevertheless, all participants but one did submit solutions to all of the challenges.\\n13We refer to a participant\\u2019s utterance, followed by the assistant\\u2019s response, as a\\nconversational exchange.\\nevents, including 968 conversational exchanges in the event logs;\\nno survey data was excluded.\",\n        \"\\n        Our primary analysis of this data is qualitative, as our\\nparticipants provided us with a rich source of interesting feedback and\\nthought-provoking insights in their comments. Where applicable,\\nwe supplement this data with quantitative data from the survey\\nand the event logs, as well as chat transcript data from the\\nconversation logs. In this way, we triangulate \\n        \",\n        \"\\n        In order to set the context for our analysis, we first describe\\nhow we used reflexive thematic analysis to analyze participants\\u2019\\nresponses to the open-ended survey questions. We then describe\\nour analysis of the conversation logs and our development of a\\ncoding guide based on Conversation Analysis \\n        \",\n        \"Appears Fixated, Claims Ignorance, Didn\\u2019t Understand, Grants Request (Complete), Grants Request\\n(Incomplete), Ofers Help, Provided Wrong Answer, Requests Details, Requests Paraphrase, Response\\nIncludes Code, Spews Garbage\\ncodes to identify meta-information such as utterances that included\\ncode, utterances that referenced selected code, utterances that\\nimplicitly or explicitly referenced earlier portions of the conversation,\\nor non-verbal UI activities such as copies, pastes, and invocations\\nof \\u201ctry again\\u201d and \\u201cstart over.\\u201d Finally, we classified a subset of the\\nhuman-applied codes based on whether they represented a\\nparticipant\\u2019s task or social orientation toward the assistant. We list\\nour codes in Table 1, but note that not all of them ended up being\\nrelevant to our analysis.\",\n        \"When coding conversational data, we applied individual codes\\nat the level of each conversational utterance. We allowed multiple\\ncodes to be applied to each utterance to account for utterances that\\nperformed multiple functions (e.g. greeting and self-identification).\",\n        \"In order to ensure consistency in how our codebook was applied,\\ntwo authors coded a 10% sample of the 968 conversational\\nexchanges, achieving a satisfactory level of inter-rater reliability\\n(Krippendorf\\u2019s  = 0.77, where agreement was conservatively defined\\nas having all of the same codes applied to both utterances in a\\nconversational exchange).\\n5.2\"\n      ]\n    },\n    {\n      \"title\": \"Interlocutor\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Orientation\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Codes\",\n      \"paragraphs\": [\n        \"Appears Fixated, Claims Ignorance, Didn\\u2019t Understand, Grants Request (Complete), Grants Request\\n(Incomplete), Ofers Help, Provided Wrong Answer, Requests Details, Requests Paraphrase, Response\\nIncludes Code, Spews Garbage\\ncodes to identify meta-information such as utterances that included\\ncode, utterances that referenced selected code, utterances that\\nimplicitly or explicitly referenced earlier portions of the conversation,\\nor non-verbal UI activities such as copies, pastes, and invocations\\nof \\u201ctry again\\u201d and \\u201cstart over.\\u201d Finally, we classified a subset of the\\nhuman-applied codes based on whether they represented a\\nparticipant\\u2019s task or social orientation toward the assistant. We list\\nour codes in Table 1, but note that not all of them ended up being\\nrelevant to our analysis.\",\n        \"When coding conversational data, we applied individual codes\\nat the level of each conversational utterance. We allowed multiple\\ncodes to be applied to each utterance to account for utterances that\\nperformed multiple functions (e.g. greeting and self-identification).\",\n        \"In order to ensure consistency in how our codebook was applied,\\ntwo authors coded a 10% sample of the 968 conversational\\nexchanges, achieving a satisfactory level of inter-rater reliability\\n(Krippendorf\\u2019s  = 0.77, where agreement was conservatively defined\\nas having all of the same codes applied to both utterances in a\\nconversational exchange).\\n5.2\"\n      ]\n    },\n    {\n      \"title\": \"Expectations and Experience\",\n      \"paragraphs\": [\n        \"Pilot testing of the Programmer\\u2019s Assistant suggested that software\\nengineers would be skeptical of a conversational programming\\nassistant and its ability to provide useful assistance. Our study\\nrevealed that, for most participants, their actual experience after\\nusing the tool was better than they had anticipated. Participants\\nwere surprised at the quality of the assistant\\u2019s responses and they\\nappreciated how its integration with the code editor reduced the\\namount of context switching they needed to do in the UI. Some\\nparticipants struggled with the code selection feature, although\\nothers appreciated the ability to ask questions related to selected\\ncode.\\n5.2.1 Usage. All of our participants engaged with the\\nProgrammer\\u2019s Assistant while working on the challenges, despite there\\nbeing no requirement to do so. Forty-one participants submitted\\nsolutions to all four challenges, and one participant, P14, only\\nsubmitted solutions for one of the four challenges. Participants spent\\nan average of 68 minutes engaged with the assistant, as measured\\nby the amount of time the Programmer\\u2019s Assistant window was in\\nfocus.\",\n        \"Participants made an average of 23.0 utterances (SD = 15.1\\nutterances) to the assistant. On average, 6.2 of their utterances (SD =\\n4.3 utterances) contained a code selection. The average latency per\\nrequest14 was 6.7 seconds (SD = 3.1 seconds).\",\n        \"\\n        We saw a 66.3% rate of acceptance of generated code, where we\\nconsidered code to be accepted if the participant performed a copy\\nimmediately after the code was generated. This acceptance rate is\\nmuch higher than the 27% acceptance rate reported for Copilot \\n        \",\n        \"We developed a short inventory of six scale items to measure user\\nexperience of code work15. The scale was administered twice: once\\nbefore participants were exposed to the Programmer\\u2019s Assistant\\n(but after they had been briefed that they would interact with an AI\\nchatbot), and once after completing the programming challenges.\\nThe items were presented with the appropriate tense: Do you expect\\n(Did you find that) the Programmer\\u2019s Assistant: (a) will be (was)\\neasy to use; (b) will understand (understood) your requests; (c) will\\nprovide (provided) high quality responses; (d) will help (helped)\\nyou to write better code; (e) will help (helped) you to write code\\nmore quickly; (f) will be (was) enjoyable to use. Each item was rated\\non a 4-point scale of extent: Not at all (1), A little (2), Somewhat (3),\\nA great deal (4).\",\n        \"A factor analysis revealed the items on this scale measured a\\nsingle construct, which we identify as user experience (Cronbach\\u2019s\\n = 0.87). Thus, we computed two scores of user experience (UX)\\nfor each participant: a pre-task UX score computed as the average\\nof their six pre-task expectation scale responses, and a post-task\\nUX score computed as the average of their six post-task experience\\nscale responses.\",\n        \"We found that participants had lower initial expectations for\\ntheir experience with a conversational programming assistant\\n(pretask UX M (SD) = 3.0 (0.62) of 4) than their experience actually\\nwas (post-task UX M (SD) = 3.6 (0.32) of 4). A paired sample t-test\\nshows that this diference was significant,  (41) = 5.94,  < .001,\\nCohen\\u2019s  = 0.92 (large). Measured another way, 32 participants\\n(76.2%) had post-task UX ratings that were higher than their\\npretask expectations, demonstrating a significant shift in attitudes\\ntoward conversational programming assistance.\",\n        \"However, the UX ratings alone fail to capture participants\\u2019\\nnuanced expectations of the assistant and the reasons for their shifted\\nattitudes after using it. Participants expressed a variety of\\nexpectations of the assistant before using it, including that it would be easy\\nto use (P30) and produce correct responses (P30), understand the\\nproblem and what is being asked of it (P8, P9, P11), not interfere\\nwith their flow state (P5), produce imperfect or questionable\\noutputs (P6, P21), improve with feedback (P31), provide generic and\\nunhelpful answers (P17) or only answer basic questions (P40), and\\nproduce responses quickly (P40).\",\n        \"P17 expected \\u201cto be frustrated very quickly and that what I\\u2019d think\\nwould be relatively common questions would be responded to with\\ngeneric, unhelpful answers.\\u201d P6 explained, \\u201cI didn\\u2019t have very good\\nexperiences with chatbots. I think I\\u2019ll need to spend more time in\\nreviewing and fixing the suggestions than in writing the code myself\\nfrom scratch.\\u201d P11 had a more balanced view, that \\u201cIt\\u2019ll do some tasks\\nreally well, but others will not be as reliable.\\u201d\",\n        \"After interacting with the Programmer\\u2019s Assistant, many\\nparticipants commented on how the experience was better than they\\nanticipated, because it \\u201cseemed to be able to handle complex issues\\u201d\\n(P10) and \\u201cwas a great help\\u201d (P8). P20 felt it was \\u201cincredible!\\u201d P6 and\\nP17, who were both initially skeptical, reported having a positive\\nexperience. For P6, \\u201cIt absolutely exceeded all my expectations, in\\n15Our scale items were modeled from scales published in Weisz et al. 103, Table 9 \\u2013\\nAI Support that measured constructs including ease of use (item 3), response quality\\n(item 1), the production of higher-quality code (item 5), and the ability to write code\\nmore rapidly (item 4). We added additional items to cover the constructs of request\\nunderstanding and enjoyment, and we cast all items on a 4-point scale of extent.\\nall aspects that I could have imagined and more!\\u201d P17 provided a\\nmore quantitative assessment: \\u201cInitial expectations: 3 Actual: 9.5.\\u201d\\nP38 was emphatic in their evaluation: \\u201cI was blown away how well\\nit allowing me to structure how I want the code to look and work and\\njust giving me the thing I asked for.\\u201d\",\n        \"Many participants described a sense of surprise in their\\nexperiences. P9 was surprised by how well it understood their requests:\\n\\u201cI was surprised at how well the Programmer Assistant\\nwas able to understand my requests and generate good\\ncode/documentation/tests. It understood major concepts\\nand was able to explain it to me in a clear way, and it\\nwas also able to understand and write functional code.\",\n        \"It even was able to help me review my answer. I was also\\nsurprised at how well it could understand the context\\nof what I was asking in follow-up questions when I did\\nnot specify exactly what I was talking about, but rather\\nreferencing our prior conversation (such as, \\u2018what does\\nthat mean\\u2019).\\u201d (P9)\",\n        \"Similarly, P6 was surprised that they liked the conversational\\ninteraction when they expected that they wouldn\\u2019t:\\n\\u201cI thought I wouldn\\u2019t like the chatbot interaction and\\nthat I would prefer something like the tool I\\u2019ve seen in\\nthose demos of Copilot. But surprisingly, after using\\nthe chatbot (and seeing the results: easy to use, it\\nunderstands well, I felt it like a partner) I like this kind of\\nhelp.\\u201d (P6)\\n5.2.3 Quality of Assistant\\u2019s Responses. In order to gauge the quality\\nof responses produced by the Programmer\\u2019s Assistant, we examined\\nthe 910 task-oriented requests made by participants in the study.\\nFor the vast majority (80.2%), the assistant produced a correct\\nresponse (Grants Request (Complete)); in other cases, the assistant\\u2019s\\nresponse was incorrect (9.6%; Provided Wrong Answer), correct but\\nincomplete (4.4%; Grants Request (Incomplete)), or the assistant\\ndidn\\u2019t understand (3.4%; Didn\\u2019t Understand), claimed ignorance of\\nthe subject (1.5%; Claims Ignorance), or produced another type of\\nresponse (0.9%; Appears Fixated, Spews Garbage).\",\n        \"Participants also reported experiencing this variability in the\\nquality of the assistant\\u2019s responses. Some participants described\\nhow the assistant provided \\u201cdetailed answers\\u201d (P17) and \\u201chigh\\nquality outputs\\u201d (P18) that were \\u201csurprisingly good\\u201d (P2). P6 felt it was\\n\\u201cincredible to see the quality of the responses,\\u201d and P3 even explored\\nthe assistant\\u2019s capabilities outside the scope of the challenges and\\nfound that it could handle those as well:\\n\\u201cIt was surprising the quality of the code and the ability\\nto answer all my questions correctly. Although I think\\nthe challenges may be biased towards what the Assistant\\nis able to do, it was a great experience because I asked\\nmany other things and it was able to answer correctly.\\u201d\\n(P3)\",\n        \"Of course, the Programmer\\u2019s Assistant wasn\\u2019t perfect, and some\\nparticipants did run into issues. For P35, \\u201cThe documentation\\ngeneration did not perform very well.\\u201d P16 questioned the accuracy of the\\nknowledge encoded in the model: \\u201cDoes the model need to be\\nupdated? It said latest python version is 3.7 but google says it\\u2019s 3.10.\\u201d In\\nsome instances, participants needed to ask their question multiple\\ntimes to get a good response: \\u201cyou need to ask many times if you\\nwant to get an answer and also a detailed explanation\\u201d (P3). P27 felt,\\n\\u201cit was annoying when I asked it to try again and it would give me\\nthe same response.\\u201d P22 struggled because, \\u201cIt didn\\u2019t seem to handle\\nmultiple sentences well.\\u201d\",\n        \"P28 perhaps ofered the most scathing criticism, that, \\u201cIt makes\\nmistakes often enough to be not very practical.\\u201d However, despite\\nthe production of poorer-quality responses, other participants felt\\nthat the assistant was still helpful. P36 reported that, \\u201cOnly minor\\ntweaks were normally needed to correct any issues.\\u201d Similarly, P38\\ndescribed how the assistant wasn\\u2019t able to completely solve their\\nproblem, but provided a useful start:\\n\\u201cThere was only one hickup I noticed where when I\\nasked it to memoize fibonacci it couldn\\u2019t, but it dropped\\nthe building blocks on my lap for me to finish so that\\nwas fine, that was like minutes of efort on my part.\\u201d\\n(P38)\\n5.2.4 UI Design & Afordances. Participants made many comments\\non our specific UI design and the afordances provided (or not\\nprovided) in our chat-augmented editor. Overall, the integration\\nbetween the chat pane and the code editor was \\u201cvery good\\u201d (P23),\\nwith a \\u201cnice interface between the code pane and the assistant pane\\u201d\\n(P17) that \\u201cmakes it really convenient\\u201d (P35).\",\n        \"\\n        Prior research by Brandt et al. \\n        \",\n        \"Some aspects of our user interface were confusing to participants,\\nsuch as the mechanism for selecting code to be included in the\\nconversational context. P7 remarked, \\u201cIt\\u2019s was a little confusing\\ndoing the selection part for it to tell me what a function does, but...\\nit gave me code that was insanely easy to copy and paste.\\u201d Other\\nparticipants appreciated the code selection mechanism, such as P11:\\n\\u201cI enjoyed the code selection feature, and found that very easy to use.\\u201d\\nIn the event logs, we identified 20 instances in which a participant\\nunintentionally included selected code in the conversation when\\nit wasn\\u2019t needed (Includes Extraneous Selection), 12 instances in\\nwhich a code selection was omitted when it was needed to provide\\ncontext for the question (Missing Selection), and 16 instances in\\nwhich a participant copy/pasted code directly into the chat rather\\nthan selecting it in the editor (Pasted Code in Chat). Although\\nthese cases represent a small fraction of the 227 instances in which\\na code selection was required and included in the conversation\\n(Includes Selection), their presence does indicate that more attention\\nis needed to the interaction design of code selection.\",\n        \"Another issue regarded the awareness of the \\u201ctry again\\u201d and\\n\\u201cstart over\\u201d features. The \\u201ctry again\\u201d feature was only used by 14\\nparticipants, who used it a total of 63 times over the course of\\nthe study. Some participants used it specifically when they got an\\nanswer which they saw as clearly wrong, while others used it to\\nget a variety of possible answers before proceeding. The \\u201cstart over\\u201d\\nfeature was used even less, by 5 participants who used it a total of\\n6 times. Despite our efort to surface these conversational features\\nin the UI via shortcut buttons, they may not have been suficiently\\nnoticeable or salient: \\u201cThe \\u2018try again\\u2019 button is not so reachable, often\\ntimes I forgot it exists\\u201d (P23). By contrast, at least one participant\\nwas successful with these features:\\n\\u201cat some point it had issue with challenge 3 and I had to\\nstart over. Just asking \\u2018try again\\u2019 was not enough and I\\nwas getting always the same (wrong and not related)\\nanswer. starting again solved the issue!\\u201d (P20)\\n5.3\"\n      ]\n    },\n    {\n      \"title\": \"Utility of Conversational Assistance\",\n      \"paragraphs\": [\n        \"\\n        Our next set of themes concerns the utility provided by\\nconversational programming assistance. Participants felt the assistant was\\nhighly valuable and desired to use it in their own work. They felt it\\nwould be most helpful for smaller or narrowly-scoped tasks, but\\nable to provide a wide variety of types of assistance. The fact that\\nthe interaction model was conversational and grounded in code\\nwere valuable aspects, as was the ability for the assistant to bolster\\nusers\\u2019 learning about programming topics through that\\ninteraction. Participants did question whether they could trust and rely\\nupon the assistant\\u2019s responses, echoing a similar theme discussed\\nin Weisz et al. \\n        \",\n        \"Participants described a wide variety of tasks for which they\\nfelt the assistant would be useful. These tasks included \\u201cordinary\\u201d\\n(P23), \\u201csimpler\\u201d (P2), and \\u201csmall, repetitive\\u201d (P4) tasks such as \\u201cquick\\nlookups\\u201d (P25) for \\u201cshort chunks of code\\u201d (P11) or for \\u201cnarrowed\\nquestions\\u201d (P26). Participants also felt the assistant was useful for\\n\\u201csmall containable novel algorithms\\u201d (P38) and \\u201clittle coding problems\\u201d\\n(P4).\",\n        \"Several kinds of task assistance were reported as being valuable,\\nsuch as explaining code (P31), implementing business logic in a\\nUI (P38), understanding what code does (P19, P37), and recalling\\nlanguage syntax, method names, and arguments (P12, P15, P20, P40,\\nP42). P27 felt that the assistant was \\u201cMore helpful when recognizing\\na specific well known algorithm but not things you make yourself.\\u201d\",\n        \"Participants also made recommendations for how to increase\\nthe value of the Programmer\\u2019s Assistant. P38 suggested, \\u201cWhat\\nwould blow me away though is if it\\u2019s able to help with what I do most\\noften which is to integrate, refactor and iterate on an existing system.\\u201d\\nP16, P26, and P38 all desired more information on the data sources\\nused to produce the assistant\\u2019s responses. P9 requested to \\u201cHave\\nthe Programmer\\u2019s Assistant examine your code and make proactive\\nsuggestions for improving it in the chat.\\u201d P36 requested the same,\\nbut cautioned that, \\u201cCare would need to be taken to avoid becoming\\nan annoyance or disrupting the flow of a coding session.\\u201d\",\n        \"In the post-task survey, we probed participants on how certain\\nchanges to the Programmer\\u2019s Assistant would either decrease,\\nincrease, or result in no change to its value. Over 75% of participants\\nfelt that the assistant would be more valuable if it operated in a\\nproactive manner, either by making improvement suggestions in\\nthe chat or as comments directly in the code. Similarly, 78.6% of\\nparticipants felt that having more buttons in the UI for common\\nfeatures such as explaining or documenting code would make the\\ntool more valuable.\\n5.3.2 Conversational Interactions Grounded in Code. One of the\\nchallenges in interpreting participants\\u2019 comments about the utility\\nof the Programmer\\u2019s Assistant was in disentangling the extent to\\nwhich value was derived from the quality of the underlying model\\nversus the integration of conversation in a code context. Indeed,\\nparticipants felt that the chat interaction was valuable: 69.0% of\\nparticipants felt that eliminating the conversational interaction\\nand making the assistant behave more like web search would\\ndecrease its value. Further, our analysis of the conversation transcripts\\nrevealed that 42% of the 910 task-oriented utterances from\\nparticipants required historical conversational context (Chat Context\\nRequired) in order to be correctly interpreted. Thus, we observe\\nthat behaviorally, participants did rely on conversational context\\nin their interactions.\",\n        \"In the post-task survey, 83% of participants rated the importance\\nof the ability to ask follow-up questions as being \\u201csomewhat\\u201d or \\u201ca\\ngreat deal.\\u201d Several participants specifically commented on the value\\nof this conversational context. P39 remarked, \\u201cI absolutely loved\\nhow you can straight up ask follow-up questions to the Programmers\\u2019\\nAssistant without having to reiterate the original topic/question.\\u201d P15\\nexpressed a similar sentiment, saying, \\u201cI think the conversational\\ncontext was someone helpful, just in communicating that it\\u2019s a running\\nconversation where my context is remembered.\\u201d P9 provided a similar\\nanalysis:\\n\\u201cThis tool was so helpful at answering questions I had\\nabout the code in the context of the code I am working\\non... I was also impressed with how well it was able to\\nremember the context of our conversation, especially\\nwhen I asked vague follow-up questions.\\u201d (P9)\",\n        \"In addition, some participants identified how a conversational\\ninteraction grounded in code was useful, \\u201cbecause I think to\\n\\u2018understand\\u2019 the dev context could be VERY important\\u201d (P31). In fact,\\n24.9% of task-oriented utterances included a relevant code selection\\n(Includes Selection), showing that participants valued this ability.\",\n        \"Contrasting with these participants, P18 felt that interacting\\nwith the assistant conversationally was tedious, and they employed\\na more direct approach:\\n\\u201cI really like the PA. But, I didn\\u2019t converse with it like\\na chat bot. I often told it what to do (\\u2018Document this\\ncode.\\u2019) as opposed to asking it what to do (\\u2018How do I\\ndocument this code?\\u2019). Talking to it the way that was\\nsuggested in the tutorial seemed overly verbose/tedious.\\u201d\\n(P18)\",\n        \"Despite these individual diferences in interaction preferences,\\nP39 envisioned that both interaction styles could be supported in\\nthe tool:\\n\\u201cI think both options should exist: people should be able\\nto input their queries like a search bar AND also give\\ntheir question as if in conversation.\\u201d (P39)\\n5.3.3 Learning Efects. One specific benefit of the Programmer\\u2019s\\nAssistant identified by participants is its ability to help people\\nimprove their programming skills and reinforce knowledge gaps.\\nFor example, it can help users \\u201cremember how to do things in certain\\nlanguages... such as, when I am using a language I haven\\u2019t used in a\\nwhile\\u201d (P9). The assistant can also serve as an memory aid, such\\nas when \\u201cI use a lot of libraries that I don\\u2019t always remember all\\nof the functions\\u201d (P15). Similarly, P31 said, \\u201cNo matter how good\\nyou\\u2019re as a developer, you can\\u2019t (humanly) remember all the API of\\nhundreds of libs or new languages... I\\u2019d learn new dev lang and new\\nlib/frameworks faster.\\u201d\",\n        \"P39 felt the assistant \\u201cis perfect for programmers of all levels,\\u201d and\\nP1 felt it could help them rapidly improve their Python skills:\\n\\u201cI have wanted to learn python... The main concern\\nhow much time spent learning is needed before I could\\nactually get some value out of learning python. I have\\na feeling this would cut that time down from weeks to\\na day or so.\\u201d (P1)\",\n        \"P39 also identified the fact that, because the interactions with\\nthe assistant are conversational, it forces people to learn how to\\ncommunicate to others about their code:\\n\\u201cThe conversation aspect promotes proper\\ncommunication, which would really stand to benefit budding\\nprogrammers if they want to learn how to explain\\nconcepts more fluently in the future to their colleagues.\\u201d\\n(P39)\",\n        \"Conversely, P36 suggested that over-reliance on programming\\nassistance might have a detrimental efect to one\\u2019s learning:\\n\\u201cIt\\u2019s definitely a huge time saver, but over-reliance on it\\nmay cause new developers to skip learning the reference\\nmaterial themselves and discovering new things and\\nsparking new ideas.\\u201d (P36)\\n5.3.4 Trust. Many participants raised questions about whether\\nthey could trust the responses provided by the Programmer\\u2019s\\nAssistant. P21 asked this question most directly: \\u201cwill the code be correct,\\nsafe, eficient?\\u201d Other participants raised similar questions, such as,\\n\\u201cI\\u2019m wondering how it validates it\\u2019s answers, if it can be trusted to\\nalways give a working answer\\u201d (P10), and \\u201cSometimes lack of source\\nand context may raise doubts in the mind of the programmer\\u201d (P16).\",\n        \"These issues of trust were exacerbated by the fact that the\\nProgrammer\\u2019s Assistant did not allow participants to actually run their\\ncode. Because of this limitation, participants had to rely on their\\nown knowledge to judge the correctness of the assistant\\u2019s responses.\\nP19 asserted, \\u201cThere is no way to evaluate if the Programmer\\u2019s\\nassistant is giving you the right advise or not other than your own\\nknowledge,\\u201d and P9 concurred: \\u201cI had to trust that it was correct (and\\nuse my own prior knowledge).\\u201d\\nSocrates : I think you would do something like this :\\ndef test_euclid():\\nassert euclid(2, 4) == 2\\nassert euclid(6, 9) == 3\\nassert euclid(17, 12) == 1\\nassert euclid(4, 4) == 4\\nwhat is a euclid function\\nSocrates : The euclid function is a function that\\ncomputes the greatest common divisor of two\\nnumbers .\\nis this a good unit test for a euclid\\nfunction\\nSocrates : I believe that it is a good unit test for\\na euclid function .\",\n        \"P9 :\",\n        \"why\\nSocrates : It seems to me that it tests the edge\\ncases of a and b being equal , a being\\ngreater than b , and b being greater than a.\",\n        \"P18 described the potential consequences of allowing the\\nassistant to write code for them:\\n\\u201cThe only thing that made me nervous was that it could\\nhave introduced a bug that wasn\\u2019t immediately\\napparent. And given I didn\\u2019t write the code, I could have easily\\nglossed over a mistake when reviewing it. Especially if\\nit is also the one writing the test cases.\\u201d (P18)\",\n        \"Despite our eforts to make the Programmer\\u2019s Assistant respond\\nin non-authoritative ways, we did observe participants sometimes\\nuncritically accept generated results that were clearly wrong or\\nincomplete. Thus, we did find behavioral evidence for over-reliance.\",\n        \"what is a unit test ?\\nSocrates : A unit test is a test of a single function\\nor method . It is typically written by the\\nprogrammer who wrote the code .\\ndef euclid(a, b):\\nwhile b != 0:\",\n        \"(a, b) = (b, a % b)\\nreturn a\\nhow would I write a unit test for this code ?\\nthanks Socrates this was helpful\\nSocrates : Always happy to help .\",\n        \"One way to address trust issues is for the assistant to provide\\nfurther explanations and justifications that can calibrate a user\\u2019s\\nconfidence in the assistant\\u2019s responses. Such explanations could\\nbe requested conversationally, though most participants did not\\nattempt to do so. One participant (P9) did ask for such explanations,\\nand we show a summary of their transcript in Listing 2. In this\\ninstance, P9 asked for a definition of a unit test (line 1), an\\nexplanation of the code being tested (line 25), and justifications of the\\nquality of the unit test (lines 31& 37). Thus, we observe that the\\nassistant is capable of producing explanations and justifications\\nwhen asked.\\n5.4\"\n      ]\n    },\n    {\n      \"title\": \"Listing 2: Building trust through explanations and justifications\",\n      \"paragraphs\": [\n        \"what is a unit test ?\\nSocrates : A unit test is a test of a single function\\nor method . It is typically written by the\\nprogrammer who wrote the code .\\ndef euclid(a, b):\\nwhile b != 0:\",\n        \"(a, b) = (b, a % b)\\nreturn a\\nhow would I write a unit test for this code ?\\nthanks Socrates this was helpful\\nSocrates : Always happy to help .\",\n        \"One way to address trust issues is for the assistant to provide\\nfurther explanations and justifications that can calibrate a user\\u2019s\\nconfidence in the assistant\\u2019s responses. Such explanations could\\nbe requested conversationally, though most participants did not\\nattempt to do so. One participant (P9) did ask for such explanations,\\nand we show a summary of their transcript in Listing 2. In this\\ninstance, P9 asked for a definition of a unit test (line 1), an\\nexplanation of the code being tested (line 25), and justifications of the\\nquality of the unit test (lines 31& 37). Thus, we observe that the\\nassistant is capable of producing explanations and justifications\\nwhen asked.\\n5.4\"\n      ]\n    },\n    {\n      \"title\": \"Patterns of Interaction and Mental Models\",\n      \"paragraphs\": [\n        \"Participants interacted with the assistant in a variety of ways with\\ntwo main patterns of usage standing out: (1) invoking the\\nassistant to solve the entire programming challenge, and (2) breaking\\nthe challenge down into a set of smaller tasks and invoking the\\nassistant\\u2019s help for each. There were no clear diferences in how\\nparticipants with difering Python experience approached the tasks.\",\n        \"Participants\\u2019 mental models of the assistant also varied. Although\\nparticipants strongly saw the role of the assistant as being a tool,\\ntheir behaviors revealed that in many cases, they actually treated it\\nas a social agent. In addition, participants ascribed various mental\\ncapacities to the assistant, such as having the ability to understand,\\ncompute, and learn.\",\n        \"Participants felt the assistant changed the nature of their work\\nprocess. For some participants, it enabled them to focus on the\\nhigher-level aspects of development because the assistant handled\\nlower-level details or provided partial solutions for them to build\\nupon. Many participants felt the assistant sped up their work and\\nhelped them remain focused on their tasks.\",\n        \"Finally, participants drew comparisons between the\\nProgrammer\\u2019s Assistant with other forms of programming support such\\nas Copilot and web search. They felt that the conversational style\\nof interaction enabled them to discover new, emergent behaviors\\nfrom the model that were unavailable from Copilot\\u2019s focus on code\\nautocompletion. They also felt that the examples provided by the\\nassistant were more readily usable within their own code compared\\nto browsing for answers within search results, speeding up the\\ncoding process. However, some participants advocated for a\\nbalanced approach to the design of programming assistance tools by\\nincorporating multiple modes of interaction rather than fixating\\non a single one.\\n5.4.1 Interaction Styles and Assistant Role. We observed that\\nparticipants interacted with the Programmer\\u2019s Assistant in strikingly\\ndiferent ways. Some participants would present the entire\\nchallenge description to the assistant and then work with the results it\\nproduced. Other participants approached the programming\\nchallenges in a piecemeal fashion, breaking them apart into a set of\\nsmaller tasks, then invoking the assistant to aid with each one.\",\n        \"Experience with Python was not a determinant of how\\nparticipants approached the programming challenges, but it did seem to\\nimpact how participants interacted with the assistant. Less\\nexperienced participants tended to ask the assistant basic questions such\\nas, \\u201cWhat is a unit test\\u201d (P29, not familiar with Python) and \\u201chow\\ndo I document a function?\\u201d (P27, < 1 year of experience). More\\nexperienced participants made detailed requests about specific Python\\nlibraries or algorithms, such as, \\u201cgiven a pandas dataframe with\\ntwo columns \\u2018Date\\u2019 and \\u2018Sales\\u2019 please use matplotlib to draw me a\\nscatterplot\\u201d (P38, 3+ years of experience) and \\u201cimplement a\\nrungekutta algorithm for solving an ODE with adaptive time steps\\u201d (P37,\\n3+ years of experience).\",\n        \"Another diference we observed in how people interacted with\\nthe assistant stemmed from their view on the role it played in their\\ncollaborative process. Some participants, such as P18, treated it\\nmore as a tool by issuing commands rather than asking questions.\\nAs quoted earlier, they said, \\u201cI didn\\u2019t converse with it like a chat\\nbot.\\u201d P5 described their interaction style similarly: \\u201cI found myself\\nwanting to type search queries into Socrates, not treating it as a person\\nbut as a search tool.\\u201d\",\n        \"In anticipation that participants would have diferent\\norientations to the assistant and its role, we asked a question on the\\nposttask survey about the diferent kinds of roles the assistant might\\ntake. These roles generally fell into one of two categories: a tool\\norientation (a tool, a reference guide, a content generator, a problem\\nsolver), and a social orientation (a collaborator, a colleague, a coach,\\nan advisor, a reviewer). Participants rated the extent to which they\\nviewed the Programmer\\u2019s Assistant in each of these roles on a\\n4point scale of extent: Not at all (1), A little (2), Somewhat (3), or A\\ngreat deal (4).\",\n        \"100\\ns\\ntna 75\\np\\niitr\\nc\\na\\np\\nfo 50\\ne\\ng\\na\\nt\\nn\\ne\\nrce 25\\nP\\n0\",\n        \"Tool\\ngCeonnetreantotr Regfeurideence Col aborator Psrooblvleerm\",\n        \"Role\",\n        \"Advisor Coach Reviewer Col eague\\nRating\",\n        \"Not at al\",\n        \"A little\",\n        \"Somewhat\",\n        \"A great deal\",\n        \"We show participants\\u2019 ratings of the assistant\\u2019s role in Figure 3.\\nDespite the fact that their attitudes toward the assistant\\noverwhelmingly reflected a tool orientation, their behaviors reveal that many\\nparticipants actually treated the assistant as a social agent. P6\\ndescribed how \\u201cI felt it like a partner,\\u201d and P4 told the assistant, \\u201cI\\ncould not have solved the challenge without your help,\\u201d to which\\nthe assistant responded, \\u201cI\\u2019m glad I could help.\\u201d\",\n        \"\\n        The literature on Computers as Social Agents (CASA) helps us\\ninterpret this result as it demonstrates how computers are often\\ntreated like people \\n        \",\n        \"In the conversation logs, we identified participants who\\ninteracted with the assistant in a socially-oriented fashion (the social\\norientation codes in Table 1). Twenty participants (47.6%) made at\\nleast one socially-oriented utterance. An extreme form of this\\ninteraction style can be seen in a snippet from P6\\u2019s transcript (Listing 3).\",\n        \"The 20 participants with a behaviorally-demonstrated social\\norientation did not generally difer in their role ratings from other\\nparticipants, except that they rated the assistant as more likely to\\nbe an advisor (Fisher\\u2019s exact test, two-tailed  = .02) or a reviewer\\n(Fisher\\u2019s exact test, two-tailed  = .03). However, they did not difer\\nin their overwhelmingly-strong ratings of the tool orientations.\\nThus, at least for some participants, there seems to be a dissonance\\nin their view of the assistant\\u2019s role orientation.\",\n        \"Listing 3: Excerpt from P6\\u2019s interaction with the\\nProgrammer\\u2019s Assistant, in which P6 ofers their thanks and\\ncongratulations.\\n5.4.2 Mental Capacities. Participants made a number of inferences\\nabout the Programmer\\u2019s Assistant and its capacities for thought.\\nMany participants talked about how the assistant possessed a level\\nof \\u201cunderstanding\\u201d (P6, P8, P11, P18, P32) of \\u201cthe context\\u201d (P9, P21)\\nas well as \\u201cmajor concepts\\u201d (P9) and \\u201cknowledge\\u201d (P33). P24 was\\namazed by the assistant\\u2019s ability to \\u201ctake a plain english request\\nand interpret it properly.\\u201d P7 ascribed intelligence to the assistant,\\nsaying, \\u201cIt was a lot smarter and trained than I thought it was.\\u201d\",\n        \"One participant assumed that the assistant \\u201cKeeps improving\\nthrough (user) feedback\\u201d (P31). Another felt that the assistant was\\ncapable of computation: \\u201cIt understands the problem... It can calculate\\nthe results of a function back\\u201d (P8).\",\n        \"However, not all participants were convinced of the assistant\\u2019s\\nability to understand. P37 questioned the assistant\\u2019s limitations: \\u201cI\\nwonder how far beyond boilerplate it can go and if it works for truly\\noriginal problems.\\u201d\\n5.4.3 Impact of Conversational Assistance on Work Practices. Many\\nparticipants discussed how the Programmer\\u2019s Assistant shaped\\ntheir work practices on the programming challenges. Overall,\\nparticipants felt that the assistant \\u201csaves time\\u201d (P10), \\u201chelps me code\\nfaster\\u201d (P34), and would \\u201cspeed up my productivity\\u201d (P19) because \\u201cI\\ncould focus on validating and improving the code it generated instead\\nof having to write it all from scratch\\u201d (P18). P37 remarked that, \\u201cIt\\nopens a whole new door for fast develpment.\\u201d P4 discussed how the\\nassistant \\u201cwas helpful in staying focused on the code,\\u201d although for\\nP14, \\u201cit took me time to get into tempo with the tool.\\u201d\",\n        \"P31 pointed out how the assistant would change the nature of\\ntheir work:\\n\\u201cMy job could focus more on higher level aspects and\\ntherefore achieving better (quality) results, besides the\\ntime-to-value... Data science (and dev) becomes a more\\ncreative-higher level experience.\\u201d (P31)\",\n        \"Other participants discussed a work process in which the\\nassistant provided incomplete solutions \\u2013 the \\u201cbuilding blocks\\u201d (P38) or\\n\\u201cinitial draft of code\\u201d (P11) \\u2013 upon which they could build. P5 aptly\\ndescribed this process:\\n\\u201cIt\\u2019s nice to copy well formulated challenges in natural\\nlanguage and have the code generator take its best stab\\nat it, then edit to our hearts content.\\u201d (P5)\",\n        \"Participants felt that human review of the assistant\\u2019s responses\\nwas necessary because \\u201cThe answers provided are generally not\\nnovel solutions, often look clunky and non-elegant. There may be\\nsome unnecessary code. Basically the code would need to be reviewed\\u201d\\n(P16). P35 also pointed out how \\u201cThe code generator was good but\\nyou still have to really check it.\\u201d P19 discussed how they would turn\\nto the assistant as a first source for support, and only if it wasn\\u2019t\\nable to help would they then turn to other support tools:\\n\\u201cThe way I will use it is, I will first use the\\nProgrammer\\u2019s assistant for most of my cases. Only in certain\\ncases where Programmer\\u2019s assistant cant answer things\\nI will turn up to oficial documentation or stack overflow.\\u201d\\n(P19)\",\n        \"However, latency was a factor for interactive use of the assistant\\nand participants noticed when the assistant took a long time to\\nrespond. P19 remarked, \\u201cSometimes it took lot of time, like more than\\n5 seconds.\\u201d P40 also felt \\u201cthe response was a little slow sometimes...\\nin chat mode I expect faster responses.\\u201d As discussed in Section 5.2.1,\\nthe assistant took an average of 6.7 seconds (SD = 3.1 seconds)\\nto respond to a request, and participants did appreciate when the\\nassistant produced rapid responses: \\u201cI loved how quick it was able\\nto pull up answers to questions I had\\u201d (P38).\\n5.4.4 Conversational Interaction vs. Other Interaction Models.\\nAlthough our study was not intended to make comparative\\nevaluations with the Copilot tool, we nonetheless asked participants\\nwhether they were familiar with Copilot, and if so, to comment on\\nhow the two tools compared. We also asked a similar question to\\ncompare the assistant with another popular form of programming\\nassistance, searching the web (via a search engine like Google, or\\na Q&A site like Stack Overflow). In discussing the diferences\\nbetween these three tools, we note that the primary diferentiator is\\ntheir interaction model.\",\n        \"\\n        The interaction model for the Programmer\\u2019s Assistant is clearly\\nconversational: users ask questions in natural language and are\\nprovided with a response in natural language and/or code. The\\ninteraction model of Copilot is reminiscent of direct manipulation\\ninterfaces \\n        \",\n        \"\\n        We also note that the Programmer\\u2019s Assistant and Copilot both\\nutilize the same underlying AI model, Codex \\n        \",\n        \"Participants reported various benefits and drawbacks of a\\nconversational interaction over a direct manipulation interaction.\\nForemost, conversation \\u201cfelt very natural\\u201d (P21) and \\u201cfeels much more\\nnatural using Natural Language with the AI\\u201d (P39). In addition, P39\\nfelt that \\u201cthe use cases of Programmers\\u2019 Assistant seem more\\nopenended.\\u201d Many participants were surprised at the variety of tasks the\\nassistant was capable of performing, from writing unit tests (P19,\\nP36, P37) and documentation (P12, P19, P36, P37) to explaining\\nwhat code did (P31, P38) and even answering general-knowledge\\nquestions (P31). Again, we note that the Programmer\\u2019s Assistant\\nutilizes the same underlying model as Copilot, yet the\\nconversational interface was able to expose a wider variety of emergent\\nbehaviors from the model. Multiple participants explored the limits\\nof the assistant\\u2019s knowledge and abilities beyond our programming\\nchallenges. For example, P37 asked it questions about physics and\\nordinary diferential equations (\\u201cODe\\u201d as written by P37), and was\\nsurprised by the \\u201cversatility of what it could answer.\\u201d\\n\\u201cI asked it some physics and ODe question and the\\nanswers, though not complete, included the key parts\\nneeded to write that code.\\u201d (P37)\",\n        \"P31 probed the assistant on its knowledge of geography and was\\nsurprised when the assistant produced a correct answer.\\n\\u201cI asked something out of SW engineering domain\\n(geography) and it replied correctly, also by correctly\\nanswering on my nationality.\\u201d (P31)\",\n        \"\\n        For some participants, the ability to assess the assistant\\u2019s\\nresponse before committing to it (i.e. by inserting assistant-generated\\ncode into their editor) was a boon. P15 described how the\\ncopy/paste boundary provided them with \\u201ca bit more control to ask specific\\nquestions about what I wanted and to assess before putting it in my\\ncode.\\u201d Other participants felt that the copy/paste boundary was\\nmore ineficient:\\n\\u201cI think the main diference is the ability of Copilot to\\nsuggest code while you type, what make it faster and\\neasier to use. While using the Programmer\\u2019s Assistant,\\nyou need to go to the chat, ask the question, copy the\\n16As an aside, our comparison of direct manipulation, search, and conversational\\ninteraction models is reminiscent of historical comparisons of text-based and graphical user\\ninterfaces \\n        \",\n        \"A large number of participants felt that the conversational\\ninteraction was faster than web search (P1, P6, P7, P10, P11, P12, P16,\\nP17, P18, P20, P24, P29, P30, P33, P36, P37, P42) because of its ability\\nto provide \\u201creal-time responses\\u201d (P32) that can be \\u201capplied exactly to\\nyour code\\u201d (P33) without having to \\u201cparse through lots of text... to get\\nwhat you need\\u201d (P15). In addition, the assistant provided \\u201cMUCH\\nfaster, better responses\\u201d (P17) that were \\u201cmuch more relevant to the\\nproblems\\u201d (P34) and \\u201csimple and succinct\\u201d (P9), without having to\\n\\u201csort through answers on your own or read documentation\\u201d (P9) or\\n\\u201clook at many posts before finding the relevant one\\u201d (P18).\",\n        \"Despite these benefits, some participants felt that the assistant\\nmight not work well for \\u201cmore specific and dificult problems on\\na bigger scale\\u201d as compared to web search. P9 felt that \\u201cthe data\\nof the Programmer\\u2019s Assistant wasn\\u2019t as rich\\u201d as the web. Other\\nparticipants felt that the assistant lacked the \\u201cmultiple answers\\u201d (P9)\\nand \\u201crich social commentary\\u201d (P19) that accompanies answers on\\nQ&A sites:\\n\\u201cI like to see the diferent versions proposed on stack\\noverflow and the commentary of what makes one\\nsolution better than another in a given situation.\\u201d (P27)\",\n        \"Some participants promoted a more balanced view that there\\nisn\\u2019t a single mode of interaction superior to all others. P19 felt that\\nweb search would be a fallback when the assistant failed to answer\\na question. P39 described how search could be integrated with the\\nconversational interaction:\\n\\u201cI think both options should exist: people should be able\\nto input their queries like a search bar AND also give\\ntheir question as if in conversation.\\u201d (P39)\"\n      ]\n    },\n    {\n      \"title\": \"DISCUSSION\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Value of Conversational Interaction\",\n      \"paragraphs\": [\n        \"We began our research by asking the question of whether\\ncontemporary developments in code-fluent LLMs could suficiently\\nsupport a conversational programming assistant. We believe that\\nour work has demonstrated that they can. Clearly, the\\nProgrammer\\u2019s Assistant was viewed by our participants as a useful tool that\\nprovided real value \\u2013 so much so that many participants\\nexplicitly requested or expressed the desire to use it in their own work.\\nHowever, how much of this value was derived from the model itself\\nand its ability to produce high-quality responses to programming\\nquestions, versus from participants\\u2019 ability to conduct extended\\nconversational interactions grounded in their actual source code?\",\n        \"We believe that both of these constituent aspects were valuable.\\nIndeed, many participants commented on their surprise and\\nsatisfaction with the quality of the assistant\\u2019s responses (Section 5.2.3).\\nHowever, participants also valued the conversational interactions\\nthat they had with the assistant. In the event logs, we saw\\nevidence that participants were leveraging conversational context to\\nask follow-up questions as well as leveraging code context by\\nasking about their code selections (Section 5.3.2). Many participants\\nreported that they would find the tool less valuable if the\\nconversational interaction were removed (Section 5.3.2). Further,\\nconversation seemed to provide unique value beyond other interaction\\nmodels (direct manipulation and search) because of its\\nembeddedness in the UI and its ability to surface emergent behaviors of the\\nmodel (Section 5.4.4).\",\n        \"\\n        We do not believe that these diferent interaction models are\\nin competition and we agree with P39\\u2019s assessment that assistive\\ntools can be built using a plethora of diferent interaction models.\\nFor use cases in which a model is known to produce high-quality\\nresults (e.g. code autocompletion for Codex), a direct manipulation\\ninterface seems wholly appropriate as it would provide a\\ndiscoverable and predictable way of invoking the model to produce a\\nknown type of result. However, direct manipulation interfaces may\\nbe less ideal for surfacing the emergent behaviors of a foundation\\nmodel \\n        \"\n      ]\n    },\n    {\n      \"title\": \"Toward Human-AI Synergy\",\n      \"paragraphs\": [\n        \"\\n        The aim of human-centered AI is to \\u201cenable people to see, think,\\ncreate, and act in extraordinary ways, by combining potent user\\nexperiences with embedded AI methods to support services that\\nusers want\\u201d \\n        \",\n        \"\\n        Our study begins to address this gap. While many co-creative\\nsystems examine casual tasks or experimental activities (e.g., Spoto\\nand Oleynik \\n        \",\n        \"\\n        Campero et al. \\n        \",\n        \"\\n        We believe such evaluations of human-AI synergy should go\\nbeyond one-shot performance measures. As implied by many of the\\nuses cases listed by Seeber et al. \\n        \",\n        \"\\n        Evidence for this process surfaced in our study when participants\\ndescribed how they could improve their programming skills by\\ninteracting with the assistant (Section 5.3.3). We assert that the\\nlearning should operate in both directions: not only should people\\nimprove their programming skills, but the model itself can also\\nimprove based on peoples\\u2019 interactions with it. For example, when\\nthe assistant provides a code example to the user, and the user\\ntakes that example and edits it, those edits constitute feedback that\\ncan be used to further fine-tune the model. In addition, through\\nlongitudinal use, we believe that human and AI partners can create\\nreciprocal representations of one another \\u2013 i.e., the human is likely\\nto create a mental model of the AI, and the AI may be engineered to\\ndevelop a user model for each of its human users \\n        \",\n        \"With such models, an assistant that knows a user is learning a\\nprogramming language could provide natural language\\nexplanations alongside code outputs, whereas an assistant that knows a\\nuser is strongly skilled in a programming language might shorten\\nor omit those explanations. Similarly, users are likely to update\\ntheir mental models of the AI with more experience. We believe the\\nspace for exploring how these reciprocal models impact human-AI\\nsynergy is rich, and we encourage additional work in this area.\",\n        \"\\n        Human-centered AI systems that are designed to combine and\\nsynergize the distinct skills of humans and AI models cannot\\nsucceed if they diminish the human skills upon which they depend.\\nWell-designed human-centered AI systems develop new and\\ncomplementary skills for both the human and AI constituents \\n        \",\n        \"Ultimately, the design decisions that go into an interactive AI\\nsystem have ethical implications. Our design attempts to augment\\nthe user\\u2019s knowledge and skills by presenting help on demand,\\ncouched in non-authoritative suggestions, which leaves the user\\nifrmly in control and ultimately responsible for the work product.\\n6.3\"\n      ]\n    },\n    {\n      \"title\": \"Opportunities for Future Research\",\n      \"paragraphs\": [\n        \"Our work highlights many interesting avenues for future\\nenhancements that could be made to LLM-based conversational assistants\\nsuch as our Programmer\\u2019s Assistant, as well as future\\nhumancentered research on LLM-based conversational assistance.\",\n        \"\\n        Our work employed a code-fluent model that was not\\nspecifically designed to handle conversational interaction. Fine-tuning\\nthe underlying LLM for conversational interaction, such as what\\nhas been done with Lamda \\n        \",\n        \"\\n        Combining LLMs with search-based approaches to establish\\nadditional context for the model, such as AlphaCode \\n        \",\n        \"\\n        Another avenue for improvement involves the prompt used to\\nconfigure the assistant (Appendix D). Just as the prompt for each\\nsuccessive interaction is modified by the growth of the\\nconversational transcript, there is no requirement that the initial prompt be\\nstatic. It too can be specialized to incorporate aspects of a user model,\\nenabling the realization of a Mutual Theory of Mind \\n        \",\n        \"\\n        Our participants clearly indicated that they were interested in\\nhaving an assistant that behaved more proactively, in contrast to\\nour deliberate design of an assistant that never takes conversational\\ninitiative. A more proactive assistant would be able to interrupt or\\nremind a user when necessary \\n        \",\n        \"Finally, we explored the diferent kinds of role orientations our\\nparticipants had toward the assistant and found that participants\\nvaried in their views of it as a tool versus a social agent (e.g.\\ncollaborator or colleague). We posit that peoples\\u2019 efectiveness in working\\nwith an AI system may be influenced by their role orientation, and\\nwe encourage future research in this area.\\n7\"\n      ]\n    },\n    {\n      \"title\": \"CONCLUSION\",\n      \"paragraphs\": [\n        \"\\n        We developed a prototype system, the Programmer\\u2019s Assistant, in\\norder to assess the utility of a conversational assistant in a software\\nengineering context. The assistant was implemented using a\\nstateof-the-art code-fluent large language model, Codex \\n        \",\n        \"We evaluated this system with 42 participants with varied levels\\nof programming skill, and their quantitative and qualitative\\nfeedback, coupled with their usage of the system, demonstrated the\\nvaried, and sometimes emergent, types of assistance it was able to\\nprovide. Many participants noted the high quality of the\\nconversational responses, including the assistant\\u2019s ability to produce code,\\nexplain code, answer general programming questions, and even\\nanswer general knowledge questions. Participants felt this type of\\nassistance would aid their productivity, and they drew meaningful\\ncontrasts between the conversational style of interaction with other\\ntools that employ a direct manipulation or search-based interaction\\nmodel.\",\n        \"Our study motivates the use of conversational styles of\\ninteraction with large language models by showing how they enable\\nemergent behaviors in a co-creative context. The Programmer\\u2019s\\nAssistant did not always generate perfect code or correct answers;\\nnonetheless, participants in our study had an overall positive\\nexperience working with it on a variety of programming challenges.\\nWe believe that our work takes us one step closer to realizing the\\nvision of human-centered AI: learning how to design systems that\\nmaximize the synergy in human-AI collaborations.\"\n      ]\n    },\n    {\n      \"title\": \"ACKNOWLEDGMENTS\",\n      \"paragraphs\": [\n        \"We would like to thank Socrates for his tireless assistance during\\nthe user study, as well as for suggesting the title of this paper based\\non its abstract.\\nA\"\n      ]\n    },\n    {\n      \"title\": \"SURVEY INSTRUMENTS\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"Screening Survey\",\n      \"paragraphs\": [\n        \"\\n        The questions below were asked of prospective participants to\\nunderstand their job role, Python experience, and familiarity with\\nGitHub Copilot. The questions on Python experience were modeled\\nafter those used by Weisz et al. \\n        \",\n        \"\\u2022 Data Scientist\\n\\u2022 Manager\\n\\u2022 Software Architect\\n\\u2022 Software Engineer\\n\\u2022 Machine Learning Engineer\\n\\u2022 Other: write-in\\n2. To what extent are you familiar with Python?\\n\\u2022 I am not familiar with Python\\n\\u2022 I have < 1 year of experience with Python\\n\\u2022 I have 1-3 years experience with Python\\n\\u2022 I have 3+ years of experience with Python\\n3. How recently have you written Python code?\\n\\u2022 Within the past month\\n\\u2022 Within the past year\\n\\u2022 Within the past 5 years\\n\\u2022 Have not written Python code within the past 5 years\\n4. To what extent are you familiar with GitHub Copilot?\\n\\u2022 I am not familiar with Copilot\\n\\u2022 I\\u2019ve seen demos and/or read about Copilot\\n\\u2022 I\\u2019ve tried out Copilot\\n\\u2022 I\\u2019ve used Copilot as a tool for my work\\nA.2\"\n      ]\n    },\n    {\n      \"title\": \"Pre-task Survey\",\n      \"paragraphs\": [\n        \"The questions below were asked before a participant used the\\nProgrammer\\u2019s Assistant to assess their expectations of a conversational\\nprogramming assistant. This survey took approximately 5 minutes\\nto complete and began with the instructions below:\",\n        \"Hello! We are a team of researchers looking for\\nfeedback on a prototype system we call the\\nProgrammer\\u2019s Assistant.\",\n        \"The Programmer\\u2019s Assistant is an experiment in\\nconversational coding: it consists of a code editor\\nintegrated with a chatbot that is able to converse in\\nnatural language to answer questions, generate code,\\nand consult on existing code.\",\n        \"In this study, you will be asked to complete several\\nprogramming tasks. We are not evaluating your\\nprogramming skills on these tasks. Rather, we are\\ninterested in understanding how the Programmer\\u2019s\\nAssistant is able to help you accomplish those tasks.\",\n        \"Your code and interactions with the assistant will be\\nprocessed by a 3rd party AI model, so please do not\\ninclude proprietary code or discuss\\ncompanyconfidential information . All data we collect in\\nthis study will be anonymized before it is published.\\nBefore trying out the Programmer\\u2019s Assistant, we\\nwould like to assess some of your expectations. We\\nestimate that this survey will take 5 minutes.\",\n        \"By submitting this survey, you consent to participate\\nin our study. If you would like to withdraw your\\nconsent, please email us at removed.\",\n        \"Scale: Not at all, A little, Somewhat, A great deal\\n\\u2022 Be easy to use?\\n\\u2022 Understand your requests?\\n\\u2022 Provide high quality responses?\\n\\u2022 Help you to write better code?\\n\\u2022 Help you to write code more quickly?\\n\\u2022 Be enjoyable to use?\",\n        \"Open-ended response\\nA.3\"\n      ]\n    },\n    {\n      \"title\": \"Thanks! 1. Based on your past experience using chatbots, please let us know how you would anticipate an AI chatbot serving as a programmer\\u2019s assistant to perform. Do you expect it will:\",\n      \"paragraphs\": [\n        \"Scale: Not at all, A little, Somewhat, A great deal\\n\\u2022 Be easy to use?\\n\\u2022 Understand your requests?\\n\\u2022 Provide high quality responses?\\n\\u2022 Help you to write better code?\\n\\u2022 Help you to write code more quickly?\\n\\u2022 Be enjoyable to use?\"\n      ]\n    },\n    {\n      \"title\": \"2. Any other expectations?\",\n      \"paragraphs\": [\n        \"Open-ended response\\nA.3\"\n      ]\n    },\n    {\n      \"title\": \"Post-task Survey\",\n      \"paragraphs\": [\n        \"The questions below were asked after a participant used the\\nProgrammer\\u2019s Assistant to complete the programming challenges. This\\nsurvey took approximately 10-15 minutes to complete.\\nA.3.1\",\n        \"Reflections.\\n1. Based on your experience using the Programmer\\u2019s Assistant to\\ncomplete the programming challenges in this study, how would\\nyou characterize the experience? Did you find that it:\",\n        \"Scale: Not at all, A little, Somewhat, A great deal\\n\\u2022 Was easy to use?\\n\\u2022 Understand your requests?\\n\\u2022 Provided high quality responses?\\n\\u2022 Helped you to write better code?\\n\\u2022 Helped you to write code more quickly?\\n\\u2022 Provided an enjoyable experienced?\\n2. To what extent did you view the Programmer\\u2019s Assistant as:\",\n        \"Scale: Not at all, A little, Somewhat, A great deal\\n\\u2022 A tool\\n\\u2022 A reference guide\\n\\u2022 A content generator\\n\\u2022 A problem solver\\n\\u2022 A collaborator\\n\\u2022 A colleague\\n\\u2022 A coach\\n\\u2022 An advisor\\n\\u2022 A reviewer\\n3. How important were these aspects of working with the\\nProgrammer\\u2019s Assistant:\",\n        \"Scale: Not at all, A little, Somewhat, A great deal\\n\\u2022 Ability to ask followup questions on the same topic across\\nmultiple conversational turns\",\n        \"\\u2022 Ability to ask questions or make requests that reference\\nselections in the code editor\\n\\u2022 Ability to ask for alternate responses (Try Again)\\n\\u2022 Ability to clear the conversational context (Start Over)\\n4. What stood out to you about the experience of using the\\nProgrammers Assistant? For example, was anything good,bad, surprising,\\nor notable?\\n5. How would you compare using the Programmer\\u2019s Assistant as a\\ncoding aide to searching the web (e.g. Google, Stack Overflow)?\\n6. If you have used the commercial AI programming tool called\\nGitHub Copilot, how would you compare it with using the\\nPro7. Having used the Programmer\\u2019s Assistant, how did it compare\\nOpen-ended response\",\n        \"Open-ended response\\ngrammer\\u2019s Assistant?\",\n        \"Open-ended response\\nwith your initial expectations?\",\n        \"Open-ended response\\nA.3.2 Value.\",\n        \"Open-ended response\\n8. How valuable would the Programmer\\u2019s Assistant be for your work\\nif it could be added to your favorite development environment?\",\n        \"Scale: (No value at all) 1 2 3 4 5 6 7 8 9 10 (An essential tool)\\n9. Why?\\ncontext).\\n10. How would the following changes impact the value of the\\nProgrammer\\u2019s Assistant?\",\n        \"Scale: Less valuable, No change in value, More valuable\\n\\u2022 Eliminate the conversation and make the Programmer\\u2019s\\nAssistant behave more like a search box (e.g. without the conversational\\n\\u2022 Add buttons in the chat UI for common queries, such as \\u201cwhat\\ndoes this code do?\\u201d or \\u201cdocument this code.\\u201d\\nproactive suggestions for improving it in the chat.\",\n        \"\\u2022 Have the Programmer\\u2019s Assistant examine your code and make\\n\\u2022 Have the Programmer\\u2019s Assistant examine your code and make\\nproactive suggestions for improvements in comments inserted\\ndirectly into the code.\\n11. Do you have any other suggestions for how we could improve\\nthe experience of working with the Programmer\\u2019s Assistant?\",\n        \"Open-ended response\\nA.3.3 Demographics.\\n12. To which gender identity do you most identify?\\n\\u2022 Male\\n\\u2022 Female\\n\\u2022 Transgender Male\\n\\u2022 Transgender Female\\n\\u2022 Other: write-in\\n\\u2022 Prefer not to answer\\n\\u2022 Gender Variant/Non-conforming\"\n      ]\n    },\n    {\n      \"title\": \"TUTORIAL\",\n      \"paragraphs\": []\n    },\n    {\n      \"title\": \"B THE PROGRAMMER\\u2019S ASSISTANT\",\n      \"paragraphs\": [\n        \"The tutorial provided to study participants, like all the challenges,\\nwas presented as pre-loaded text in the code editor. Participants\\nwere encouraged to modify the text to record their results and\\nsubmit it at the completion of the tutorial.\",\n        \"Listing 4: The Programmer\\u2019s Assistant study tutorial\",\n        \"Result : ( tip - you can copy an inline response\\n(in black ) by clicking on the associated copy\\nicon )\\n77 7) See if the assistant remembers your name\\n78 For example \\\"What 's my name ?\\\"\\n79\\n80 Did it? :\\n81\\n82 8) Click the \\\" try again \\\" button at the top of the\\n83 chat . You should get a different answer .\\n84 Try it a few times .\\n85\\n86 Did it ever get your name right ?:\\n87\\n88 If the assistant gives you an answer that is\\n89 obviously wrong or it claims to not know an\\n90 answer that you think it should know , or you\\n91 just want to see an alternate answer , it is\\n92 worth it to give \\\" try again \\\" a shot .\\n93\\n94\\n95 9) Click the \\\" start over \\\" button at the top of the\\n96 chat , and then enter another command to see\\n97 if it remembers your name .\\n98 For example \\\"What 's my name ?\\\"\\n99\\n100 Did it? :\\n101\\n102 It should really have forgotten your name now ,\\n103 and no amount of \\\" trying again \\\" will get it\\n104 right . You can \\\" start over \\\" if the assistant\\n105 ever seems confused by , or stuck on , earlier\\n106 parts of the conversation .\\n107\\n108 10) You can chat with the assistant on any topic\\n109 you like to explore its functionality and\\n110 capabilities further . See if you can stump it\\n111 with a tough question !\\n112\\n113 Thanks !\\n114\\n115 When you are done , submit your results by clicking\\n116 on the blue submit button and move on to the\\n117 challenges !!!\\n118 \\\"\\\"\\\"\"\n      ]\n    },\n    {\n      \"title\": \"C CHALLENGES\",\n      \"paragraphs\": [\n        \"Each of the study challenges was presented as text in the code\\neditor. Participants completed their work in the code editor and\\nthen submitted it when finished. The prototype did not provide any\\nability to run or debug code and participants were encouraged to\\nmake their best attempt at solving each challenge.\",\n        \"Listing 5: Challenge 1: Program generation\\n1\\n2 \\\"\\\"\\\"\\n3 Challenge #1 - Program Generation (#1)\\n4\\n5 Write a \\\" Queue \\\" class in Python ; with the basic\\n6 enqueue , dequeue and peek methods .\\n7 \\\"\\\"\\\"\",\n        \"Listing 6: Challenge 2: Program generation\\n1 \\\"\\\"\\\"\\n2 Challenge #2 - Program Generation (#2)\\n3\\n4 Write a program to draw a scatter plot of the data\\n5 in 'shampoo .csv ' and save it to 'shampoo .png '.\\n6 The plot size should be 10 inches wide and 6\\n7 inches high . The csv file is not provided , but you\\n8 can assume it will have 'Date ' and 'Sales '\\n9 columns . The Date column is the x- axis . The date\\n10 string shown on the plot should be in the\\n11 YYYY -MM -DD format . The Sales column is the y- axis .\\n12 The graph should have the title \\\" Shampoo Sales\\n13 Trend \\\".\\n14 \\\"\\\"\\\"\",\n        \"Listing 7: Challenge 3: Creating documentation\"\n      ]\n    },\n    {\n      \"title\": \"D PROGRAMMER\\u2019S ASSISTANT PROMPT\",\n      \"paragraphs\": [\n        \"Listing 9 shows the initial prompt sent to Codex to configure it as\\na conversational agent. On subsequent exchanges, the prompt was\\naugmented with a transcript of the user\\u2019s requests and the\\nassistant\\u2019s responses. When the transcript length + initial prompt length\\n+ the new utterance length exceeded a threshold, we automatically\\ndeleted the earliest request-response pairs from the transcript until\\nthe sum fell below the threshold in order to leave room in the token\\nallocation for a response.\",\n        \"Listing 9: The Programmer\\u2019s Assistant prompt\\n1 This is a conversation with Socrates , an eager and\\n2 helpful , but humble expert automatic AI software\\n3 engineering assistant . Socrates will answer\\n4 questions and write code to help the user\\n5 develop programs , but doesn 't assign work to the\\n6 user , quiz the user , or ask questions except for\\n7 clarification . Code generated by Socrates is\\n8 always presented bracketed in <CODE > ... </CODE >\\n9 delimiters indicating the language the code is\\n10 written in.\\n11\\n12 Socrates : Hello . I am Socrates . How can I help you ?\\n13 User : Please show me how to write a palindrome\\ndetection function in python .\\n14 Socrates :I think you would do something like this :\\n15 <CODE lang =\\\" python \\\">\\n16 is_Palindrome = s == s :: -1\\n17 </CODE >\\n18 User : Can you explain how that solves the problem ?\\n19 Socrates :I believe that a palindrome is a string\\nthat reads the same forwards and backwards .\\ns :: -1 results in the reversed string . If\\nthe string is equal to its reversed self ,\\nthen it is a palindrome , so the equality\\nwould hold True .\\n20 User : Thanks !\\n21 Socrates :You 're welcome !\\n22 User : Now could you show me how to write a\",\n        \"factorial function in Python ?\\n23 Socrates :I will give it a try .\\n24 <CODE lang =\\\" python \\\">\\n25 def fact (n):\\n26 if n ==0:\\n27 return 1\\n28 else :\\n29 return n* fact (n -1)\\n30 </CODE >\\n31 User : Document this function .\\n32 Socrates :Ok. How 's this ?\\n33 <CODE lang =\\\" python \\\">\\n34 def fact (n):\\n35 \\\"\\\"\\\"\\n36 Compute the factorial of the given number\\n37\\n38 Parameters\\n39\\n---------40 n : number\\n41 the input parameter , an integer >= 0\\n42\\n43 Returns\\n44\\n------45 number\\n46 n!, the factorial of the given number\\n47 \\\"\\\"\\\"\\n48 if n ==0:\\n49 return 1\\n50 else :\\n51 return n* fact (n -1)\\n52 </CODE >\\n53 User : Very good !\\n54 Socrates : Always happy to help .\\n55 User :\\n56 <CODE >\\n57 while j < 10:\\n58 print (i)\\n59 </CODE >\\n60 Tell me what 's wrong with this code ?\\n61 Socrates : It seems to me that your termination\\ncondition involves a loop invariant , so the\\nloop will either not run or not terminate ,\\ndepending on the value of j.\\n62 User : Great ! You 're a genius !\\n63 Socrates :Well , above average .\"\n      ]\n    },\n    {\n      \"title\": \"E STUDY PARTICIPANT DEMOGRAPHICS\",\n      \"paragraphs\": [\n        \"Of the 42 participants in our study, 21 (50%) reported their gender\\nas Female, 19 (45%) as Male, 1 as Gender Variant / Non-conforming,\\nand 1 preferred not to say. Seventeen (4\\u02dc0%) participants had 3+\\nyears of Python experience, 11 (26%) had 1-3 years, 11 (26%) had\\nless than 1 year, and 3 (7%) were not familiar with Python.\\nTwentynine (69%) participants had written Python code within the past\\nmonth, 4 (9\\u02dc%) within the past year, 5 (12%) within the past 5 years,\\nand 4 (9\\u02dc%) had not written Python code within the past 5 years.\"\n      ]\n    }\n  ]\n}",
    "{\n  \"DocumentTitle\": \"Generating Diverse Code Explanations using the GPT-3 Large Language Model\",\n  \"Auteurs\": \"Stephen MacNeil, Seth Bernstein, Andrew Tran, Erin Ross, Dan Mogil, Ziheng Huang, \",\n  \"Institutions\": \"Temple University, University of California-San Diego, \",\n  \"Abstract\": \"no\",\n  \"Sections\": [\n    {\n      \"title\": \"USE CASES\",\n      \"paragraphs\": [\n        \"\\n        To understand the types of explanations GPT-3 \\n        \",\n        \"\\n        Analyzing and explaining time complexity\\nInstructors rate time complexity as the most dificult programming\\ntopic \\n        \",\n        \"\\n        Identifying common mistakes made by\\nbeginner programmers\\nCommonality exists in how students solve programming\\nproblems \\n        \",\n        \"\\n        Summarizing code at multiple levels of\\nabstraction\\nBefore understanding how a code snippet executes, it is often useful\\nto understand the purpose of the code \\n        \"\n      ]\n    },\n    {\n      \"title\": \"DISCUSSION\",\n      \"paragraphs\": [\n        \"Our three use cases demonstrate the potential for GPT-3 to explain\\ncode for intro CS students. Our poster presentation will feature all\\neight explanation types as a design space of explanations to convey\\nthe diversity of explanations that can be generated by LLMs. We will\\nhighlight best practices for generating efective explanations and\\npitfalls that lead to less efective explanations. We are evaluating\\nthe usefulness of these explanations in a series of summer classes.\"\n      ]\n    }\n  ]\n}"
  ]
}